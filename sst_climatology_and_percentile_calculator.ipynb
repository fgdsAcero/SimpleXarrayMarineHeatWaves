{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da928dd-13eb-496a-83a8-c4bdf5a5b7c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import psutil\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import date\n",
    "\n",
    "#from marineHeatWaves import marineHeatWaves\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee69a162-a864-4c58-85ec-ccdf15cd5e16",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFE099; padding: 10px; border: 3px solid #FFC233; text-align: center; font-family: Georgia, serif; font-weight: bold; white-space: pre;\">||| -------------------------------------------- |||    NOTES    ||| -------------------------------------------- |||\n",
    "</div>\n",
    "<div style=\"background-color: #EFFAFA; border: 2px solid #A2E2E2; font-family: Georgia, serif; padding: 10px\">\n",
    "    <br>This is the script to <strong>process percentile thresholds</strong> and <strong>climatological means</strong> from the data downloaded in <strong>data_downloader_script.ipynb</strong>.<div>\n",
    "    <br>&#x27A1;&#xFE0E; You can duplicate this script and run its copies simultaneously to download more percentiles/means.\n",
    "    <br>&#x27A1;&#xFE0E; If unusual errors appear after attempting to run a cell again, restart the kernel (and run the cell again afterward)!\n",
    "    <br>&#x27A1;&#xFE0E; After saving percentiles/means, check your file directories and compare their file sizes to see which ones ought to be removed/redownloaded!\n",
    "    <br>&#x27A1;&#xFE0E; Data directories that can be further modified by you may be identified by searching for \"NOTE: POTENTIAL DIRECTORY TWEAKING HERE\" in this script.\n",
    "    <br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49942d61-7a45-4c5e-988b-35881b5d6d11",
   "metadata": {},
   "source": [
    "<div style=\"color:#CD6600; padding: 10px; text-align: center; font-family: Georgia, serif; font-weight: bold; white-space: pre;\">°º¤ø,¸¸,ø¤º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø¤º°`°º¤ø,¸    LOADING FULL OBSERVED GLOBAL DATASETS    °º¤ø,¸¸,ø¤º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø¤º°`°º¤ø,¸\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4614ef2-755b-4d55-8669-08c7516e18ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- IMPORTANT SCRIPT-WIDE CONSTANTS ------------------------------------------------------------------------------------\n",
    "## Baseline set up:\n",
    "# Running the main function for the 1993–2022 baseline period (a 30 year baseline)\n",
    "baseline_choice = \"Baseline9322\" # identifier\n",
    "baseline_period_slice_choice = slice('1993-01-01', '2022-12-31') # for slicing time\n",
    "\n",
    "## Folder identifiers\n",
    "folder_name_choice = \"Full\" # A \"regional\" identifier to save the severity dataset with (\"{folder_name_choice}_SST... .zarr\")\n",
    "custom_id = \"fgd\" # A custom identifier to help further identify the downloaded data; can be left as \"\"\n",
    "my_root_directory = \"\" # Should be your root directory, from which you access data from and save data to\n",
    "\n",
    "# Doys (days of the year) set up\n",
    "starting_day_of_the_year = 1 # Your starting doy point for processing/saving severity (can be 1-366)\n",
    "ending_day_of_the_year = 366 # Your ending doy point for processing/saving severity (can be 1-366)\n",
    "\n",
    "# Misc\n",
    "minutes_choice = 10 # minutes (roughly) per memory update (to keep track of its use and avoid crashing/issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a87417-0765-4d0a-8e5f-21f340d1e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Full dataset loading (from my raw data) --------------------------------------------------------------------------\n",
    "# Your dataset directory here; use a * to collect ALL the applicable datasets for a given naming set up (using glob).\n",
    "raw_data_directory = f'{my_root_directory}/OISST/Data/sst.day.mean.*.nc'\n",
    "\n",
    "# Your data's maximum bounds\n",
    "lat_bounds = slice(-15, 90)\n",
    "lon_bounds = slice(0, 360)\n",
    "\n",
    "# Load the datasets as one\n",
    "ds = xr.open_mfdataset(\n",
    "    raw_data_directory,         # Glob pattern (the * grabs all datasets)\n",
    "    parallel=True,              # Enable parallel file opening \n",
    "    chunks='auto',              # Let dask choose optimal chunking \n",
    "    combine='by_coords',        # Merge based on coordinate values\n",
    "    engine='netcdf4')           # Specify the engine (may crash without this; restart the kernel if it happens)\n",
    "\n",
    "full_ds = ds.sel(lat=lat_bounds, lon=lon_bounds).sst\n",
    "print(\"Full raw data dataset:\\n\", full_ds, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4862c-305d-4a26-b0c4-a3c44ea338de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechunk the raw data dataset\n",
    "optimal_chunking = {'lat': 210, 'lon': 160}\n",
    "sst_full = full_ds.chunk(optimal_chunking)\n",
    "print(\"Final (properly chunked) raw data dataset:\\n\", sst_full)\n",
    "\n",
    "# NOTE: Check the printed dataset above and determine the optimal chunksize(s) for your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d12c80-665c-4589-9af3-9220ca854794",
   "metadata": {},
   "source": [
    "<div style=\"color:#104E8B; padding: 10px; text-align: center; font-family: Georgia, serif; font-weight: bold; white-space: pre;\">°º¤ø,¸¸,ø¤º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø¤º°`°º¤ø,¸    CALCULATING CLIMATOLOGY    °º¤ø,¸¸,ø¤º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø¤º°`°º¤ø,¸</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c92d36-a8d0-40bb-ba6d-5222b639961d",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* My code does not including the padding for missing values found in Eric Oliver's marineHeatWave code. \n",
    "* Percentiles are calculated and saved exclusively for single unique days of the year.\n",
    "* Means are calculated and saved for unique days of the year within custom batches.\n",
    "* Saving larger/\"complete\" mean datasets for a desired region is recommended over saving smaller subsets for the region, at least time-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724ec30-60a2-4364-ab67-51cf08331786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' ----------------------------------------------- '''\n",
    "''' Function to keep track of and show memory usage '''\n",
    "''' ----------------------------------------------- '''\n",
    "\n",
    "stop_monitoring = True # We begin by NOT showing any memory usage\n",
    "\n",
    "def monitor_memory(interval_minutes=5, log_file=None):\n",
    "    interval = interval_minutes * 60  \n",
    "    \n",
    "    while not stop_monitoring:\n",
    "        mem = psutil.Process(os.getpid()).memory_info().rss / (1024**3)  # in GB\n",
    "        print(f\" | Memory usage: {mem:.2f} GB | Memory: {psutil.virtual_memory().percent}% used | \")\n",
    "        \n",
    "        if log_file:\n",
    "            with open(log_file, 'a') as f:\n",
    "                f.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')}: {mem:.2f} GB\\n\")\n",
    "        time.sleep(interval)\n",
    "\n",
    "## -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' ------------------------------------------------------------------------------------------------ '''\n",
    "''' Function to normalize the unique day of the year value of each observed day in the format: 1-366 '''\n",
    "''' ------------------------------------------------------------------------------------------------ '''\n",
    "\n",
    "def normalize_dayofyear(time_coord):\n",
    "    doy = time_coord.dt.dayofyear\n",
    "    is_leap = time_coord.dt.is_leap_year\n",
    "\n",
    "    # This code ensures March 1 is always day 61, regardless of leap year\n",
    "    normalized_doy = xr.where(\n",
    "        (~is_leap) & (doy >= 60),  # If it is a non-leap year, doy 60 is March 1. If we have March 1 or later,\n",
    "        doy + 1,                          # then we push forward March 1 and/or the later days by 1 day.\n",
    "        doy                               # Otherwise, we keep original for leap years and Jan-Feb 28.\n",
    "    )\n",
    "    return normalized_doy\n",
    "\n",
    "## -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' ------------------------------------------------------------------------------- '''\n",
    "''' Function to check whether the inputted time period is at least roughly 30 years '''\n",
    "''' ------------------------------------------------------------------------------- '''\n",
    "\n",
    "def rough_30_year_period_check(time_slice, tolerance=0.01):\n",
    "    start_str = time_slice.start\n",
    "    stop_str = time_slice.stop\n",
    "\n",
    "    start_date = datetime.strptime(start_str, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(stop_str, '%Y-%m-%d')\n",
    "\n",
    "    delta = relativedelta(end_date, start_date)\n",
    "    total_years = delta.years + delta.months/12 + delta.days/365.25\n",
    "    \n",
    "    return total_years, abs(total_years - 30) <= tolerance\n",
    "\n",
    "## -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' ------------------------------------------------------------------------------------- '''\n",
    "''' Function to just save the percentile threshold/climatological mean dataset to storage '''\n",
    "''' ------------------------------------------------------------------------------------- '''\n",
    "def save_dataset_to_storage(folder_name_arg, baseline_name_arg, custom_file_id_arg,\n",
    "                            show_debug_arg, single_download_arg, \n",
    "                            current_percentile=None,\n",
    "                            start_val_arg=None, end_val_arg=None, \n",
    "                            ds_to_save=None):\n",
    "    \n",
    "    # We check if we are saving a climatology dataset or percentile one after subsetting the datasets appropriately\n",
    "    print(\"Starting doy for current subset: \", start_val_arg)\n",
    "    print(\"Ending doy for current subset: \", end_val_arg, '\\n')\n",
    "\n",
    "    # Set up a filepath/file identifier, if one is provided\n",
    "    if custom_file_id_arg != \"\":\n",
    "        id = f\"{custom_file_id_arg}_\"\n",
    "    else:\n",
    "        id = \"\"\n",
    "    \n",
    "    if start_val_arg == 1 and end_val_arg == 366:\n",
    "        final_ds_to_save = ds_to_save\n",
    "    else:\n",
    "        final_ds_to_save = ds_to_save.sel({'normalized_doy': slice(start_val_arg, end_val_arg)})\n",
    "    \n",
    "    # Here, we load the filepath destinations for the climatology datasets, the percentiles or means\n",
    "    # This runs for a climatological means dataset; NOTE: POTENTIAL DIRECTORY TWEAKING HERE\n",
    "    if current_percentile is None:\n",
    "        print(f\"Current climatological means subset to save:\\n{final_ds_to_save}\\n\")\n",
    "        file_name = f\"{folder_name_arg}_{id}sst_clim_subset_{start_val_arg}_to_{end_val_arg}_{baseline_name_arg}.zarr\"\n",
    "        filepath = f'{my_root_directory}/OISST/Clim/{baseline_name_arg}/{file_name}'\n",
    "    # This runs for a percentile dataset; NOTE: POTENTIAL DIRECTORY TWEAKING HERE\n",
    "    else:\n",
    "        print(f\"Current {current_percentile}th percentile subset to save:\\n{final_ds_to_save}\\n\")\n",
    "        file_name = f\"{folder_name_arg}_{id}sst_thresh_subset_{start_val_arg}_to_{end_val_arg}_{baseline_name_arg}.zarr\"\n",
    "        filepath = f'{my_root_directory}/OISST/Thresh{current_percentile}th/{baseline_name_arg}/{file_name}'\n",
    "    print(\"Filepath of subset: \", filepath)\n",
    "\n",
    "    # Lastly, we either finish by saving or stop if we are at the end of the debug\n",
    "    if show_debug_arg:\n",
    "        raise ValueError('End of debug. Proceed with the setting \"show_debug = False\" to start saving the percentile thresholds.')\n",
    "    else:\n",
    "        with ProgressBar():\n",
    "            final_ds_to_save.to_zarr(filepath, mode='w', consolidated=True)\n",
    "  \n",
    "        print(f\"Saved subset: doys {start_val_arg} to {end_val_arg}\\nMoving on!\\n\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        # Optional: for single downloads\n",
    "        if single_download_arg:\n",
    "            stop_monitoring = True\n",
    "            raise ValueError(\"Single dataset file-saving finished. Please enter a new desired chunk starting value to begin from.\")\n",
    "            \n",
    "## -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' ----------------------------------------------------------------- '''\n",
    "''' Function to actually calculate the percentile threshold dataset to storage '''\n",
    "''' ----------------------------------------------------------------- '''\n",
    "\n",
    "# Function to calculate the percentile threshold values for specific depths\n",
    "def calculate_sst_thresh_or_clim_given_a_percentile(sst_data, baseline_slice,\n",
    "                                                    folder_name, baseline_name, \n",
    "                                                    optimal_chunks, custom_file_id=\"\", window_half_width=5,\n",
    "                                                    minutes_per_memory_update=5, percentile=None, \n",
    "                                                    start_chunking_doy=1, end_chunking_doy=366,\n",
    "                                                    show_debug=True, single_download=False):     \n",
    "    \n",
    "    if show_debug:\n",
    "        debug_message_1 = \"You have set show_debug to true; this will show how the percentiles/means are processed based on your inputted arguments \\nand provide a preview of the output.\\n\"\n",
    "        debug_message_2 = \"\\nIf you are satisfied with the output (and your arguments), compute and save the calculated percentiles/means by setting\\nshow_debug to false.\\n\"\n",
    "        print(debug_message_1, debug_message_2)\n",
    "    \n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Part 0: Running a few quick error checks for the provided arguments!\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "              \n",
    "    # Checking start and end bounds\n",
    "    if start_chunking_doy < 1 or end_chunking_doy > 366:\n",
    "        raise ValueError(\"Please provide a start_chunking_doy that is ≥ 1 and an end_chunking_doy that is ≤ 366.\")\n",
    "        \n",
    "    if end_chunking_doy < start_chunking_doy: ## bug in other code, this is named chunk_end, not explicitly called.\n",
    "        raise ValueError(\"Please make sure your end_chunking_doy is greater than your start_chunking_doy; these are your dataset processing bounds.\")\n",
    "        \n",
    "    # Running a rough time check for the baseline provided\n",
    "    total_time, is_time_slice_30_years = rough_30_year_period_check(baseline_slice)\n",
    "    \n",
    "    if not is_time_slice_30_years:\n",
    "        error_message = \"Please check that your chosen baseline time slice covers a 30 year period.\"\n",
    "        raise ValueError(f\"{error_message}.\\n            The chosen slice covers roughly {total_time} years.\")\n",
    "    \n",
    "    # Establishing if we calculating percentiles or means \n",
    "    calculate_means = True if (percentile == None) else False    \n",
    "    chunk_size = end_chunking_doy\n",
    "    \n",
    "    # Checking the chunk size for our climatological calculations\n",
    "    if calculate_means and chunk_size == 0:\n",
    "        raise ValueError(\"Please set an integer value for end_chunking_doy, which sets the size of the doy batch you use to save the means in.\")\n",
    "\n",
    "    \n",
    "    if show_debug: \n",
    "        print(\"All clear!\\n\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Part 1: Assign normalized unique day of the year (doy) values to the sliced observation dataset\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    print(f\"Chosen baseline slice: {baseline_slice}\")\n",
    "    print(\"Note: the chosen baseline period has been identified as roughly covering a 30 year period. Do ensure this is the case separately.\\n\")\n",
    "    print(f\"Chosen window half-width: {window_half_width}\")\n",
    "    print(f\"(This means we use {window_half_width} days before and after each day of the year for each doy in our climatology/threshold.)\", '\\n')\n",
    "    \n",
    "    if calculate_means:\n",
    "        print(f\"The selected doy chunk size for climatological mean dataset batches is {chunk_size} doys.\\n\")\n",
    "        # if we are downloading only a single dataset, we adjust the upper/maximum limit to the data calculated/saved accordingly\n",
    "        end_chunk_val = 366 if not single_download else (start_chunking_doy + chunk_size - 1)\n",
    "        \n",
    "        print(f\"Calculating means in batches of (at most) {chunk_size} doys between {start_chunking_doy} and {end_chunk_val}.\\n\")\n",
    "    else:\n",
    "        percentile_used = percentile/100\n",
    "        print(f\"Final percentile used (in calculations): {percentile_used} ({percentile}th percentile)\", '\\n')\n",
    "        print(f\"Calculating thresholds individually for doys between {start_chunking_doy} and {end_chunking_doy}.\\n\")\n",
    "    \n",
    "    # Doy values for specific dates (for later)\n",
    "    feb28_doy = 59\n",
    "    feb29_doy = 60\n",
    "    mar1_doy = 61\n",
    "    \n",
    "    ## Extracting baseline period data\n",
    "    sst_baseline = sst_data.sel(time=baseline_slice)\n",
    "    if show_debug: print(\"Original SST Baseline Period Data: \", '\\n', sst_baseline, '\\n')\n",
    "   \n",
    "    # Assigning normalized doy values to the baseline period dataset\n",
    "    sst_norm = sst_baseline.assign_coords(\n",
    "        normalized_doy=('time', normalize_dayofyear(sst_baseline.time).data))\n",
    "    if show_debug: print(\"SST with Normalized Doy: \", '\\n', sst_norm, '\\n')\n",
    "        \n",
    "    '''\n",
    "    # Totally optional debug option here: show ALL normalized day of the year (doy) values;\n",
    "    # all years are in the 366-day format, with some missing day 60 (feb 29)\n",
    "    with np.printoptions(threshold=np.inf):\n",
    "        print(sst_norm.normalized_doy.values) \n",
    "    '''\n",
    "    \n",
    "    if show_debug: \n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Part 2: Get the actual doy values of the baseline period data (should be 1 - 366)\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "    unique_doys = np.unique(sst_norm.normalized_doy.data)\n",
    "    unique_doys = unique_doys[~np.isnan(unique_doys)]  # Remove any NaN values\n",
    "    unique_doys = unique_doys.astype(int)  # Ensure integer day-of-year values\n",
    "    if show_debug: print(f\"Found {len(unique_doys)} unique day-of-year values!\")\n",
    "    if show_debug: print(\"Unique doys:\", '\\n', unique_doys, '\\n')\n",
    "\n",
    "         \n",
    "    global stop_monitoring\n",
    "    if show_debug: \n",
    "        choice_message = \"climatological mean\" if calculate_means else \"percentile threshold\"\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "        print(f\"Part 3: Calculate the desired {choice_message} data for the desired doy(s).\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "        stop_monitoring = True # We don't want to start showing memory use.\n",
    "    else:\n",
    "        # We start monitoring here so that it only runs once\n",
    "        stop_monitoring = False # We do want to start showing memory use.\n",
    "        monitor_thread = threading.Thread(target=monitor_memory, kwargs={'interval_minutes': minutes_per_memory_update})\n",
    "        monitor_thread.daemon = True\n",
    "        monitor_thread.start()\n",
    "        \n",
    "    # Error messages for later\n",
    "    chunk_message_finished = \"Set chunk end reached!\\n\"\n",
    "    no_feb29_possible_warning = \"WARNING: Cannot interpolate Feb 29; missing Feb 28 or Mar 1 data!\\n\"\n",
    "    \n",
    "    # Initialize a dictionary for the climatological means or percentile thresholds\n",
    "    seas_dict = {}\n",
    "        \n",
    "    # Bool for debug purposes\n",
    "    shown_once = False\n",
    " \n",
    "    # Loop for doys 1 - 366 (excluding Feb 29, doy 60)\n",
    "    for doy in unique_doys:\n",
    "        # We skip February 29th (to interpolate later)\n",
    "        if doy == feb29_doy:  \n",
    "            continue # Note: doy 60 data is still used within the appropriate window_data when available\n",
    "        \n",
    "        # Create window around this DOY\n",
    "        window_doys = []\n",
    "        \n",
    "        for w in range(-window_half_width, window_half_width + 1):\n",
    "            target_doy = doy + w\n",
    "            \n",
    "            if show_debug and not shown_once: \n",
    "                print(\"Day of the year: \", doy, \"| Target Window Index: \", w, \"| Target Window Value: \", target_doy)\n",
    "\n",
    "            # Handle year wraparound properly\n",
    "            if target_doy < 1:\n",
    "                target_doy += 366\n",
    "            elif target_doy > 366:\n",
    "                target_doy -= 366\n",
    "            \n",
    "            # Handle year boundaries by keeping only valid doys\n",
    "            if target_doy in unique_doys:\n",
    "                window_doys.append(target_doy)\n",
    "            \n",
    "            if show_debug and not shown_once: print(\"Window Doys: \", window_doys, '\\n')\n",
    "\n",
    "        # Now, we select the data for this window\n",
    "        window_data = sst_norm.where(sst_norm.normalized_doy.isin(window_doys), drop=True)\n",
    "        \n",
    "        '''\n",
    "        ### Feature to be added: the ability to tweak the data prior to any percentile calculations in a manner like so:\n",
    "        window_data = window_data.sel(latitude=slice(-3, 0))\n",
    "        '''\n",
    "        \n",
    "        if show_debug and not shown_once: print(f\"Final window data for doy {doy} from the baseline period dataset: \", '\\n', window_data, '\\n')\n",
    "        \n",
    "        # Now, we calculate the percentile threshold/climatological mean across the time dimension\n",
    "        if window_data.time.size > 0:\n",
    "            # We calculate the climatological mean if that is what is desired\n",
    "            if calculate_means:\n",
    "                doy_to_save = window_data.mean(dim = 'time', skipna = True).expand_dims(normalized_doy=[doy])\n",
    "            # Otherwise, we calculate the percentile for a doy \n",
    "            else:\n",
    "                doy_to_save = window_data.chunk({'time':-1}).quantile(percentile_used, dim='time', skipna=True).expand_dims(normalized_doy=[doy])\n",
    "            seas_dict[doy] = doy_to_save\n",
    "\n",
    "            if show_debug and not shown_once:\n",
    "                message_type = \"climatological means\" if calculate_means else \"percentile thresholds\"\n",
    "                print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "                print(f\"Part 4: Store the {message_type} across all doys in an empty dictionary!\")\n",
    "                print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "                first_part = f\"Dictionary presently updated for doy {doy} with the time-averaged final window dataset in Part 3.\"\n",
    "                print(f\"{first_part}\\n\\nDictionary entry:\\n\", seas_dict[doy], '\\n')\n",
    "                shown_once = True  \n",
    "                \n",
    "    # After the for loop over the 1-366 day of the year range, we handle February 29th using linear interpolation\n",
    "    if (feb29_doy in unique_doys):\n",
    "        # If we have a dictionary with our percentiles/climatologies...\n",
    "        if feb28_doy in seas_dict and mar1_doy in seas_dict:\n",
    "            feb_28_ds = seas_dict[feb28_doy].squeeze().drop_vars('normalized_doy')\n",
    "            mar_1_ds = seas_dict[mar1_doy].squeeze().drop_vars('normalized_doy')\n",
    "            seas_dict[feb29_doy] = 0.5 * (feb_28_ds + mar_1_ds)\n",
    "            seas_dict[feb29_doy] = seas_dict[feb29_doy].expand_dims(normalized_doy=[feb29_doy])\n",
    "            if show_debug: print(\"Interpolated February 29 dataset (doy 60) in the dictionary:\\n\", seas_dict[feb29_doy], '\\n')\n",
    "        else:\n",
    "            print(no_feb29_possible_warning)\n",
    "            \n",
    "    # Now we proceed with the full climatology dictionary\n",
    "    if show_debug: \n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Part 5: Creating the complete climatology dataset from the dictionary\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "    # We create the correct coordinates from our dictionary for our final dataset\n",
    "    doy_coords = np.array(sorted(seas_dict.keys())) # array for full year (1 to 366, if leap)\n",
    "    if show_debug: print(\"Dictionary Keys of Registered Unique Day of the Year (doy) Climatological Mean Datasets\\n\", \n",
    "                         \"(Should include all values from 1 to 366):\\n\", doy_coords, '\\n')\n",
    "\n",
    "    # We stack the resulting dictionary datasets while maintaining the correct order\n",
    "    seas_list = [seas_dict[doy] for doy in doy_coords]\n",
    "    seas_year = xr.concat(seas_list, dim='normalized_doy')\n",
    "    seas_year = seas_year.assign_coords(normalized_doy=('normalized_doy', doy_coords))\n",
    "    seas_year = seas_year.chunk(optimal_chunks)\n",
    "    \n",
    "    # Additional chunking that prevents crashing (can be lowered for more stability)\n",
    "    if chunk_size <= 61:\n",
    "        seas_year = seas_year.chunk({'normalized_doy': chunk_size})\n",
    "    else:\n",
    "        seas_year = seas_year.chunk({'normalized_doy': 61})\n",
    "        \n",
    "    print(\"\\nFinal Climatology Dataset:\\n\", seas_year, '\\n')\n",
    "\n",
    "    # We check if we are saving this datasets fully or in batches; a chunk_size of 366 implies the full dataset is being saved (no batch saving)\n",
    "    if chunk_size == 366:\n",
    "        batch_saving = False\n",
    "    else:\n",
    "        batch_saving = True\n",
    "            \n",
    "    if show_debug: \n",
    "        saving_choice_message = \"in one go\" if not batch_saving else f\"via batches of {chunk_size} doys\"\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "        print(f\"Part 5: Saving the climatology dataset {saving_choice_message}!\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "        \n",
    "    # Coordinate values for batch saving (not full climatology dataset saving)\n",
    "    coord_values = seas_year['normalized_doy'].values\n",
    "\n",
    "    # We are saving the dataset in batches\n",
    "    if batch_saving:\n",
    "        for i in range(0, len(coord_values), chunk_size):\n",
    "            # First, we gather the starting and ending values of the processed chunk\n",
    "            start_val = coord_values[i]\n",
    "\n",
    "            # Quick check to see where to begin downloading a batch from...\n",
    "            if start_val < start_chunking_doy:\n",
    "                continue\n",
    "\n",
    "            # Grab the end index and value\n",
    "            end_idx = min(i + chunk_size, len(coord_values))\n",
    "            end_val = coord_values[end_idx - 1]\n",
    "            \n",
    "            # Save the dataset (subsetting occurs in the function)              \n",
    "            save_dataset_to_storage(folder_name_arg=folder_name, baseline_name_arg=baseline_name, custom_file_id_arg=custom_file_id,\n",
    "                                    show_debug_arg=show_debug, single_download_arg=single_download,\n",
    "                                    current_percentile=percentile, start_val_arg=start_val, \n",
    "                                    end_val_arg=end_val, ds_to_save=seas_year) \n",
    "            \n",
    "    # We are saving the full dataset\n",
    "    else:                      \n",
    "        save_dataset_to_storage(folder_name_arg=folder_name, baseline_name_arg=baseline_name, custom_file_id_arg=custom_file_id,\n",
    "                                    show_debug_arg=show_debug, single_download_arg=single_download,\n",
    "                                    current_percentile=percentile, start_val_arg=start_chunking_doy, \n",
    "                                    end_val_arg=end_chunking_doy, ds_to_save=seas_year) \n",
    "\n",
    "        \n",
    "    stop_monitoring = True # reset the monitoring before the next loop\n",
    "    \n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Calling the function to calculate the climatological datasets\n",
    "# Remember to update their arguments if you changed their names above!\n",
    "\n",
    "'''\n",
    "Argument explanations:\n",
    "* percentile=None  -  if percentile is set to None, the function processes CLIMATOLOGICAL MEANS\n",
    "* percentile=##  -  if a numeric percentile (under 1.0) is set, it processes the CLIMATOLOGICAL PERCENTILES (with that percentile) \n",
    "* optimal_chunking  -  this was set up earlier when loading the observed data!  \n",
    "''';\n",
    "\n",
    "calculate_sst_thresh_or_clim_given_a_percentile(sst_data=sst_full, baseline_slice=baseline_period_slice_choice,\n",
    "                                                folder_name=folder_name_choice, baseline_name=baseline_choice,\n",
    "                                                optimal_chunks=optimal_chunking, custom_file_id=custom_id, window_half_width=5, \n",
    "                                                minutes_per_memory_update=minutes_choice, percentile=None, \n",
    "                                                start_chunking_doy=starting_day_of_the_year, end_chunking_doy=ending_day_of_the_year,\n",
    "                                                show_debug=True, single_download=False)\n",
    "\n",
    "calculate_sst_thresh_or_clim_given_a_percentile(sst_data=sst_full, baseline_slice=baseline_period_slice_choice,\n",
    "                                                folder_name=folder_name_choice, baseline_name=baseline_choice,\n",
    "                                                optimal_chunks=optimal_chunking, custom_file_id=custom_id, window_half_width=5, \n",
    "                                                minutes_per_memory_update=minutes_choice, percentile=90,\n",
    "                                                start_chunking_doy=starting_day_of_the_year, end_chunking_doy=ending_day_of_the_year,\n",
    "                                                show_debug=True, single_download=False)\n",
    "\n",
    "# disable chunk list, not need. for now, just test if works.\n",
    "print(\"We have finished saving all desired doy datasets completely!\")\n",
    "stop_monitoring = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c102917e-226f-4081-b4bc-b454f4fde893",
   "metadata": {},
   "source": [
    "<div style=\"color:#008B00; padding: 10px; text-align: center; font-family: Georgia, serif; font-weight: bold; white-space: pre;\">ø¤º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø¤º°`°º¤ø,¸  FILE VALIDATION, VERIFICATION, AND ANIMATIONS  °º¤ø,¸¸,ø¤º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø¤\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a911489a-27f6-450b-a56d-fd5ff74d9e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to create an animation that shows the mean and percentile latitude and longitude maps for the full 1 - 366 period. \n",
    "def check_processed_datasets_with_an_animation(baseline_name_arg, folder_name_arg,\n",
    "                                               custom_output_filename=None, \n",
    "                                               percentile=None):\n",
    "    \n",
    "    ## Check proper arguments are provided for threshold datasets\n",
    "    if percentile == None:\n",
    "        show_climatological_means = True\n",
    "    else:\n",
    "        show_climatological_means = False\n",
    "        if percentile <= 0:\n",
    "            raise ValueError(\"Please set a positive, non-zero numeric percentile (based on the percentile you used above in your percentile datasets)!\")\n",
    "    \n",
    "    ## Gather the stored dataset filepaths\n",
    "    if show_climatological_means:\n",
    "        data_type = \"Clim\"\n",
    "        data_path = f\"{data_type}/{folder_name_arg}\"\n",
    "    else:\n",
    "        data_type = f\"Thresh{percentile}th\"\n",
    "        data_path = f\"{data_type}/{folder_name_arg}\"\n",
    "        \n",
    "    data_directory = f'{my_root_directory}/OISST/{data_path}'   \n",
    "    paths = glob.glob(f'{data_directory}_sst*{baseline_name_arg}.zarr')\n",
    "    \n",
    "    # A quick check to ensure we have located files given our arguments\n",
    "    if not paths:\n",
    "        start_error = \"No files found matching the pattern\"\n",
    "        cont_error = \"\\nPlease verify you inputted the proper baseline_name, folder_name, sub_folder_name, percentile, and show_climatological_means arguments!\"\n",
    "        raise FileNotFoundError(f\"{start_error}:\\n{data_directory}_sst...{baseline_name_arg}.zarr\\n{cont_error}\")\n",
    "    \n",
    "    \n",
    "    ## Fill a dictionary where all (1 to 366) doys are matched with their corresponding filepaths\n",
    "    doys_dict = {}\n",
    "    \n",
    "    for filepath in paths:\n",
    "        # Open and check what doys are in this file\n",
    "        ds = xr.open_zarr(filepath).sst\n",
    "        file_doys = ds['normalized_doy'].values\n",
    "\n",
    "        # Handle both single value and arrays\n",
    "        if np.isscalar(file_doys):\n",
    "            file_doys = [file_doys]\n",
    "         \n",
    "        # Map each doy to its file\n",
    "        for doy in file_doys:\n",
    "            doys_dict[int(doy)] = filepath\n",
    "         \n",
    "        ds.close()\n",
    "    \n",
    "    \n",
    "    ## Use a file and its features to set up the plot\n",
    "    available_doys = sorted(doys_dict.keys())\n",
    "    setup_file = doys_dict[available_doys[0]]\n",
    "    \n",
    "    if not show_climatological_means:\n",
    "        setup_ds = xr.open_zarr(filepath).sst.drop_vars(\"quantile\")\n",
    "    else:\n",
    "        setup_ds = xr.open_zarr(filepath).sst\n",
    "    \n",
    "    # Quick fix for my personal, early datasets\n",
    "    if 'doy' in setup_ds.coords:\n",
    "        setup_ds = setup_ds.rename({'doy': 'normalized_doy'}).expand_dims('normalized_doy')\n",
    "    \n",
    "    lon = setup_ds.lon.values\n",
    "    lat = setup_ds.lat.values\n",
    "    \n",
    "    # Check for single or multiple-doys in the setup dataset, and return the thetao data for just one (the first) doy\n",
    "    if len(setup_ds['normalized_doy'].values.shape) == 0 or setup_ds['normalized_doy'].values.size == 1:\n",
    "        # Single day file\n",
    "        setup_ds   = setup_ds.drop_vars(\"normalized_doy\").squeeze()\n",
    "        setup_data = setup_ds.values\n",
    "    else:\n",
    "        # Multi-day file\n",
    "        setup_data = setup_ds.isel(normalized_doy=0).values\n",
    "    \n",
    "    setup_ds.close()\n",
    "    \n",
    "    \n",
    "    ## Initialize the plot    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6), \n",
    "                           subplot_kw={'projection': ccrs.Mercator()})\n",
    "    \n",
    "    pcm = ax.pcolormesh(\n",
    "        lon, lat, setup_data,\n",
    "        cmap='RdYlBu_r',\n",
    "        vmin=-5, vmax=35,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "    )\n",
    "    \n",
    "    ax.set_extent([0, 360, -5, 90], crs=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.LAND, color='lightgray')\n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth=0.8)\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    \n",
    "    title = ax.set_title('')\n",
    "    title_base = '(Relative to 1993-2022)' if baseline_name_arg == 'Baseline9322' else f'({baseline_name_arg})'\n",
    "    \n",
    "    \n",
    "    ## Animation function\n",
    "    def animate(i):\n",
    "        doy = available_doys[i]\n",
    "        filepath = doys_dict[doy]\n",
    "        \n",
    "        # Load the dataset\n",
    "        ds = xr.open_zarr(filepath).sst\n",
    "      \n",
    "        if not show_climatological_means:\n",
    "            ds = ds.drop_vars(\"quantile\")\n",
    "            \n",
    "        # Check if this is a single-day or multi-day file\n",
    "        doy_values = ds['normalized_doy'].values\n",
    "        \n",
    "        # Load the data if available for a single doy or select the correct doy in a dataset\n",
    "        if np.isscalar(doy_values) or doy_values.size == 1:\n",
    "            frame_data = ds.values\n",
    "        else:\n",
    "            doy_idx = np.where(doy_values == doy)[0][0]\n",
    "            frame_data = ds.isel(normalized_doy=doy_idx).values\n",
    "        \n",
    "        # Update the plot\n",
    "        pcm.set_array(frame_data.ravel())\n",
    "        title.set_text(f'{data_type} Day {doy} of the Year\\n{title_base}')\n",
    "        \n",
    "        ds.close()\n",
    "        return pcm, title\n",
    "    \n",
    "    \n",
    "    ## Create the resulting animation\n",
    "    type_message = \"climatological means\" if show_climatological_means else \"percentile thresholds\"\n",
    "    print(f\"Began animation for the {type_message} of the {folder_name_arg} datasets!\")\n",
    "    \n",
    "    chosen_doys = len(available_doys)\n",
    "    \n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, animate,\n",
    "        frames=chosen_doys,\n",
    "        interval=200,\n",
    "        blit=False,\n",
    "        repeat=True\n",
    "    )\n",
    "    \n",
    "    writer = animation.PillowWriter(fps=2)\n",
    "    \n",
    "    if custom_output_filename == None:\n",
    "        output_filename = f\"{folder_name_arg}_{data_type}_{baseline_name_arg}_{chosen_doys}_doys_total.gif\"\n",
    "    else:\n",
    "        output_filename = custom_output_filename\n",
    "    print(f\"Saving animation at: {output_filename}\") \n",
    "    \n",
    "    \n",
    "    ## Save the resulting animation\n",
    "    def print_frame_progress(current_frame, total_frames):\n",
    "        print(f\"\\r → Doy (Frame) Processed: {current_frame + 1}/{total_frames}\", end='', flush=True)\n",
    "\n",
    "    #with ProgressBar():\n",
    "    anim.save(output_filename, writer=writer, dpi=100,\n",
    "              progress_callback=print_frame_progress)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.close(fig)\n",
    "    print(f\"\\nAnimation finished and saved!\\n\")\n",
    "    \n",
    "    return anim\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Note: loading a percentile produces an animation for percentile thresholds, and setting it to None an animation for means\n",
    "selected_baseline = baseline_choice # you can tweak this here, or use the one you selected at the top of the script\n",
    "check_processed_datasets_with_an_animation(baseline_name_arg=selected_baseline, folder_name_arg=\"Full\", \n",
    "                                           custom_output_filename=None, percentile=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ddb4fb-de04-4b3b-9e41-564073a63aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to verify against the outputs of marineHeatWaves to come in the future..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangeo23",
   "language": "python",
   "name": "pangeo23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
