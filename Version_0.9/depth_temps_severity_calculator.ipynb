{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63adab-d7ea-4d07-9288-1e572bd42d79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import psutil\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "import copernicusmarine\n",
    "from copernicusmarine import get\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee16f9d-09eb-479f-bb0c-b8d3a217d9c3",
   "metadata": {},
   "source": [
    "Load the raw, percentile, and climatological data for a given location (the script is functional, but work is in progress)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc462ab-c4e4-422d-b942-359083405d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to replace all instances of YOUR_DIRECTORY with your directory filepath!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28169f7a-70b1-4136-92b4-a5d3b10a4d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD THE CLIMATOLOGICAL MEANS, PERCENTILE THRESHOLDS, AND RAW DATA FOR A GIVEN LOCATION!\n",
    "\n",
    "# Function to correct some early saved datasets that had doy rather than the normalized_doy coordinate\n",
    "def add_ds_or_correct(path_arg):\n",
    "    # Open the dataset from its path\n",
    "    ds = xr.open_zarr(path_arg).thetao\n",
    "\n",
    "    # Correct for any datasets without normalized_doy but doy instead\n",
    "    if 'doy' in ds.coords:\n",
    "        ds = ds.rename({'doy': 'normalized_doy'}).expand_dims('normalized_doy')\n",
    "\n",
    "    return ds\n",
    "\n",
    "# Function to grab datasets from a specified directory\n",
    "def grab_data(directory, folder_filename_arg, is_raw=False):\n",
    "    # Get the ordered paths...\n",
    "    if is_raw:\n",
    "        paths = glob.glob(f'{directory}/daily_data*.zarr')\n",
    "    else:\n",
    "        paths = glob.glob(f'{directory}/{folder_filename_arg}_*.zarr')\n",
    "    paths.sort()\n",
    "\n",
    "    # Load the datasets properly\n",
    "    datasets = [add_ds_or_correct(path) for path in paths]\n",
    "\n",
    "    # Merge, rechunk, and return them\n",
    "    if is_raw:\n",
    "        full_ds = xr.concat(datasets, dim=\"time\").sortby(\"time\")\n",
    "        full_ds = full_ds.assign_coords(normalized_doy=('time', normalize_dayofyear(full_ds.time).data))\n",
    "    else:\n",
    "        full_ds = xr.concat(datasets, dim=\"normalized_doy\").sortby(\"normalized_doy\")\n",
    "    return full_ds\n",
    "    \n",
    "# Obtain the max and min values of the passed dataset\n",
    "def get_max_and_mins(ds):\n",
    "    min_lat = ds.latitude.min().item()\n",
    "    max_lat = ds.latitude.max().item()\n",
    "    min_lon = ds.longitude.min().item()\n",
    "    max_lon = ds.longitude.max().item()\n",
    "    return min_lat, max_lat, min_lon, max_lon\n",
    "            \n",
    "# Function to normalize the time values by the unique day of year (doy) coordinates\n",
    "def normalize_dayofyear(time_coord):\n",
    "    doy = time_coord.dt.dayofyear\n",
    "    is_leap = time_coord.dt.is_leap_year\n",
    "\n",
    "    # This code ensures March 1 is always day 61, regardless of leap year\n",
    "    normalized_doy = xr.where(\n",
    "        (~is_leap) & (doy >= 60),  # If it is a non-leap year, doy 60 is March 1. If we have March 1 or later,\n",
    "        doy + 1,                          # then we push forward March 1 and/or the later days by 1 day.\n",
    "        doy                               # Otherwise, we keep original for leap years and Jan-Feb 28.\n",
    "    )\n",
    "\n",
    "    return normalized_doy\n",
    "\n",
    "def extra_filter(ds): \n",
    "    if folder_filename == \"Atlantic\":\n",
    "        if (min(ds.longitude.values)==-101):\n",
    "            ds = ds.sel(longitude=slice(-101, -14.001))\n",
    "        if (min(ds.latitude.values)<0):\n",
    "            max_val = max(ds.latitude.values)\n",
    "            ds = ds.sel(latitude=slice(-0.75, max_val))\n",
    "    return ds\n",
    "    \n",
    "def show_map(ds_input, title, date, chosen_depth, chosen_doy, is_raw):\n",
    "    if is_raw:\n",
    "        ds = ds_input.sel(time=date, depth=chosen_depth, method='nearest')\n",
    "    else:\n",
    "        ds = ds_input.sel(normalized_doy=chosen_doy, depth=chosen_depth, method='nearest')\n",
    "        \n",
    "    projection_choice = ccrs.Mercator()\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), \n",
    "                           subplot_kw={'projection': projection_choice})\n",
    "    im = ax.pcolormesh(ds.longitude, ds.latitude, ds,\n",
    "                       transform=ccrs.PlateCarree(),\n",
    "                       cmap='RdYlBu_r')\n",
    "    ax.set_extent([0, 360, -30, 90], crs=ccrs.PlateCarree())\n",
    "    ax.coastlines()\n",
    "    ax.gridlines(draw_labels=True)\n",
    "    cbar = plt.colorbar(im, ax=ax, shrink=0.7)\n",
    "    cbar.set_label(f'{title}', rotation=270, labelpad=15)\n",
    "    ax.set_title(f'{title} {date}', fontsize=14)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    \n",
    "## PICK THE LOCATION HERE!\n",
    "# Regions to process thresholds for\n",
    "region_dict_list = [\n",
    "    {\"Atlantic\": [\n",
    "       \"Center\", \n",
    "        \"Right\",\n",
    "        \"Top\",          \n",
    "    ]},\n",
    "  #  {\"Pacific\": [\"Center\", \n",
    "                 #\"Left\"\n",
    "   #             ]},\n",
    "    #{\"Mid\": [\"Mid\",\n",
    "        #\"All\",\n",
    "    #]}\n",
    "]\n",
    "\n",
    "# Dictionary of chunk configs (set these to divide their coordinates' values by a small number less than 400 that leaves no remainder)\n",
    "best_chunk_configs = {\n",
    "    \"Atlantic\": {'depth': -1, 'normalized_doy':61, 'latitude': 109, 'longitude': 242},\n",
    "}\n",
    "\n",
    "best_raw_chunk_configs = {\n",
    "    \"Atlantic\": {'depth': -1, 'time': 24, 'latitude': 109, 'longitude': 242},\n",
    "}\n",
    "\n",
    "percentile = 90\n",
    "show_map = True\n",
    "\n",
    "\n",
    "for folder in region_dict_list:\n",
    "    print(\"Now creating combined climatological datasets for a particular region, which have the same dimensions...\\n\")\n",
    "    \n",
    "    for folder_filename, sub_folders in folder.items():\n",
    "        print(f\"Current main region: {folder_filename}\\n\")\n",
    "        \n",
    "        # Initializing...\n",
    "        optimal_chunking = None\n",
    "        \n",
    "        obs_datasets = []\n",
    "        thresh_datasets = []\n",
    "        \n",
    "        # Code to concatenate:\n",
    "        for sub_folder_filename in sub_folders:\n",
    "            print(f\"Current custom subregion: {sub_folder_filename}\\n\")\n",
    "            \n",
    "            # Your filepath here; this is my setup.\n",
    "            id_path = f\"{folder_filename}_{sub_folder_filename}\"\n",
    "            thresh_data_directory = f'YOUR_DIRECTORY/Thresh{percentile}th/{folder_filename}/{id_path}'\n",
    "                \n",
    "            # Grab the percentile threshold data\n",
    "            thresh_ds = grab_data(thresh_data_directory, folder_filename, False)\n",
    "            \n",
    "            # Grab the raw (\"observed\") data\n",
    "            obs_data_directory = f'YOUR_DIRECTORY/Data/{folder_filename}/{id_path}'\n",
    "            obs_ds = grab_data(obs_data_directory, folder_filename, True)\n",
    "            \n",
    "            # Filter the observed data\n",
    "            thresh_min_lat, thresh_max_lat, thresh_min_lon, thresh_max_lon = get_max_and_mins(thresh_ds)\n",
    "            obs_ds = obs_ds.sel(latitude=slice(thresh_min_lat, thresh_max_lat),\n",
    "                                longitude=slice(thresh_min_lon, thresh_max_lon))\n",
    "            \n",
    "            # Filter the threshold data\n",
    "            obs_min_lat, obs_max_lat, obs_min_lon, obs_max_lon = get_max_and_mins(obs_ds)\n",
    "            thresh_ds = thresh_ds.sel(latitude=slice(obs_min_lat, obs_max_lat),\n",
    "                                      longitude=slice(obs_min_lon, obs_max_lon))\n",
    "\n",
    "            thresh_ds = extra_filter(thresh_ds)\n",
    "            obs_ds = extra_filter(obs_ds)\n",
    "            \n",
    "            # Appending datasets\n",
    "            obs_datasets.append(obs_ds)\n",
    "            thresh_datasets.append(thresh_ds)\n",
    "        \n",
    "        print(\"--------------------------------------------------------------------------------------------------\\n\\n\")\n",
    "        \n",
    "        ## Combining stored datasets for each folder...\n",
    "        print(\"Folder:\", folder_filename, '\\n')\n",
    "        \n",
    "        # Setting up chunks\n",
    "        optimal_chunks = best_chunk_configs[folder_filename]\n",
    "        raw_optimal_chunks = best_raw_chunk_configs[folder_filename]\n",
    "        \n",
    "        # Observed datasets\n",
    "        if len(obs_datasets) > 1: \n",
    "            # If we have many datasets appended to the list, we combine them\n",
    "            obs_combined_ds = xr.combine_by_coords(obs_datasets, compat='no_conflicts')\n",
    "        # If there is just one dataset appended to the list, we ignore the list\n",
    "        else:\n",
    "            obs_combined_ds = obs_ds\n",
    "        obs_combined_ds = obs_combined_ds.thetao\n",
    "        \n",
    "        if len(thresh_datasets) > 1: \n",
    "            thresh_combined_ds = xr.combine_by_coords(thresh_datasets, compat='no_conflicts')\n",
    "        # If there is just one dataset appended to the list, we ignore the list\n",
    "        else:\n",
    "            thresh_combined_ds = thresh_ds\n",
    "        thresh_combined_ds = thresh_combined_ds.thetao\n",
    "        \n",
    "        # Getting the max and mins of the dataset for the climatology slicing\n",
    "        obs_comb_min_lat, obs_comb_max_lat, obs_comb_min_lon, obs_comb_max_lon = get_max_and_mins(obs_combined_ds)\n",
    "            \n",
    "        ## Clim dataset\n",
    "        # Directory path setup for accessing my climatological means data\n",
    "        main_folder = folder_filename\n",
    "        if id_path == \"Mid_Mid\":\n",
    "            main_folder = \"Mid_Mid\"\n",
    "        elif id_path == \"Mid_All\":\n",
    "            main_folder = \"Mid_All\"\n",
    "            \n",
    "        clim_data_directory = f'YOUR_DIRECTORY/Clim/Full/{main_folder}'\n",
    "        clim_ds = grab_data(clim_data_directory, folder_filename, False)\n",
    "        \n",
    "        globals()[f'{folder_filename}_clim'] = clim_ds.sel(latitude=slice(obs_comb_min_lat, obs_comb_max_lat),\n",
    "                              longitude=slice(obs_comb_min_lon, obs_comb_max_lon)).chunk(optimal_chunks)\n",
    "        \n",
    "        #clim_min_lat, clim_max_lat, clim_min_lon, clim_max_lon = get_max_and_mins(clim_ds)\n",
    "        globals()[f'{folder_filename}_thresh'] = thresh_combined_ds.chunk(optimal_chunks)\n",
    "        globals()[f'{folder_filename}_obs'] = obs_combined_ds.chunk(raw_optimal_chunks)\n",
    "        \n",
    "        #thresh_combined_ds = thresh_combined_ds.sel(latitude=slice(clim_min_lat, clim_max_lat),\n",
    "         #                     longitude=slice(clim_min_lon, clim_max_lon)).chunk(optimal_chunks)\n",
    "        \n",
    "        #obs_combined_ds = obs_combined_ds.sel(latitude=slice(clim_min_lat, clim_max_lat),\n",
    "         #                     longitude=slice(clim_min_lon, clim_max_lon)).chunk(raw_optimal_chunks)\n",
    "                \n",
    "        print(\"Final observed:\\n\", globals()[f'{folder_filename}_obs'], '\\n')\n",
    "        print(\"Final thresh:\\n\", globals()[f'{folder_filename}_thresh'], '\\n')\n",
    "        print(\"Final means:\\n\", globals()[f'{folder_filename}_clim'], '\\n\\n')\n",
    "        \n",
    "        # CURRENTLY UNAVAILABLE\n",
    "        #if show_map:\n",
    "            # Showing a quick map of the dataset (to make sure everything came out right!)\n",
    "         #   show_map(ds_input=globals()[f'{folder_filename}_obs'], title=\"Observed (deg C)\", date=\"2003-02-28\", is_raw=False)\n",
    "          #  show_map(globals()[f'{folder_filename}_thresh'], \"90th Percentile (deg C)\")\n",
    "          #  show_map(globals()[f'{folder_filename}_clim'], \"Climatological Mean (deg C)\")\n",
    "        \n",
    "        print(\"------------------------------------------------o-------------------------------------------------\\n\")\n",
    "            \n",
    "            # function to calc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfe3e10-4d64-4c23-bd98-b2f38d90acd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Thetao depth saving/loading\n",
    "\n",
    "# First load a FULL dataset with an additional depth value past your target value\n",
    "depth_filepath = \"YOUR_DIRECTORY/Data/Misc/example_file_depths_up_to_NUMBERm.zarr\"\n",
    "depth_final = 319 # the upper bound for the depths + 1 after the target \n",
    "\n",
    "\n",
    "# Next, gather all relevant observed depth values\n",
    "depths_ds = xr.open_zarr(depth_filepath)\n",
    "\n",
    "all_depth_levels = depths_ds.depth.where(depths_ds.depth <= depth_final, drop=True) # We get an additional depth point after \n",
    "print(\"All depth levels: \", '\\n', all_depth_levels, '\\n')\n",
    "\n",
    "## From this output, determine what depths you are particularly interested in reaching towards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cef78e4-7658-4c63-88f3-8e6cd7f9b7b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to calculate column-averaged severity\n",
    "def get_max_and_min_years(ds):\n",
    "    min_yr = ds.time[0].dt.year.item()\n",
    "    max_yr = ds.time[-1].dt.year.item()\n",
    "    return min_yr, max_yr\n",
    "\n",
    "\n",
    "def check_valid_type(data, data_type, data_name, correction_message):\n",
    "    if not isinstance(data, data_type):\n",
    "        raise ValueError(f\"Invalid {data_name} provided. Please provide a proper {data_type.__name__} {data_name}, or {correction_message}.\")\n",
    "\n",
    "\n",
    "def event_gap_filling(events_series: np.ndarray, minDuration: int = 5, maxGap: int = 2) -> np.ndarray:\n",
    "    '''\n",
    "    Gap-filling function for marine heatwave (MHW) events following the Hobday et al. (2016) definition.\n",
    "    \n",
    "    Key rules:\n",
    "    1. Only events of 5+ (or minDuration+) days duration are considered valid MHWs\n",
    "    2. Gaps of 2 (or maxGap) days or less between any valid events should merge them\n",
    "    3. When merging occurs, the entire merged period becomes one event\n",
    "    4. The resulting event must still meet the 5-day minimum duration\n",
    "    \n",
    "    Examples from Hobday et al. (2016):\n",
    "    - [5hot, 2cool, 6hot] → 13-day event (5 + 2 + 6 = 13)\n",
    "    - [5hot, 1cool, 2hot] → 5-day event (only first 5 days qualify)\n",
    "    - [2hot, 1cool, 5hot] → 5-day event (only last 5 days qualify)\n",
    "    - [5hot, 4cool, 6hot] → two separate events (5-day + 6-day)\n",
    "    '''\n",
    "    \n",
    "    # Input validation and conversion\n",
    "    assert events_series.ndim == 1, f\"Expected 1D series, got {events_series.ndim}D\"\n",
    "\n",
    "    # We convert the dataset to boolean if it is not boolean\n",
    "    if events_series.dtype != bool:\n",
    "        print(\"Dataset conversion occurred!\", '\\n')\n",
    "        events_series = events_series.astype(bool)\n",
    "\n",
    "    # If the input series is full of False values, we return an all 0's array\n",
    "    if not np.any(events_series):\n",
    "        return np.zeros_like(events_series, dtype=int)\n",
    "    \n",
    "    ## First, we find all consecutive True sequences that are ≥ minDuration\n",
    "    # We initialize our empty list that stores all the valid sequences ≥ the minDuration value\n",
    "    valid_sequences = []\n",
    "    i = 0\n",
    "\n",
    "    # Main loop that examines every position in the series\n",
    "    while i < len(events_series):\n",
    "        # If we identify a True value, we check for a potential marine heatwave event\n",
    "        if events_series[i]:\n",
    "            # We save the starting point\n",
    "            start_idx = i\n",
    "            \n",
    "            # We advance over a series of consecutive True values (until reaching the end of the array or a False value)\n",
    "            while i < len(events_series) and events_series[i]:\n",
    "                i += 1\n",
    "\n",
    "            # We save the end value and duration of our series accordingly\n",
    "            end_idx = i - 1 \n",
    "            duration = end_idx - start_idx + 1 # we add 1 since indeces are 0-based\n",
    "\n",
    "            # If the minimumDuration is met, we store the start and end indeces as a tuple in our list\n",
    "            if duration >= minDuration:\n",
    "                valid_sequences.append((start_idx, end_idx))\n",
    "                \n",
    "        else: # If the current position if a False day, we move along\n",
    "            i += 1\n",
    "\n",
    "    # If no valid sequences exist, we return a 0-value array\n",
    "    if not valid_sequences:\n",
    "        return np.zeros_like(events_series, dtype=int)\n",
    "    \n",
    "    ## Next, we group valid event sequences by checking gaps between them\n",
    "    # We initialize an empty list to store events that should be merged\n",
    "    event_groups = []\n",
    "    current_group = [valid_sequences[0]] # sets the current group to check\n",
    "\n",
    "    # We loop through all valid sequences (starting from the second one), comparing each with the previous\n",
    "    for i in range(1, len(valid_sequences)):\n",
    "        \n",
    "        prev_end = current_group[-1][1] # we extract the end index ([1]) of the last sequence in the current_group ([-1])\n",
    "        curr_start = valid_sequences[i][0] # we extract the start index ([0]) of the current valid_sequence checked ([i])\n",
    "\n",
    "        # We calculate the number of days between the current event's start index and the previous event's end index\n",
    "        gap_length = curr_start - prev_end - 1\n",
    "\n",
    "        # If the gap is <= the maxGap value, we save/merge (via append) the current valid sequence to/with the current group\n",
    "        if gap_length <= maxGap:\n",
    "            current_group.append(valid_sequences[i])\n",
    "        # If the gap is not <= the maxGap value, we add the untouched current group back to the merged event list and reset the current group\n",
    "        else:\n",
    "            event_groups.append(current_group)\n",
    "            current_group = [valid_sequences[i]]\n",
    "\n",
    "    # We add the last group to the merged event group list (manually)\n",
    "    event_groups.append(current_group)\n",
    "    \n",
    "    ## Finally, we create our output with appropriate event IDs\n",
    "    # We begin with an all 0's array\n",
    "    out = np.zeros_like(events_series, dtype=int)\n",
    "\n",
    "    # We iterate through all the groups in the merged group list\n",
    "    for event_id, group in enumerate(event_groups, 1):\n",
    "        # We find the overall start and end index for each group \n",
    "        group_start = group[0][0]\n",
    "        group_end = group[-1][1]\n",
    "\n",
    "        # We assign a unique event id for all values from group start to the group end indeces (inclusive)\n",
    "        out[group_start:group_end + 1] = event_id\n",
    "\n",
    "    # We return our output\n",
    "    return out\n",
    "        \n",
    "    \n",
    "# Create a continous time coordinate for a passed dataset\n",
    "def create_continuous_time_coordinate(ds):\n",
    "    n_timesteps = len(ds.normalized_doy)\n",
    "    continuous_time = np.arange(1, n_timesteps + 1)\n",
    "    \n",
    "    # Add the continuous coordinate and make it the dominant coordinate\n",
    "    ds = ds.assign_coords(time_continuous=('normalized_doy', continuous_time))\n",
    "    ds = ds.swap_dims({'normalized_doy':'time_continuous'}).chunk({'time_continuous':-1})\n",
    "    return ds\n",
    "\n",
    "\n",
    "def run_mhw_test(initial_mask_ds, final_mask_ds, lat_val=None, lon_val=None, depth_val=None):\n",
    "    # Quick check to ensure values were inputted\n",
    "    if any(val is None for val in (lat_val, lon_val, depth_val)):\n",
    "        raise ValueError(\"Missing a key argument in the run_mhw_test function. Please ensure a valid integer value is provided for each value argument!\")\n",
    "    \n",
    "    print(\"--------------------------------------------------   TEST START   ---------------------------------------------------\\n\")\n",
    "    print(f\"RUNNING TEST FOR NEAREST LATITUDE: {lat_val}, LONGITUDE: {lon_val}, AND DEPTH: {depth_val}.\\n\")\n",
    "    \n",
    "    # Grab a subset from the initial_mask_ds\n",
    "    subset_original = initial_mask_ds.sel(latitude=lat_val, longitude=lon_val, depth=depth_val, method='nearest')\n",
    "    real_lat = subset_original.latitude.values.item()\n",
    "    real_lon = subset_original.longitude.values.item()\n",
    "    real_depth = subset_original.depth.values.item()\n",
    "    print(f\"ACTUAL VALUES DETECTED:\\nLATITUDE: {real_lat}, LONGITUDE: {real_lon}, DEPTH: {real_depth}\\n\")\n",
    "    \n",
    "    original_labeled = subset_original.values\n",
    "    print(\"Exceed values:\\n\", original_labeled, '\\n\\n')\n",
    "\n",
    "    events_labeled = final_mask_ds.sel(latitude=lat_val, longitude=lon_val, depth=depth_val, method='nearest').values\n",
    "    print(\"Event gap-filled values:\\n\", events_labeled, '\\n\\n')\n",
    "\n",
    "    print(\"----------o-----------0----------o-------------\\n\")\n",
    "\n",
    "    length = min(len(original_labeled), len(events_labeled))\n",
    "    step = 10\n",
    "\n",
    "    for i in range(0, length, step):\n",
    "        end_idx = min(i + step, length)\n",
    "        print(f\"Values {i} to {end_idx - 1}:\")\n",
    "        print(\"Exceed values:\", original_labeled[i:end_idx])\n",
    "        print(\"Event gap-filled values:\", events_labeled[i:end_idx])\n",
    "        print(\"-\" * 60)\n",
    "        print(\" \")\n",
    "    print(\"--------------------------------------------------   END OF TEST   ---------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "stop_monitoring = True # We begin by NOT showing any memory usage\n",
    "def monitor_memory(interval_minutes=5, log_file=None):\n",
    "    interval = interval_minutes * 60  \n",
    "    \n",
    "    while not stop_monitoring:\n",
    "        mem = psutil.Process(os.getpid()).memory_info().rss / (1024**3)  # in GB\n",
    "        print(f\" | Memory usage: {mem:.2f} GB | Memory: {psutil.virtual_memory().percent}% used | \")\n",
    "        \n",
    "        if log_file:\n",
    "            with open(log_file, 'a') as f:\n",
    "                f.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')}: {mem:.2f} GB\\n\")\n",
    "        time.sleep(interval)\n",
    "    \n",
    "    \n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "def calculate_severity(obs_data, thresh_data, clim_data, \n",
    "                       baseline_name, folder_name, sub_folder_name,\n",
    "                       best_chunks, best_chunks_no_depth, \n",
    "                       depths_list, depth_thicknesses,\n",
    "                       smoothWidth=31, minDaysMhwDuration=5, maxGapDaysInMhw=2, \n",
    "                       minutes_per_mem_update=5,\n",
    "                       show_debug=True,\n",
    "                       custom_years=False, start_yr=None, end_yr=None,\n",
    "                       mhw_test=False, mhw_test_lat=None, mhw_test_lon=None, mhw_test_depth=None):\n",
    "\n",
    "    # Quick check for inputs regarding a marine heatwave test a specific coordinate\n",
    "    if mhw_test:\n",
    "        check_valid_type(mhw_test_lat, int, \"latitude value\", \"set mhw_test to False\")\n",
    "        check_valid_type(mhw_test_lon, int, \"longitude value\", \"set mhw_test to False\")\n",
    "        check_valid_type(mhw_test_depth, int, \"depth value\", \"set mhw_test to False\")\n",
    "    \n",
    "    # Gather the min and max years in the observed data\n",
    "    min_obs_yr, max_obs_yr = get_max_and_min_years(obs_data)\n",
    "\n",
    "    # Setting up custom years, or going with the default (min to max years in the observed data)\n",
    "    if custom_years:\n",
    "        check_valid_type(start_yr, int, \"starting year\", \"set custom_years to False\")\n",
    "        check_valid_type(end_yr, int, \"ending year\", \"set custom_years to False\")\n",
    "        years = range(start_yr, end_yr + 1)\n",
    "    else:\n",
    "        years = range(min_obs_yr, max_obs_yr + 1)\n",
    "        \n",
    "\n",
    "    # Beginning of code for severity calculation:\n",
    "    print(\"--------------------o-------------------------------o------------------------------o----------------------------\")\n",
    "    print(f'\\nBaseline used: {baseline_name}\\n') # We only have one baseline\n",
    "    print(f\"The smoothWidth argument was set to: {smoothWidth}.\") \n",
    "    print(f\"This means a rolling window of {smoothWidth} doys centered on each individual doy is used.\")\n",
    "    print(f\"For each doy, the rolling window contains the previous {(smoothWidth-1)/2} doys, the center doy, and the next {(smoothWidth-1)/2} doys.\\n\")\n",
    "\n",
    "    # Showing the original datasets if show_debug is enabled\n",
    "    if show_debug: \n",
    "        print('Original raw, \"observed\" data:\\n', obs_data, '\\n')\n",
    "        print(\"Original percentile threshold data:\\n\", thresh_data, '\\n')\n",
    "        print(\"Original climatological means data:\\n\", clim_data, '\\n')\n",
    "        print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    \n",
    "    ## Calculating layer thicknesses, and taking the column-average of the severity dataset for marine heatwaves and entire columns\n",
    "    for target_depth in depths_list:\n",
    "        # Getting the lower bound from the target list\n",
    "        max_wanted_depth = depth_thicknesses.sel(depth=target_depth, method='nearest').item()\n",
    "        print(\"Inputted Lower-Bound Depth: \", max_wanted_depth)\n",
    "            \n",
    "        # Get the positional index of the bound\n",
    "        depth_coord = depth_thicknesses.get_index('depth').values\n",
    "        max_depth_index = np.where(depth_coord == max_wanted_depth)[0][0]\n",
    "\n",
    "        if max_depth_index + 1 < len(depth_coord):\n",
    "            next_depth = depth_coord[max_depth_index + 1]\n",
    "            next_value = depth_thicknesses.sel(depth=next_depth).item()\n",
    "            print(\"Following Lower-Bound Depth: \", next_value)\n",
    "        else:\n",
    "            raise ValueError(\"Error: Depth bound exceeded! Please provide a target depth list with lower depth values!\")\n",
    "\n",
    "        # Gathering only the values we intend on using (depth values up to 50 meters)\n",
    "        valid_idx = np.where(depth_thicknesses <= max_wanted_depth)[0]\n",
    "        valid_depths = depth_thicknesses[valid_idx]\n",
    "        depth_values = valid_depths.values\n",
    "\n",
    "        # Creating layer bounds, using depth temperatures as midpoints\n",
    "        bounds = np.zeros(len(depth_values) + 1)\n",
    "        bounds[0] = 0 # Surface boundary\n",
    "\n",
    "        # Midpoints between depths for internal bounds\n",
    "        bounds[1:-1] = (depth_values[:-1] + depth_values[1:]) / 2\n",
    "\n",
    "        # The last bound is the midpoint between the last included depth (in depth_values) and first excluded depth (in all_depth_values)\n",
    "        if len(valid_idx) < len(depth_thicknesses):\n",
    "            bounds[-1] = (depth_values[-1] + next_value) / 2\n",
    "            final_max_depth_val = round(bounds[-1], 1)\n",
    "            print(f\"Final (Interpolated) Lower-Bound Depth Used: {final_max_depth_val} meters\\n\") \n",
    "\n",
    "        if show_debug: \n",
    "            print(\"Depth values: \", '\\n', depth_values, '\\n')\n",
    "            print(\"Bounds: \", '\\n', bounds, '\\n')    \n",
    "            print(\"Length of valid depths: \", len(depth_values))\n",
    "            print(\"Length of bounds: \", len(bounds), '(Should be 1 more than valid depths, for thickness calculation)\\n')\n",
    "\n",
    "        # Creating a layer thickness data array\n",
    "        layer_thickness = xr.DataArray(np.diff(bounds), coords={\"depth\": valid_depths}, dims=\"depth\")\n",
    "        if show_debug: \n",
    "            print(\"Final layer thicknesses: \", '\\n', layer_thickness, '\\n')\n",
    "            print(\"--------------------------------------------------------------------------------------\", '\\n')\n",
    "            \n",
    "        # Subsetting the datasets by depth\n",
    "        obs_data = obs_data.sel(depth=slice(0, max_wanted_depth))\n",
    "        thresh_data = thresh_data.sel(depth=slice(0, max_wanted_depth))\n",
    "        clim_data = clim_data.sel(depth=slice(0, max_wanted_depth))\n",
    "        \n",
    "        # Showing the depth-subsetted datasets if show_debug is enabled\n",
    "        if show_debug: \n",
    "            print('Depth-Subsetted \"observed\" data:\\n', obs_data, '\\n')\n",
    "            print(\"Depth-Subsetted percentile threshold data:\\n\", thresh_data, '\\n')\n",
    "            print(\"Depth-Subsetted climatological means data:\\n\", clim_data, '\\n')\n",
    "            print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "    \n",
    "       \n",
    "        ## PADDING AND SMOOTHING\n",
    "        # Padding with smoothWidth (which is more than enough for the desired smoothing with smoothWidth)\n",
    "        padded_clim = clim_data.pad(normalized_doy=smoothWidth, mode='wrap')\n",
    "        if show_debug: print(\"Padded Climatology:\\n\", padded_clim, '\\n')\n",
    "\n",
    "        padded_thresh = thresh_data.pad(normalized_doy=smoothWidth, mode='wrap')\n",
    "        if show_debug: print(\"Padded Threshold:\\n\", padded_thresh, '\\n\\n')\n",
    "\n",
    "        # Smoothing\n",
    "        clim_smoothed = padded_clim.rolling(normalized_doy=smoothWidth, center=True, min_periods=smoothWidth).mean() \n",
    "        if show_debug: print(f\"Padded Climatology Smoothed by {smoothWidth} doys:\\n\", clim_smoothed, '\\n')\n",
    "\n",
    "        clim_smoothed = clim_smoothed.isel(normalized_doy=slice(smoothWidth, -smoothWidth))\n",
    "        if show_debug: print(\"Smoothed Climatology (No Pad):\\n\", clim_smoothed, '\\n\\n')\n",
    "\n",
    "        thresh_smoothed = padded_thresh.rolling(normalized_doy=smoothWidth, center=True, min_periods=smoothWidth).mean()\n",
    "        if show_debug: print(f\"Padded Threshold Smoothed by {smoothWidth} doys: \", thresh_smoothed, '\\n')\n",
    "\n",
    "        thresh_smoothed = thresh_smoothed.isel(normalized_doy=slice(smoothWidth, -smoothWidth))\n",
    "        if show_debug: print(\"Threshold Smoothed (No Pad): \", thresh_smoothed, '\\n')\n",
    "\n",
    "        # We also save early and late smoothed doy data for later \n",
    "        # MAY NEED TO BE IN THE FOR LOOP IF USING A LARGER smoothWidth (if the smoothWidth includes Feb 29, doy 60)\n",
    "        thresh_start_days = thresh_smoothed.isel(normalized_doy=slice(0, smoothWidth)).copy()\n",
    "        thresh_end_days = thresh_smoothed.isel(normalized_doy=slice(-smoothWidth, None)).copy()\n",
    "\n",
    "\n",
    "        global stop_monitoring\n",
    "        if show_debug:\n",
    "            stop_monitoring = True # We don't want to start showing memory use.\n",
    "            print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "        else:\n",
    "            # We start monitoring here so that it only runs once\n",
    "            stop_monitoring = False # We do want to start showing memory use.\n",
    "            monitor_thread = threading.Thread(target=monitor_memory, kwargs={'interval_minutes': minutes_per_mem_update})\n",
    "            monitor_thread.daemon = True\n",
    "            monitor_thread.start()\n",
    "\n",
    "        shown_once = False # for debugging purposes\n",
    "\n",
    "\n",
    "        ## CALCULATING ANOMALIES BY YEAR (observed - climatological means) AND DETECTING MHWS\n",
    "        for year in years:\n",
    "            # Time subsetting\n",
    "            obs_year_data = obs_data.sel(time=f'{year}')\n",
    "            obs_year_data_norm = obs_year_data.swap_dims({'time':'normalized_doy'})\n",
    "            if show_debug and not shown_once: print(f\"Observed data for {year}:\\n\", obs_year_data_norm, '\\n')\n",
    "\n",
    "            # Aligning the datasets (returns doys common to both)\n",
    "            obs_aligned, clim_aligned = xr.align(obs_year_data_norm, clim_smoothed, join=\"inner\")\n",
    "            clim_aligned = clim_aligned.chunk(best_chunks)\n",
    "\n",
    "            obs_aligned, thresh_aligned = xr.align(obs_year_data_norm, thresh_smoothed, join=\"inner\")\n",
    "            thresh_aligned = thresh_aligned.chunk(best_chunks)\n",
    "\n",
    "            if show_debug and not shown_once: \n",
    "                print(f\"Thresh aligned: \", thresh_aligned, '\\n')\n",
    "                print(f\"Clim aligned: \", clim_aligned, '\\n')\n",
    "\n",
    "            # We also save the time variable separately for later, then drop it from the observed\n",
    "            obs_aligned_time = obs_aligned.swap_dims({'normalized_doy':'time'}).drop_vars('normalized_doy').time\n",
    "            if show_debug and not shown_once: print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "            ### DETECTING MHWS ACROSS THE YEARS\n",
    "            ## Grabbing previous and next year data\n",
    "            obs_aligned = obs_aligned.swap_dims({'normalized_doy':'time'})\n",
    "            if show_debug and not shown_once: print(f\"Full Year Data from the Current Year ({year}):\\n\", obs_aligned, '\\n')\n",
    "\n",
    "            # Important objects for padding\n",
    "            prev_year_data = None\n",
    "            next_year_data = None\n",
    "\n",
    "            # We grab the previous year's data if it is not the first or final year in the dataset\n",
    "            if year > min_obs_yr:\n",
    "                prev_year = year - 1\n",
    "                prev_year_full = obs_data.sel(time=f'{prev_year}')\n",
    "\n",
    "                # If it exists, we grab the previous year's final period of length smoothWidth (up to 366)\n",
    "                if len(prev_year_full) > 0:\n",
    "                    prev_year_data = prev_year_full.isel(time=slice(-smoothWidth, None))\n",
    "                    if show_debug and not shown_once: print(f\"End of the Year Data from the Previous Year ({prev_year}):\\n\", prev_year_data, '\\n')\n",
    "\n",
    "            # We grab the next year's data if it is not the first or final year in the dataset\n",
    "            if year < max_obs_yr:\n",
    "                next_year = year + 1\n",
    "                next_year_full = obs_data.sel(time=f'{next_year}')\n",
    "\n",
    "                if len(next_year_full) > 0:\n",
    "                    next_year_data = next_year_full.isel(time=slice(0, smoothWidth))\n",
    "                    if show_debug and not shown_once: print(f\"Beginning of the Year Data from the Next Year ({next_year}):\\n\", next_year_data, '\\n')\n",
    "\n",
    "            ## Merging the previous and next year datasets, if they are available\n",
    "            data_pieces = [] \n",
    "\n",
    "            # We first append the previous year data to our array, if it exists\n",
    "            if prev_year_data is not None: data_pieces.append(prev_year_data)\n",
    "\n",
    "            # Next, we append the current year data to our array, if it exists\n",
    "            data_pieces.append(obs_aligned)\n",
    "\n",
    "            # Lastly, we append the next year data to our array, if it exists\n",
    "            if next_year_data is not None: data_pieces.append(next_year_data)\n",
    "\n",
    "            if show_debug and not shown_once: print(\"Data pieces (prev + full current + next):\\n\", data_pieces, '\\n\\n')\n",
    "\n",
    "            # Merging the datasets\n",
    "            obs_year_data_extended = xr.concat(data_pieces, dim='time')\n",
    "            obs_year_data_extended = obs_year_data_extended.sortby('time')\n",
    "            if show_debug and not shown_once: print(\"Initial merged observed data:\\n\", obs_year_data_extended, '\\n')\n",
    "\n",
    "            # We switch back to our desired normalized doy time dimension\n",
    "            obs_year_data_extended = obs_year_data_extended.swap_dims({'time':'normalized_doy'}).drop_vars('time')\n",
    "            obs_year_data_extended = obs_year_data_extended.chunk(best_chunks)\n",
    "            if show_debug and not shown_once: \n",
    "                print(\"Final merged observed data: \", obs_year_data_extended, '\\n')\n",
    "                print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "            ## Threshold padding\n",
    "            data_pieces_thresh = []\n",
    "\n",
    "            # We append previous year data if it exists first\n",
    "            if prev_year_data is not None:\n",
    "                data_pieces_thresh.append(thresh_end_days)\n",
    "                if show_debug and not shown_once: print('\"Previous\" Year Threshold Data:\\n', thresh_end_days, '\\n')  # doys with smoothwidth length up to 366\n",
    "\n",
    "            # We then append current year data\n",
    "            data_pieces_thresh.append(thresh_aligned)\n",
    "\n",
    "            # We lastly append next year data if it exists\n",
    "            if next_year_data is not None:\n",
    "                data_pieces_thresh.append(thresh_start_days)\n",
    "                if show_debug and not shown_once: print('\"Next\" Year Threshold Data:\\n', thresh_start_days, '\\n') # doy 1 up to the end of smoothwidth doys\n",
    "\n",
    "            if show_debug and not shown_once: print(\"Data pieces thresh (prev + full current + next):\\n\", data_pieces_thresh, '\\n\\n')\n",
    "\n",
    "            # Since we cannot sort by time, order matters most here!\n",
    "            thresh_data_extended = xr.concat(data_pieces_thresh, dim='normalized_doy')\n",
    "            thresh_data_extended = thresh_data_extended.chunk(best_chunks)\n",
    "            if show_debug and not shown_once: \n",
    "                print(\"Final Merged Threshold Data:\\n\", thresh_data_extended, '\\n')\n",
    "                print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "            ## Creating continuous coordinates (but only if the obs and thresh datasets have matching days of the year)\n",
    "            if obs_year_data_extended.normalized_doy.equals(thresh_data_extended.normalized_doy):\n",
    "                if show_debug and not shown_once: \n",
    "                    print(\"Aligned observed and padded thresh datasets' normalized_doys match!\\n\")\n",
    "                    print(\"Creating a new, continous coordinate for each for marine heatwave detection...\\n\")\n",
    "\n",
    "                obs_time_aligned = create_continuous_time_coordinate(obs_year_data_extended)\n",
    "                thresh_time_aligned = create_continuous_time_coordinate(thresh_data_extended)\n",
    "\n",
    "                if show_debug and not shown_once:\n",
    "                    print(\"Continuous Observed: \", '\\n', obs_time_aligned, '\\n\\n',\n",
    "                          \"Continuous Thresh: \", '\\n', thresh_time_aligned, '\\n')\n",
    "            else:\n",
    "                print(\"ERROR DETECTED! PRINTING RELEVANT OUTPUT:\\n\")\n",
    "                print(obs_year_data_extended.normalized_doy, '\\n\\n', thresh_data_extended.normalized_doy, '\\n')\n",
    "                print(obs_year_data_extended.normalized_doy.values, '\\n',\n",
    "                     thresh_data_extended.normalized_doy.values)\n",
    "\n",
    "                raise ValueError(\"Unexpected Error Detected: Coordinate arrays of observed and threshold datasets do not match exactly; please debug!\")\n",
    "\n",
    "            if show_debug and not shown_once: print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "            ## Exceedence (marine heatwave) mask labeling for a series of connected events\n",
    "\n",
    "            # To be able to use the scipy.ndimage.label function properly to detect events that last over year-end boundaries, \n",
    "            # it is crucial to create a continuous coordinate first (done in previous section) and run it through the custom function.\n",
    "\n",
    "            # Exceedence bool mask creation (for marine heatwaves)\n",
    "            exceed = (obs_time_aligned > thresh_time_aligned) # initial check if the observed temps are greater than their 90th percentiles\n",
    "            exceed = exceed.fillna(False) # Replace NaNs with False   \n",
    "\n",
    "            # Applying mhw event series labeling over the lat-lon grid\n",
    "            events_gaps_filled = xr.apply_ufunc(\n",
    "                event_gap_filling,\n",
    "                exceed,\n",
    "                input_core_dims=[['time_continuous']],\n",
    "                output_core_dims=[['time_continuous']],\n",
    "                vectorize=True,\n",
    "                dask='parallelized',\n",
    "                output_dtypes=[int],\n",
    "                kwargs={'maxGap': maxGapDaysInMhw, 'minDuration':minDaysMhwDuration}, \n",
    "                dask_gufunc_kwargs={\"output_sizes\": {\"time_continuous\": exceed.sizes[\"time_continuous\"]}},\n",
    "            )\n",
    "\n",
    "            events_gaps_filled = events_gaps_filled.transpose('time_continuous','latitude','longitude', 'depth')\n",
    "            events_gaps_filled = events_gaps_filled.swap_dims({'time_continuous':'normalized_doy'})\n",
    "\n",
    "            if show_debug and not shown_once: print(\"Event-labelled, gap-filled series: \", events_gaps_filled, '\\n')\n",
    "\n",
    "            # We run the MHW test with padded data, to ensure we correctly identify MHWs within the full current year period\n",
    "            if mhw_test:\n",
    "                run_mhw_test(exceed, events_gaps_filled, lat_val=mhw_test_lat, lon_val=mhw_test_lon, depth_val=mhw_test_depth)\n",
    "\n",
    "            # Now, we remove the padding\n",
    "            prev_yr_slice = smoothWidth if (prev_year_data is not None) else 0\n",
    "            next_yr_slice = -smoothWidth if (next_year_data is not None) else None\n",
    "\n",
    "            events_gaps_filled_unpadded = events_gaps_filled.isel(normalized_doy=slice(prev_yr_slice, next_yr_slice))\n",
    "            if show_debug and not shown_once: print(\"Marine Heatwave Events Bool Dataset, Unpadded: \", events_gaps_filled_unpadded, '\\n')\n",
    "\n",
    "            mhw_bool_final = events_gaps_filled_unpadded.drop_vars('time_continuous').chunk({'normalized_doy':-1})\n",
    "            if show_debug and not shown_once: \n",
    "                print(\"Final Marine Heatwave Events Bool Dataset: \", mhw_bool_final, '\\n')\n",
    "                print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "            # Rechunk the observed dataset\n",
    "            obs_aligned = obs_aligned.swap_dims({'time':'normalized_doy'}).drop_vars('time').chunk(best_chunks)\n",
    "            if show_debug and not shown_once: \n",
    "                print(\"Aligned observed data:\\n\", obs_aligned, '\\n')\n",
    "                print(\"Aligned thresh data:\\n\", thresh_aligned, '\\n')\n",
    "                print(\"Aligned clim data:\\n\", clim_aligned, '\\n\\n')\n",
    "\n",
    "            ## SEVERITY DENOM (PC90 - CLIM)\n",
    "            sev_denom = thresh_aligned - clim_aligned\n",
    "            depth_wtd_sev_denom = sev_denom.weighted(layer_thickness).mean(dim='depth')\n",
    "            \n",
    "            mhw_sev_denom = xr.where(mhw_bool_final, thresh_aligned - clim_aligned, np.nan).chunk({'normalized_doy': -1})\n",
    "            depth_wtd_mhw_sev_denom = mhw_sev_denom.weighted(layer_thickness).mean(dim='depth')\n",
    "            \n",
    "            ## SEVERITY NUM (OBS - CLIM)  \n",
    "            sev_num = obs_aligned - clim_aligned\n",
    "            depth_wtd_sev_num = sev_num.weighted(layer_thickness).mean(dim='depth')\n",
    "            \n",
    "            mhw_sev_num = xr.where(mhw_bool_final, obs_aligned - clim_aligned, np.nan).chunk({'normalized_doy': -1})\n",
    "            depth_wtd_mhw_sev_num = mhw_sev_num.weighted(layer_thickness).mean(dim='depth')\n",
    "\n",
    "            ## SEVERITY (NUM/DENOM)\n",
    "            severity = depth_wtd_sev_num / depth_wtd_sev_denom\n",
    "            severity = severity.chunk(best_chunks_no_depth)\n",
    "            \n",
    "            mhw_severity = depth_wtd_mhw_sev_num / depth_wtd_mhw_sev_denom\n",
    "            mhw_severity = mhw_severity.chunk(best_chunks_no_depth)\n",
    "            \n",
    "            if show_debug: \n",
    "                print(\"Depth-Weighted Severity [(OBS - CLIM) / (PC90 - CLIM)]:\\n\", severity, '\\n')\n",
    "                print(\"Depth-Weighted Severity (only for marine heatwaves):\\n\", mhw_severity, '\\n')\n",
    "                print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "            # Reassigning time\n",
    "            if len(obs_aligned_time) == severity.sizes['normalized_doy']:\n",
    "                # We rename the mhw labeled events dataset for merging\n",
    "                mhw_sev_final = mhw_severity.rename(f'severity_mhw_{final_max_depth_val}')\n",
    "                sev_final = severity.rename(f'severity_{final_max_depth_val}')\n",
    "\n",
    "                # Merge the datasets\n",
    "                full_sev = xr.merge([sev_final, mhw_sev_final])\n",
    "\n",
    "                # If there is perfect alignment with our severity dataset, we assign the original time coordinate back to it!\n",
    "                severity_final = full_sev.assign_coords(time=('normalized_doy', obs_aligned_time.values))\n",
    "                severity_final = severity_final.swap_dims({'normalized_doy': 'time'}).drop_vars('normalized_doy').chunk({'time':-1})\n",
    "            else:\n",
    "                # If there's a dimension mismatch, we raise an error!\n",
    "                raise ValueError(f\"Time/normalized_doy dimension mismatch detected for computation of severity for the {year} year!\")\n",
    "            \n",
    "            if show_debug and not shown_once:\n",
    "                shown_once = True\n",
    "                \n",
    "                \n",
    "            ## SAVING\n",
    "            print(f\"Starting Save of {baseline_name} {year} Severity Dataset (Averaged Up to a Depth of {final_max_depth_val} m)...\\n\")\n",
    "\n",
    "            # for filepath setup\n",
    "            def safe_float_str(x):\n",
    "                s = str(x)\n",
    "                return s.replace('.', '-') # to adjust the target radius for non whole integer values\n",
    "            \n",
    "            # Setting up the destination filepath\n",
    "            id_path = f\"{folder_name}_{sub_folder_name}\"\n",
    "            filename = f\"{id_path}_severity_300m_PC90th_{year}_depth_0_to_{safe_float_str(final_max_depth_val)}_{baseline_name}.zarr\"\n",
    "            sev_filepath = f'YOUR_DIRECTORY/Severity_300m/{folder_name}/{id_path}/{filename}'\n",
    "            print(\"File Path: \", sev_filepath, '\\n')\n",
    "\n",
    "            # Last-minute chunking to not hit the maximum buffer size for chunks\n",
    "            if len(severity_final.time.values) == 365:\n",
    "                severity_final = severity_final.chunk({'time':73})\n",
    "            elif len(severity_final.time.values) == 366:\n",
    "                severity_final = severity_final.chunk({'time':61})\n",
    "            print(f\"{year} Severity:\\n\", severity_final, '\\n')\n",
    "\n",
    "            if show_debug:\n",
    "                raise ValueError(\"You have reached the end of the debug. To begin saving the data, set show_debug to False!\")\n",
    "            else:\n",
    "                with ProgressBar():\n",
    "                    severity_final.to_zarr(sev_filepath, mode='w')\n",
    "                \n",
    "                # Freeing up memory\n",
    "                severity_final.close()\n",
    "                del severity_final\n",
    "                gc.collect()\n",
    "                \n",
    "                print(\"Done saving! Moving along!\", '\\n')\n",
    "\n",
    "            print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "        \n",
    "        print(f\"Finished saving the column-averaged severity for depths up to {final_max_depth_val} m for all years! Moving on to the next depth!\\n\")\n",
    "        print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    print(\"--------------------o-------------------------------o------------------------------o----------------------------\", '\\n')\n",
    "    print(\"Finished saving data for all years!\")\n",
    "    stop_monitoring = True\n",
    "       \n",
    "        \n",
    "best_chunk_configs = {\n",
    "    \"Atlantic\": {'depth': -1, 'normalized_doy':-1, 'latitude': 109, 'longitude': 242},\n",
    "}\n",
    "\n",
    "post_thickness_configs = {\n",
    "    \"Atlantic\": {'normalized_doy':-1, 'latitude': 109, 'longitude': 242},\n",
    "}\n",
    "\n",
    "# if you get warnings about large chunks, use smaller chunks in previous step were data combined, or adjust best chunks accordingly\n",
    "\n",
    "# pick your MINIMUM target depth; the column-average of the severity you obtain lies between the minimum depth and the next depth\n",
    "target_depths_list = [#266, \n",
    "                      #47, \n",
    "                      #92, \n",
    "                      130, # for example, 130 is the lower bound, the max (next) bound was ~156, and the final depth it goes up to is ~143\n",
    "                      \n",
    "] \n",
    "\n",
    "optimal_chunks = best_chunk_configs[\"Atlantic\"]\n",
    "sev_optimal_chunks = post_thickness_configs[\"Atlantic\"]\n",
    "\n",
    "calculate_severity(globals()[f'{folder_filename}_obs'], \n",
    "                   globals()[f'{folder_filename}_thresh'].drop_vars('quantile'), \n",
    "                   globals()[f'{folder_filename}_clim'], \n",
    "                   \"Baseline9322\", \"Atlantic\", \"Full\", \n",
    "                   optimal_chunks, sev_optimal_chunks, \n",
    "                   depths_list=target_depths_list, depth_thicknesses = all_depth_levels,\n",
    "                   minutes_per_mem_update=20, show_debug=False, \n",
    "                   custom_years=True, start_yr=2000, end_yr=2002,\n",
    "                   mhw_test=False)\n",
    "# For testing the detection to ensure everything is working as follows, you can add code like these for a single point:\n",
    "# mhw_test=True, mhw_test_lat=25, mhw_test_lon=-40, mhw_test_depth=50)  \n",
    "\n",
    "stop_monitoring = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8831e8f3-f47b-4d32-b02f-2ac617689bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual stop to the memory printing function above\n",
    "stop_monitoring = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangeo23",
   "language": "python",
   "name": "pangeo23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
