{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63adab-d7ea-4d07-9288-1e572bd42d79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import psutil\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "import copernicusmarine\n",
    "from copernicusmarine import get\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.animation as animation\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ea5ef-0dd5-4416-9720-27f6da36d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Important script-wide constants\n",
    "start_val_arg = 1 # your percentile and mean datasets' starting day of the year (doy) value (both must have this)\n",
    "end_val_arg = 366 # your percentile and mean datasets' ending day of the year (doy) value (both must have this)\n",
    "baseline_name_arg = \"Baseline9322\" # your chosen baseline identifier for the 30-year baseline\n",
    "current_percentile = 90 # your chosen percentile (as a percent, not under 1.0)\n",
    "custom_id_choice = \"fgd\" # your custom identifier used in both mean and percentile datasets \n",
    "my_root_directory = \"\" # Should be your root directory, from which you access data from and save data to\n",
    "# See sst_climatology_and_percentile_calculator.ipynb for your used values (or consult your stored datasets)\n",
    "dataset_id = \"\" # The name of the dataset folder, which should contain your raw data, means, and percentiles calculed with the climatology script\n",
    "\n",
    "\n",
    "# Data directories that can be further modified by you may be identified by searching for \n",
    "# \"NOTE: POTENTIAL DIRECTORY TWEAKING HERE\" in this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8de336-9ead-409f-8cd6-61ba6da73438",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading SST data ---------------------------------------------------------------------------------\n",
    "def normalize_dayofyear(time_coord):\n",
    "    doy = time_coord.dt.dayofyear\n",
    "    is_leap = time_coord.dt.is_leap_year\n",
    "\n",
    "    # This code ensures March 1 is always day 61, regardless of leap year\n",
    "    normalized_doy = xr.where(\n",
    "        (~is_leap) & (doy >= 60),  # If it is a non-leap year, doy 60 is March 1. If we have March 1 or later,\n",
    "        doy + 1,                          # then we push forward March 1 and/or the later days by 1 day.\n",
    "        doy                               # Otherwise, we keep original for leap years and Jan-Feb 28.\n",
    "    )\n",
    "    return normalized_doy\n",
    "\n",
    "# Your filepath here; this is my setup.\n",
    "raw_data_directory = f'{my_root_directory}/{dataset_id}/Data/sst.day.mean.*.nc'\n",
    "\n",
    "# Sometimes this will result in a crash the first time. Wait a bit and run the cell again...\n",
    "ds = xr.open_mfdataset(\n",
    "    raw_data_directory,         # Glob pattern\n",
    "    parallel=True,              # Enable parallel file opening \n",
    "    chunks='auto',              # Let Dask choose optimal chunking\n",
    "    combine='by_coords',        # Merge based on coordinate values\n",
    "    engine='netcdf4'            # Specify the engine (it may crash otherwise; if it still does, restart the kernel)\n",
    ")\n",
    "\n",
    "optimal_chunking = {'lat': 210, 'lon': 160} # for the raw data\n",
    "optimal_chunks = {'normalized_doy':-1, 'lat': 210, 'lon': 160} # for the to-be-created severity data\n",
    "full_ds = ds.sel(lat=slice(-15, 90), lon=slice(0, 360)).sst\n",
    "full_ds = full_ds.chunk(optimal_chunking)\n",
    "Full_obs = full_ds.assign_coords(\n",
    "        normalized_doy=('time', normalize_dayofyear(full_ds.time).data))\n",
    "print(\"Observed data:\\n\", Full_obs, '\\n')\n",
    "\n",
    "## Loading constants\n",
    "folder_name_arg = \"Full\"\n",
    "\n",
    "custom_name_arg = \"fgd\"\n",
    "if custom_name_arg != \"\":\n",
    "    id = f\"{custom_name_arg}_\"\n",
    "else:\n",
    "    id =\"\"\n",
    "    \n",
    "## Loading climatological means data \n",
    "file_name = f\"{folder_name_arg}_{id}sst_clim_subset_{start_val_arg}_to_{end_val_arg}_{baseline_name_arg}.zarr\"\n",
    "filepath = f'{my_root_directory}/{dataset_id}/Clim/{baseline_name_arg}/{file_name}' # NOTE: POTENTIAL DIRECTORY TWEAKING HERE\n",
    "Full_means = xr.open_zarr(filepath).sst\n",
    "print(\"Clim. Means:\\n\", Full_means, '\\n')\n",
    "\n",
    "## Loading percentile threshold data\n",
    "file_name = f\"{folder_name_arg}_{id}sst_thresh_subset_{start_val_arg}_to_{end_val_arg}_{baseline_name_arg}.zarr\"\n",
    "filepath = f'{my_root_directory}/{dataset_id}/Thresh{current_percentile}th/{baseline_name_arg}/{file_name}' # NOTE: POTENTIAL DIRECTORY TWEAKING HERE\n",
    "Full_thresh = xr.open_zarr(filepath).sst\n",
    "print(\"Percentile Thresholds:\\n\", Full_thresh, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cef78e4-7658-4c63-88f3-8e6cd7f9b7b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to calculate column-averaged severity\n",
    "def get_max_and_min_years(ds):\n",
    "    min_yr = ds.time[0].dt.year.item()\n",
    "    max_yr = ds.time[-1].dt.year.item()\n",
    "    return min_yr, max_yr\n",
    "\n",
    "\n",
    "def check_valid_type(data, data_type, data_name, correction_message):\n",
    "    if not isinstance(data, data_type):\n",
    "        raise ValueError(f\"Invalid {data_name} provided. Please provide a proper {data_type.__name__} {data_name}, or {correction_message}.\")\n",
    "\n",
    "\n",
    "def event_gap_filling(events_series: np.ndarray, minDuration: int = 5, maxGap: int = 2) -> np.ndarray:\n",
    "    '''\n",
    "    Gap-filling function for marine heatwave (MHW) events following the Hobday et al. (2016) definition.\n",
    "    \n",
    "    Key rules:\n",
    "    1. Only events of 5+ (or minDuration+) days duration are considered valid MHWs\n",
    "    2. Gaps of 2 (or maxGap) days or less between any valid events should merge them\n",
    "    3. When merging occurs, the entire merged period becomes one event\n",
    "    4. The resulting event must still meet the 5-day minimum duration\n",
    "    \n",
    "    Examples from Hobday et al. (2016):\n",
    "    - [5hot, 2cool, 6hot] → 13-day event (5 + 2 + 6 = 13)\n",
    "    - [5hot, 1cool, 2hot] → 5-day event (only first 5 days qualify)\n",
    "    - [2hot, 1cool, 5hot] → 5-day event (only last 5 days qualify)\n",
    "    - [5hot, 4cool, 6hot] → two separate events (5-day + 6-day)\n",
    "    '''\n",
    "    \n",
    "    # Input validation and conversion\n",
    "    assert events_series.ndim == 1, f\"Expected 1D series, got {events_series.ndim}D\"\n",
    "\n",
    "    # We convert the dataset to boolean if it is not boolean\n",
    "    if events_series.dtype != bool:\n",
    "        print(\"Dataset conversion occurred!\", '\\n')\n",
    "        events_series = events_series.astype(bool)\n",
    "\n",
    "    # If the input series is full of False values, we return an all 0's array\n",
    "    if not np.any(events_series):\n",
    "        return np.zeros_like(events_series, dtype=int)\n",
    "    \n",
    "    ## First, we find all consecutive True sequences that are ≥ minDuration\n",
    "    # We initialize our empty list that stores all the valid sequences ≥ the minDuration value\n",
    "    valid_sequences = []\n",
    "    i = 0\n",
    "\n",
    "    # Main loop that examines every position in the series\n",
    "    while i < len(events_series):\n",
    "        # If we identify a True value, we check for a potential marine heatwave event\n",
    "        if events_series[i]:\n",
    "            # We save the starting point\n",
    "            start_idx = i\n",
    "            \n",
    "            # We advance over a series of consecutive True values (until reaching the end of the array or a False value)\n",
    "            while i < len(events_series) and events_series[i]:\n",
    "                i += 1\n",
    "\n",
    "            # We save the end value and duration of our series accordingly\n",
    "            end_idx = i - 1 \n",
    "            duration = end_idx - start_idx + 1 # we add 1 since indeces are 0-based\n",
    "\n",
    "            # If the minimumDuration is met, we store the start and end indeces as a tuple in our list\n",
    "            if duration >= minDuration:\n",
    "                valid_sequences.append((start_idx, end_idx))\n",
    "                \n",
    "        else: # If the current position if a False day, we move along\n",
    "            i += 1\n",
    "\n",
    "    # If no valid sequences exist, we return a 0-value array\n",
    "    if not valid_sequences:\n",
    "        return np.zeros_like(events_series, dtype=int)\n",
    "    \n",
    "    ## Next, we group valid event sequences by checking gaps between them\n",
    "    # We initialize an empty list to store events that should be merged\n",
    "    event_groups = []\n",
    "    current_group = [valid_sequences[0]] # sets the current group to check\n",
    "\n",
    "    # We loop through all valid sequences (starting from the second one), comparing each with the previous\n",
    "    for i in range(1, len(valid_sequences)):\n",
    "        \n",
    "        prev_end = current_group[-1][1] # we extract the end index ([1]) of the last sequence in the current_group ([-1])\n",
    "        curr_start = valid_sequences[i][0] # we extract the start index ([0]) of the current valid_sequence checked ([i])\n",
    "\n",
    "        # We calculate the number of days between the current event's start index and the previous event's end index\n",
    "        gap_length = curr_start - prev_end - 1\n",
    "\n",
    "        # If the gap is <= the maxGap value, we save/merge (via append) the current valid sequence to/with the current group\n",
    "        if gap_length <= maxGap:\n",
    "            current_group.append(valid_sequences[i])\n",
    "        # If the gap is not <= the maxGap value, we add the untouched current group back to the merged event list and reset the current group\n",
    "        else:\n",
    "            event_groups.append(current_group)\n",
    "            current_group = [valid_sequences[i]]\n",
    "\n",
    "    # We add the last group to the merged event group list (manually)\n",
    "    event_groups.append(current_group)\n",
    "    \n",
    "    ## Finally, we create our output with appropriate event IDs\n",
    "    # We begin with an all 0's array\n",
    "    out = np.zeros_like(events_series, dtype=int)\n",
    "\n",
    "    # We iterate through all the groups in the merged group list\n",
    "    for event_id, group in enumerate(event_groups, 1):\n",
    "        # We find the overall start and end index for each group \n",
    "        group_start = group[0][0]\n",
    "        group_end = group[-1][1]\n",
    "\n",
    "        # We assign a unique event id for all values from group start to the group end indeces (inclusive)\n",
    "        out[group_start:group_end + 1] = event_id\n",
    "\n",
    "    # We return our output\n",
    "    return out\n",
    "        \n",
    "    \n",
    "# Create a continous time coordinate for a passed dataset\n",
    "def create_continuous_time_coordinate(ds):\n",
    "    n_timesteps = len(ds.normalized_doy)\n",
    "    continuous_time = np.arange(1, n_timesteps + 1)\n",
    "    \n",
    "    # Add the continuous coordinate and make it the dominant coordinate\n",
    "    ds = ds.assign_coords(time_continuous=('normalized_doy', continuous_time))\n",
    "    ds = ds.swap_dims({'normalized_doy':'time_continuous'}).chunk({'time_continuous':-1})\n",
    "    return ds\n",
    "\n",
    "\n",
    "def run_mhw_test(initial_mask_ds, final_mask_ds, lat_val=None, lon_val=None):\n",
    "    # Quick check to ensure values were inputted\n",
    "    if any(val is None for val in (lat_val, lon_val)):\n",
    "        raise ValueError(\"Missing a key argument in the run_mhw_test function. Please ensure a valid integer value is provided for each value argument!\")\n",
    "    \n",
    "    print(\"--------------------------------------------------   TEST START   ---------------------------------------------------\\n\")\n",
    "    print(f\"RUNNING TEST FOR NEAREST LATITUDE: {lat_val}, LONGITUDE: {lon_val}.\\n\")\n",
    "    \n",
    "    # Grab a subset from the initial_mask_ds\n",
    "    subset_original = initial_mask_ds.sel(lat=lat_val, lon=lon_val, method='nearest')\n",
    "    real_lat = subset_original.lat.values.item()\n",
    "    real_lon = subset_original.lon.values.item()\n",
    "    print(f\"ACTUAL VALUES DETECTED:\\nLATITUDE: {real_lat}, LONGITUDE: {real_lon}\\n\")\n",
    "    \n",
    "    original_labeled = subset_original.values\n",
    "    print(\"Exceed values:\\n\", original_labeled, '\\n\\n')\n",
    "\n",
    "    events_labeled = final_mask_ds.sel(lat=lat_val, lon=lon_val, method='nearest').values\n",
    "    print(\"Event gap-filled values:\\n\", events_labeled, '\\n\\n')\n",
    "\n",
    "    print(\"----------o-----------0----------o-------------\\n\")\n",
    "\n",
    "    length = min(len(original_labeled), len(events_labeled))\n",
    "    step = 10\n",
    "\n",
    "    for i in range(0, length, step):\n",
    "        end_idx = min(i + step, length)\n",
    "        print(f\"Values {i} to {end_idx - 1}:\")\n",
    "        print(\"Exceed values:\", original_labeled[i:end_idx])\n",
    "        print(\"Event gap-filled values:\", events_labeled[i:end_idx])\n",
    "        print(\"-\" * 60)\n",
    "        print(\" \")\n",
    "    print(\"--------------------------------------------------   END OF TEST   ---------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "stop_monitoring = True # We begin by NOT showing any memory usage\n",
    "def monitor_memory(interval_minutes=5, log_file=None):\n",
    "    interval = interval_minutes * 60  \n",
    "    \n",
    "    while not stop_monitoring:\n",
    "        mem = psutil.Process(os.getpid()).memory_info().rss / (1024**3)  # in GB\n",
    "        print(f\" | Memory usage: {mem:.2f} GB | Memory: {psutil.virtual_memory().percent}% used | \")\n",
    "        \n",
    "        if log_file:\n",
    "            with open(log_file, 'a') as f:\n",
    "                f.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')}: {mem:.2f} GB\\n\")\n",
    "        time.sleep(interval)\n",
    "    \n",
    "    \n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "def calculate_severity(obs_data, thresh_data, clim_data, \n",
    "                       baseline_name, folder_name, custom_id,\n",
    "                       best_chunks,\n",
    "                       smoothWidth=31, percentile=90,\n",
    "                       minDaysMhwDuration=5, maxGapDaysInMhw=2, \n",
    "                       minutes_per_mem_update=5,\n",
    "                       show_debug=True,\n",
    "                       custom_years=False, start_yr=None, end_yr=None,\n",
    "                       mhw_test=False, mhw_test_lat=None, mhw_test_lon=None):\n",
    "\n",
    "    # Quick check for inputs regarding a marine heatwave test a specific coordinate\n",
    "    if mhw_test:\n",
    "        check_valid_type(mhw_test_lat, int, \"latitude value\", \"set mhw_test to False\")\n",
    "        check_valid_type(mhw_test_lon, int, \"longitude value\", \"set mhw_test to False\")\n",
    "    \n",
    "    # Gather the min and max years in the observed data\n",
    "    min_obs_yr, max_obs_yr = get_max_and_min_years(obs_data)\n",
    "\n",
    "    # Setting up custom years, or going with the default (min to max years in the observed data)\n",
    "    if custom_years:\n",
    "        check_valid_type(start_yr, int, \"starting year\", \"set custom_years to False\")\n",
    "        check_valid_type(end_yr, int, \"ending year\", \"set custom_years to False\")\n",
    "        years = range(start_yr, end_yr + 1)\n",
    "    else:\n",
    "        years = range(min_obs_yr, max_obs_yr + 1)\n",
    "\n",
    "    # Set up a variable to control whether a second smoothing (of the climatological means/percentiles) is applied\n",
    "    apply_smoothing = False if smoothWidth == 0 else True\n",
    "    shown_once = False # for debugging purposes\n",
    "\n",
    "    # Set up the id to save the dataset with, if desired\n",
    "    if custom_id != \"\":\n",
    "        id = f\"{custom_id}_\"\n",
    "    else:\n",
    "        id =\"\"\n",
    "    \n",
    "    # Beginning of code for severity calculation:\n",
    "    print(\"--------------------o-------------------------------o------------------------------o----------------------------\")\n",
    "    print(f'\\nBaseline used: {baseline_name}\\n') # We only have one baseline\n",
    "    print(f\"The smoothWidth argument was set to: {smoothWidth}.\")\n",
    "    \n",
    "    if apply_smoothing:\n",
    "        print(f\"This means a rolling window of {smoothWidth} doys centered on each individual doy is used.\")\n",
    "        print(f\"For each doy, the rolling window contains the previous {(smoothWidth-1)/2} doys, the center doy, and the next {(smoothWidth-1)/2} doys.\\n\")\n",
    "    else:\n",
    "        print(\"No smoothing was applied; using the threshold and mean datasets as is!\\n\")\n",
    "\n",
    "    # Showing the original datasets if show_debug is enabled\n",
    "    if show_debug: \n",
    "        print('Original raw, \"observed\" data:\\n', obs_data, '\\n')\n",
    "        print(\"Original percentile threshold data:\\n\", thresh_data, '\\n')\n",
    "        print(\"Original climatological means data:\\n\", clim_data, '\\n')\n",
    "        print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "    if apply_smoothing:\n",
    "        ## PADDING AND SMOOTHING\n",
    "        # Padding with smoothWidth (which is more than enough for the desired smoothing with smoothWidth)\n",
    "        padded_clim = clim_data.pad(normalized_doy=smoothWidth, mode='wrap')\n",
    "        if show_debug: print(\"Padded Means:\\n\", padded_clim, '\\n')\n",
    "    \n",
    "        padded_thresh = thresh_data.pad(normalized_doy=smoothWidth, mode='wrap')\n",
    "        if show_debug: print(\"Padded Threshold:\\n\", padded_thresh, '\\n\\n')\n",
    "    \n",
    "        # Smoothing\n",
    "        clim_smoothed = padded_clim.rolling(normalized_doy=smoothWidth, center=True, min_periods=smoothWidth).mean() \n",
    "        if show_debug: print(f\"Padded Means Smoothed by {smoothWidth} doys:\\n\", clim_smoothed, '\\n')\n",
    "    \n",
    "        clim_processed = clim_smoothed.isel(normalized_doy=slice(smoothWidth, -smoothWidth))\n",
    "        if show_debug: print(\"Smoothed Means (No Pad):\\n\", clim_processed, '\\n\\n')\n",
    "    \n",
    "        thresh_smoothed = padded_thresh.rolling(normalized_doy=smoothWidth, center=True, min_periods=smoothWidth).mean()\n",
    "        if show_debug: print(f\"Padded Threshold Smoothed by {smoothWidth} doys: \", thresh_smoothed, '\\n')\n",
    "    \n",
    "        thresh_processed = thresh_smoothed.isel(normalized_doy=slice(smoothWidth, -smoothWidth))\n",
    "        if show_debug: print(\"Threshold Smoothed (No Pad): \", thresh_processed, '\\n')\n",
    "\n",
    "        \n",
    "        # We also save early and late smoothed doy data for late/early-year mhw detection later\n",
    "        # MAY NEED TO SET THESE ELSEWHERE IF USING A LARGER smoothWidth (if the smoothWidth extends past 31 to include the interpolated Feb 29)\n",
    "        doys_to_gather = smoothWidth # can be adjusted to your needs\n",
    "        thresh_prev_yr_days = thresh_smoothed.isel(normalized_doy=slice(0, doys_to_gather)).copy()\n",
    "        thresh_next_yr_days = thresh_smoothed.isel(normalized_doy=slice(-doys_to_gather, None)).copy()\n",
    "        \n",
    "    elif not apply_smoothing:\n",
    "        # We still save early and late raw threshold doy data for late/early-year mhw detection later\n",
    "        doys_to_gather = 15 # additional days to append to the dataset for year-round mhw detection (should be greater than 6)\n",
    "        thresh_prev_yr_days = thresh_data.isel(normalized_doy=slice(0, doys_to_gather)).copy()\n",
    "        thresh_next_yr_days = thresh_data.isel(normalized_doy=slice(-doys_to_gather, None)).copy()\n",
    "        \n",
    "        # We also run the prior raw datasets through these new variables\n",
    "        thresh_processed = thresh_data\n",
    "        clim_processed = clim_data\n",
    "\n",
    "    if show_debug:\n",
    "        print(f'\\n\"Next Year\" Threshold Dataset Doys:\\n{thresh_next_yr_days}\\n')\n",
    "        print(f'\"Previous Year\" Threshold Dataset Doys:\\n{thresh_prev_yr_days}\\n')\n",
    "\n",
    "    \n",
    "    # Quick monitoring initiation\n",
    "    global stop_monitoring\n",
    "    if show_debug:\n",
    "        stop_monitoring = True # We don't want to start showing memory use.\n",
    "        print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "    else:\n",
    "        # We start monitoring here so that it only runs once\n",
    "        stop_monitoring = False # We do want to start showing memory use.\n",
    "        monitor_thread = threading.Thread(target=monitor_memory, kwargs={'interval_minutes': minutes_per_mem_update})\n",
    "        monitor_thread.daemon = True\n",
    "        monitor_thread.start()\n",
    "\n",
    "    ## CALCULATING ANOMALIES BY YEAR (observed - climatological means) AND DETECTING MHWS\n",
    "    for year in years:\n",
    "        # Time subsetting\n",
    "        obs_year_data = obs_data.sel(time=f'{year}')\n",
    "        obs_year_data_norm = obs_year_data.swap_dims({'time':'normalized_doy'})\n",
    "        if show_debug and not shown_once: print(f\"Observed data for {year}:\\n\", obs_year_data_norm, '\\n')\n",
    "\n",
    "        # Aligning the datasets (returns doys common to both)\n",
    "        obs_aligned, clim_aligned = xr.align(obs_year_data_norm, clim_processed, join=\"inner\")\n",
    "        clim_aligned = clim_aligned.chunk(best_chunks)\n",
    "\n",
    "        obs_aligned, thresh_aligned = xr.align(obs_year_data_norm, thresh_processed, join=\"inner\")\n",
    "        thresh_aligned = thresh_aligned.chunk(best_chunks)\n",
    "\n",
    "        if show_debug and not shown_once: \n",
    "            print(f\"Thresh aligned: \", thresh_aligned, '\\n')\n",
    "            print(f\"Clim aligned: \", clim_aligned, '\\n')\n",
    "\n",
    "        # We also save the time variable separately for later, then drop it from the observed\n",
    "        obs_aligned_time = obs_aligned.swap_dims({'normalized_doy':'time'}).drop_vars('normalized_doy').time\n",
    "        if show_debug and not shown_once: print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "        ### DETECTING MHWS ACROSS THE YEARS\n",
    "        ## Grabbing previous and next year data\n",
    "        obs_aligned = obs_aligned.swap_dims({'normalized_doy':'time'})\n",
    "        if show_debug and not shown_once: print(f\"Full Year Data from the Current Year ({year}):\\n\", obs_aligned, '\\n')\n",
    "\n",
    "        # Important objects for padding\n",
    "        prev_year_data = None\n",
    "        next_year_data = None\n",
    "\n",
    "        # We grab the previous year's data if it is not the first or final year in the dataset\n",
    "        if year > min_obs_yr:\n",
    "            prev_year = year - 1\n",
    "            prev_year_full = obs_data.sel(time=f'{prev_year}')\n",
    "\n",
    "            # If it exists, we grab the previous year's final period of length smoothWidth (up to 366)\n",
    "            if len(prev_year_full) > 0:\n",
    "                prev_year_data = prev_year_full.isel(time=slice(-doys_to_gather, None))\n",
    "                if show_debug and not shown_once: print(f\"End of the Year Data from the Previous Year ({prev_year}):\\n\", prev_year_data, '\\n')\n",
    "\n",
    "        # We grab the next year's data if it is not the first or final year in the dataset\n",
    "        if year < max_obs_yr:\n",
    "            next_year = year + 1\n",
    "            next_year_full = obs_data.sel(time=f'{next_year}')\n",
    "\n",
    "            if len(next_year_full) > 0:\n",
    "                next_year_data = next_year_full.isel(time=slice(0, doys_to_gather))\n",
    "                if show_debug and not shown_once: print(f\"Beginning of the Year Data from the Next Year ({next_year}):\\n\", next_year_data, '\\n')\n",
    "\n",
    "        ## Merging the previous and next year datasets, if they are available\n",
    "        data_pieces = [] \n",
    "\n",
    "        # We first append the previous year data to our array, if it exists\n",
    "        if prev_year_data is not None: data_pieces.append(prev_year_data)\n",
    "\n",
    "        # Next, we append the current year data to our array, if it exists\n",
    "        data_pieces.append(obs_aligned)\n",
    "\n",
    "        # Lastly, we append the next year data to our array, if it exists\n",
    "        if next_year_data is not None: data_pieces.append(next_year_data)\n",
    "\n",
    "        if show_debug and not shown_once: print(\"Data pieces (prev + full current + next):\\n\", data_pieces, '\\n\\n')\n",
    "\n",
    "        # Merging the datasets\n",
    "        obs_year_data_extended = xr.concat(data_pieces, dim='time')\n",
    "        obs_year_data_extended = obs_year_data_extended.sortby('time')\n",
    "        if show_debug and not shown_once: print(\"Initial merged observed data:\\n\", obs_year_data_extended, '\\n')\n",
    "\n",
    "        # We switch back to our desired normalized doy time dimension\n",
    "        obs_year_data_extended = obs_year_data_extended.swap_dims({'time':'normalized_doy'}).drop_vars('time')\n",
    "        obs_year_data_extended = obs_year_data_extended.chunk(best_chunks)\n",
    "        if show_debug and not shown_once: \n",
    "            print(\"Final merged observed data: \", obs_year_data_extended, '\\n')\n",
    "            print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "        ## Threshold padding\n",
    "        data_pieces_thresh = []\n",
    "\n",
    "        # We append previous year data if it exists first\n",
    "        if prev_year_data is not None:\n",
    "            data_pieces_thresh.append(thresh_prev_yr_days)\n",
    "            if show_debug and not shown_once: print('\"Previous\" Year Threshold Data:\\n', thresh_prev_yr_days, '\\n')  # doys with doys_to_gather length up to 366\n",
    "\n",
    "        # We then append current year data\n",
    "        data_pieces_thresh.append(thresh_aligned)\n",
    "\n",
    "        # We lastly append next year data if it exists\n",
    "        if next_year_data is not None:\n",
    "            data_pieces_thresh.append(thresh_next_yr_days)\n",
    "            if show_debug and not shown_once: print('\"Next\" Year Threshold Data:\\n', thresh_next_yr_days, '\\n') # doy 1 up to the end of doys_to_gather doys\n",
    "\n",
    "        if show_debug and not shown_once: print(\"Data pieces thresh (prev + full current + next):\\n\", data_pieces_thresh, '\\n\\n')\n",
    "\n",
    "        # Since we cannot sort by time, order matters most here!\n",
    "        thresh_data_extended = xr.concat(data_pieces_thresh, dim='normalized_doy')\n",
    "        thresh_data_extended = thresh_data_extended.chunk(best_chunks)\n",
    "        if show_debug and not shown_once: \n",
    "            print(\"Final Merged Threshold Data:\\n\", thresh_data_extended, '\\n')\n",
    "            print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "        ## Creating continuous coordinates (but only if the obs and thresh datasets have matching days of the year)\n",
    "        if obs_year_data_extended.normalized_doy.equals(thresh_data_extended.normalized_doy):\n",
    "            if show_debug and not shown_once: \n",
    "                print(\"Aligned observed and padded thresh datasets' normalized_doys match!\\n\")\n",
    "                print(\"Creating a new, continous coordinate for each for marine heatwave detection...\\n\")\n",
    "\n",
    "            obs_time_aligned = create_continuous_time_coordinate(obs_year_data_extended)\n",
    "            thresh_time_aligned = create_continuous_time_coordinate(thresh_data_extended)\n",
    "\n",
    "            if show_debug and not shown_once:\n",
    "                print(\"Continuous Observed: \", '\\n', obs_time_aligned, '\\n\\n',\n",
    "                      \"Continuous Thresh: \", '\\n', thresh_time_aligned, '\\n')\n",
    "        else:\n",
    "            print(\"ERROR DETECTED! PRINTING RELEVANT OUTPUT:\\n\")\n",
    "            print(obs_year_data_extended.normalized_doy, '\\n\\n', thresh_data_extended.normalized_doy, '\\n')\n",
    "            print(obs_year_data_extended.normalized_doy.values, '\\n',\n",
    "                 thresh_data_extended.normalized_doy.values)\n",
    "\n",
    "            raise ValueError(\"Unexpected Error Detected: Coordinate arrays of observed and threshold datasets do not match exactly; please debug!\")\n",
    "\n",
    "        if show_debug and not shown_once: print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "        ## Exceedence (marine heatwave) mask labeling for a series of connected events\n",
    "\n",
    "        # To be able to use the scipy.ndimage.label function properly to detect events that last over year-end boundaries, \n",
    "        # it is crucial to create a continuous coordinate first (done in previous section) and run it through the custom function.\n",
    "\n",
    "        # Exceedence bool mask creation (for marine heatwaves)\n",
    "        exceed = (obs_time_aligned > thresh_time_aligned) # initial check if the observed temps are greater than their 90th percentiles\n",
    "        exceed = exceed.fillna(False) # Replace NaNs with False   \n",
    "\n",
    "        # Applying mhw event series labeling over the lat-lon grid\n",
    "        events_gaps_filled = xr.apply_ufunc(\n",
    "            event_gap_filling,\n",
    "            exceed,\n",
    "            input_core_dims=[['time_continuous']],\n",
    "            output_core_dims=[['time_continuous']],\n",
    "            vectorize=True,\n",
    "            dask='parallelized',\n",
    "            output_dtypes=[int],\n",
    "            kwargs={'maxGap': maxGapDaysInMhw, 'minDuration':minDaysMhwDuration}, \n",
    "            dask_gufunc_kwargs={\"output_sizes\": {\"time_continuous\": exceed.sizes[\"time_continuous\"]}},\n",
    "        )\n",
    "\n",
    "        events_gaps_filled = events_gaps_filled.transpose('time_continuous','lat','lon')\n",
    "        events_gaps_filled = events_gaps_filled.swap_dims({'time_continuous':'normalized_doy'})\n",
    "\n",
    "        if show_debug and not shown_once: print(\"Event-labelled, gap-filled series: \", events_gaps_filled, '\\n')\n",
    "\n",
    "        # We run the MHW test with padded data, to ensure we correctly identify MHWs within the full current year period\n",
    "        if mhw_test:\n",
    "            run_mhw_test(exceed, events_gaps_filled, lat_val=mhw_test_lat, lon_val=mhw_test_lon)\n",
    "\n",
    "        # Now, we remove the padding\n",
    "        prev_yr_slice = doys_to_gather if (prev_year_data is not None) else 0\n",
    "        next_yr_slice = -doys_to_gather if (next_year_data is not None) else None\n",
    "\n",
    "        events_gaps_filled_unpadded = events_gaps_filled.isel(normalized_doy=slice(prev_yr_slice, next_yr_slice))\n",
    "        if show_debug and not shown_once: print(\"Marine Heatwave Events Bool Dataset, Unpadded: \", events_gaps_filled_unpadded, '\\n')\n",
    "\n",
    "        mhw_bool_final = events_gaps_filled_unpadded.drop_vars('time_continuous').chunk({'normalized_doy':-1})\n",
    "        if show_debug and not shown_once: \n",
    "            print(\"Final Marine Heatwave Events Bool Dataset: \", mhw_bool_final, '\\n')\n",
    "            print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "        # Rechunk the observed dataset\n",
    "        obs_aligned = obs_aligned.swap_dims({'time':'normalized_doy'}).drop_vars('time').chunk(best_chunks)\n",
    "        if show_debug and not shown_once: \n",
    "            print(\"Aligned observed data:\\n\", obs_aligned, '\\n')\n",
    "            print(\"Aligned thresh data:\\n\", thresh_aligned, '\\n')\n",
    "            print(\"Aligned clim data:\\n\", clim_aligned, '\\n\\n')\n",
    "\n",
    "        ## SEVERITY DENOM (PC90 - CLIM)\n",
    "        sev_denom = thresh_aligned - clim_aligned\n",
    "        mhw_sev_denom = xr.where(mhw_bool_final, thresh_aligned - clim_aligned, np.nan).chunk({'normalized_doy': -1})\n",
    "        \n",
    "        ## SEVERITY NUM (OBS - CLIM)  \n",
    "        sev_num = obs_aligned - clim_aligned\n",
    "        mhw_sev_num = xr.where(mhw_bool_final, obs_aligned - clim_aligned, np.nan).chunk({'normalized_doy': -1})\n",
    "\n",
    "        ## SEVERITY (NUM/DENOM)\n",
    "        severity = sev_num / sev_denom\n",
    "        severity = severity.chunk(best_chunks)\n",
    "        \n",
    "        mhw_severity = mhw_sev_num / mhw_sev_denom\n",
    "        mhw_severity = mhw_severity.chunk(best_chunks)\n",
    "        \n",
    "        if show_debug: \n",
    "            print(\"Severity [(OBS - CLIM) / (PC90 - CLIM)]:\\n\", severity, '\\n')\n",
    "            print(\"Severity (only for marine heatwaves):\\n\", mhw_severity, '\\n')\n",
    "            print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "        # Reassigning time\n",
    "        if len(obs_aligned_time) == severity.sizes['normalized_doy']:\n",
    "            # We rename the mhw labeled events dataset for merging\n",
    "            mhw_sev_final = mhw_severity.rename(f'severity_mhw')\n",
    "            sev_final = severity.rename(f'severity')\n",
    "\n",
    "            # Merge the datasets\n",
    "            full_sev = xr.merge([sev_final, mhw_sev_final])\n",
    "\n",
    "            # If there is perfect alignment with our severity dataset, we assign the original time coordinate back to it!\n",
    "            severity_final = full_sev.assign_coords(time=('normalized_doy', obs_aligned_time.values))\n",
    "            severity_final = severity_final.swap_dims({'normalized_doy': 'time'}).drop_vars('normalized_doy').chunk({'time':-1})\n",
    "        else:\n",
    "            # If there's a dimension mismatch, we raise an error!\n",
    "            raise ValueError(f\"Time/normalized_doy dimension mismatch detected for computation of severity for the {year} year!\")\n",
    "        \n",
    "        if show_debug and not shown_once:\n",
    "            shown_once = True\n",
    "            \n",
    "            \n",
    "        ## SAVING\n",
    "        print(f\"Starting Save of {baseline_name} {year} Severity Dataset...\\n\")\n",
    "\n",
    "        # Setting up the destination filepath\n",
    "        filename = f\"{folder_name}_severity_{id}SST_PC{percentile}th_{year}_smoothWidth{smoothWidth}_{baseline_name}.zarr\"\n",
    "        sev_filepath = f'{my_root_directory}/{dataset_id}/Severity_{percentile}th/{baseline_name}/{filename}' # NOTE: POTENTIAL DIRECTORY TWEAKING HERE\n",
    "        print(\"File Path: \", sev_filepath, '\\n')\n",
    "\n",
    "        # Last-minute chunking to not hit the maximum buffer size for chunks\n",
    "        if len(severity_final.time.values) == 365:\n",
    "            severity_final = severity_final.chunk({'time':73})\n",
    "        elif len(severity_final.time.values) == 366:\n",
    "            severity_final = severity_final.chunk({'time':61})\n",
    "        print(f\"{year} Severity:\\n\", severity_final, '\\n')\n",
    "\n",
    "        if show_debug:\n",
    "            raise ValueError(\"You have reached the end of the debug. To begin saving the data, set show_debug to False!\")\n",
    "        else:\n",
    "            with ProgressBar():\n",
    "                severity_final.to_zarr(sev_filepath, mode='w')\n",
    "            \n",
    "            # Freeing up memory\n",
    "            severity_final.close()\n",
    "            del severity_final\n",
    "            gc.collect()\n",
    "            \n",
    "            print(\"Done saving! Moving along!\", '\\n')\n",
    "            print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "    stop_monitoring = True\n",
    "    print(\"Finished saving data for all years!\")\n",
    "    print(\"--------------------o-------------------------------o------------------------------o----------------------------\", '\\n')  \n",
    "\n",
    "\n",
    "calculate_severity(obs_data=Full_obs, thresh_data=Full_thresh.drop_vars('quantile'), clim_data=Full_means, \n",
    "                   baseline_name=baseline_name_arg, folder_name=folder_name_arg, custom_id=custom_id_choice,\n",
    "                   best_chunks=optimal_chunks, \n",
    "                   smoothWidth=31, percentile=current_percentile,\n",
    "                   minDaysMhwDuration=5, maxGapDaysInMhw=2,\n",
    "                   minutes_per_mem_update=5, show_debug=False, \n",
    "                   custom_years=True, start_yr=1982, end_yr=2024,\n",
    "                   mhw_test=False)\n",
    "\n",
    "stop_monitoring = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f316991-d225-4e11-bb36-a6fa3b93d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an animation for MHW severity or severity (regardless of MHWs)\n",
    "\n",
    "## Function to create an animation that shows the mean and percentile latitude and longitude maps for the full 1 - 366 period. \n",
    "def check_severities_with_an_animation(baseline_name_arg, folder_name_arg, \n",
    "                                       year, mhw_only = False,\n",
    "                                        custom_output_filename=None, \n",
    "                                        percentile=None):\n",
    "\n",
    "    ## First, gather the appropriate stored dataset filepath\n",
    "    if mhw_only:\n",
    "        data_var_to_access = \"severity_mhw\"\n",
    "        data_type = \"MHW Severity\"\n",
    "    else:\n",
    "        data_var_to_access = \"severity\"\n",
    "        data_type = \"Severity\"\n",
    "\n",
    "    # NOTE: POTENTIAL DIRECTORY TWEAKING HERE\n",
    "    path = f'{my_root_directory}/{dataset_id}/Severity_{percentile}th/{folder_name_arg}_severity_SST_PC{percentile}th_{year}_{baseline_name_arg}.zarr'  \n",
    "    \n",
    "    ## Fill a dictionary where all present days are matched with their corresponding filepath\n",
    "    times_dict = {}\n",
    "    \n",
    "    # Open and check what days are in this file\n",
    "    try:\n",
    "        ds = xr.open_zarr(path)[data_var_to_access]\n",
    "    except:\n",
    "        raise ValueError(f\"No file found at: {path}.\")\n",
    "    \n",
    "    available_times = ds['time'].values\n",
    "    available_times = pd.to_datetime(available_times)\n",
    "   \n",
    "    # Quick fix for my personal, early datasets\n",
    "    lon = ds.lon.values\n",
    "    lat = ds.lat.values\n",
    "\n",
    "    # Set up the plot using the values in the first day\n",
    "    setup_data = ds.isel(time=0).values\n",
    "\n",
    "    ## Initialize the plot    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6), \n",
    "                           subplot_kw={'projection': ccrs.Mercator()})\n",
    "    \n",
    "    pcm = ax.pcolormesh(\n",
    "        lon, lat, setup_data,\n",
    "        cmap='RdYlBu_r',\n",
    "        vmin=-1, vmax=5,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "    )\n",
    "    cbar = plt.colorbar(pcm, ax=ax, label='Temperature Severity')  # Adjust label as needed\n",
    "    \n",
    "    ax.set_extent([0, 360, -5, 90], crs=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.LAND, color='lightgray')\n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth=0.8)\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    \n",
    "    title = ax.set_title('')\n",
    "    title_base = '(Relative to 1993-2022)' if baseline_name_arg == 'Baseline9322' else f'({baseline_name_arg})'\n",
    "    \n",
    "    ## Animation function\n",
    "    def animate(i):\n",
    "        time = available_times[i]\n",
    "        frame_data = ds.isel(time=i).values\n",
    "        \n",
    "        # Update the plot\n",
    "        pcm.set_array(frame_data.ravel())\n",
    "\n",
    "        # Update the title appropriately\n",
    "        time_str = pd.Timestamp(time).strftime('%Y-%m-%d')\n",
    "        title.set_text(f'{data_type} for {time_str}\\n{title_base}')\n",
    "        \n",
    "        return pcm, title\n",
    "    \n",
    "    \n",
    "    ## Create the resulting animation\n",
    "    type_message = \"marine heatwave severity\" if mhw_only else \"severity\"\n",
    "    print(f\"Began animation for the {type_message} in the {year} {folder_name_arg} datasets!\")\n",
    "    \n",
    "    chosen_times = len(available_times)\n",
    "    \n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, animate,\n",
    "        frames=chosen_times,\n",
    "        interval=200,\n",
    "        blit=False,\n",
    "        repeat=True\n",
    "    )\n",
    "    \n",
    "    writer = animation.PillowWriter(fps=2)\n",
    "    \n",
    "    if custom_output_filename == None:\n",
    "        output_filename = f\"{folder_name_arg}_{data_var_to_access}_{baseline_name_arg}_{year}.gif\"\n",
    "    else:\n",
    "        output_filename = custom_output_filename\n",
    "    print(f\"Saving animation at: {output_filename}\") \n",
    "    \n",
    "    \n",
    "    ## Save the resulting animation\n",
    "    def print_frame_progress(current_frame, total_frames):\n",
    "        print(f\"\\r → Date (Frame) Processed: {current_frame + 1}/{total_frames}\", end='', flush=True)\n",
    "\n",
    "    #with ProgressBar():\n",
    "    anim.save(output_filename, writer=writer, dpi=100,\n",
    "              progress_callback=print_frame_progress)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.close(fig)\n",
    "    ds.close()\n",
    "    print(f\"\\nAnimation finished and saved!\\n\")\n",
    "    \n",
    "    return anim\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "check_severities_with_an_animation(\"Baseline9322\", \"Full\", year = 2024, mhw_only = False, custom_output_filename=None, percentile=90)\n",
    "check_severities_with_an_animation(\"Baseline9322\", \"Full\", year = 2024, mhw_only = True, custom_output_filename=None, percentile=90)\n",
    "check_severities_with_an_animation(\"Baseline9322\", \"Full\", year = 1993, mhw_only = False, custom_output_filename=None, percentile=90)\n",
    "check_severities_with_an_animation(\"Baseline9322\", \"Full\", year = 1993, mhw_only = True, custom_output_filename=None, percentile=90)\n",
    "check_severities_with_an_animation(\"Baseline9322\", \"Full\", year = 1982, mhw_only = False, custom_output_filename=None, percentile=90)\n",
    "check_severities_with_an_animation(\"Baseline9322\", \"Full\", year = 1982, mhw_only = True, custom_output_filename=None, percentile=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3b1934-1d40-4eb7-9949-baf3c0197153",
   "metadata": {},
   "source": [
    "MHW Detection Validation (coming soon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangeo23",
   "language": "python",
   "name": "pangeo23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
