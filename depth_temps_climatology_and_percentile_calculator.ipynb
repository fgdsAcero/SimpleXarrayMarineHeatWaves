{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da928dd-13eb-496a-83a8-c4bdf5a5b7c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import psutil\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af0ecd3-fa74-4348-a786-cd29fe5837fe",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFE099; padding: 10px; border: 3px solid #FFC233; text-align: center; font-family: Georgia, serif; font-weight: bold; white-space: pre;\">||| -------------------------------------------- |||    NOTES    ||| -------------------------------------------- |||\n",
    "</div>\n",
    "<div style=\"background-color: #EFFAFA; border: 2px solid #A2E2E2; font-family: Georgia, serif; padding: 10px\">\n",
    "    <br>This is the script to <strong>process percentile thresholds</strong> and <strong>climatological means</strong> from <strong>temperatures at various uniform depth levels.</strong><div>\n",
    "    <br>&#10148;&#xFE0E; You can duplicate this script and run its copies simultaneously to download more percentiles/means.\n",
    "    <br>&#10148;&#xFE0E; If unusual errors appear after attempting to run a cell again, shut down/restart the kernel (and run the cell again afterward)!\n",
    "    <br>&#10148;&#xFE0E; After saving percentiles/means, check your saved files and compare their sizes to see which ones ought to be removed/redownloaded! \n",
    "    <br>&emsp;&emsp;&emsp;&#9733;&#xFE0E; Usually, extremely small file sizes indicate a dataset was not fully saved.\n",
    "    <br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a72b3c2-4291-4258-973f-e3ed4b349f41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## --- IMPORTANT SCRIPT-WIDE CONSTANTS ------------------------------------------------------------------------------------\n",
    "## Baseline set up:\n",
    "# Running the main function for the 1993–2022 baseline period (a 30 year baseline)\n",
    "baseline_choice = \"Baseline9322\" # identifier\n",
    "baseline_period_slice_choice = slice('1993-01-01', '2022-12-31') # for slicing time\n",
    "\n",
    "## Desired constants\n",
    "chosen_percentile = 90 # full number that is < 100\n",
    "\n",
    "## Folder identifiers\n",
    "folder_name_choice = \"Full\" # A \"regional\" identifier to save the severity dataset with (\"{folder_name_choice}_SST... .zarr\")\n",
    "custom_id = \"fgd\" # A custom identifier to help further identify the downloaded data; can be left as \"\"\n",
    "my_root_directory = \"/\" # Should be your root directory, from which you access data from and save data to.\n",
    "                        # The root directory should be of a format: \"/name\", where \"name\" corresponds to the name of your root directory\n",
    "temperature_dataset_id = \"\" # should be the folder name of the dataset whose raw data you downloaded. By efault, this will\n",
    "                            # also be where your means/percentiles will be saved, although you can change this. \n",
    "temperature_directory = f\"{my_root_directory}/{temperature_dataset_id}\"\n",
    "temperature_data_directory = f\"{temperature_directory}/Data\" # adjust to suit your needs\n",
    "temperature_means_directory = f\"{temperature_directory}/Clim\"\n",
    "temperature_percentile_directory = f\"{temperature_directory}/Thresh{chosen_percentile}th\"\n",
    "print(f\"Your specified temperature directory: {temperature_directory}\")\n",
    "\n",
    "# Doys (days of the year) set up\n",
    "starting_day_of_the_year = 1 # Your starting doy point for processing/saving severity (can be 1-366)\n",
    "ending_day_of_the_year = 366 # Your ending doy point for processing/saving severity (can be 1-366)\n",
    "\n",
    "# Misc\n",
    "minutes_choice = 10 # minutes (roughly) per memory update (to keep track of its use and avoid crashing/issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49942d61-7a45-4c5e-988b-35881b5d6d11",
   "metadata": {},
   "source": [
    "<div style=\"color:#CD6600; padding: 10px; text-align: center; font-family: Georgia, serif; font-weight: bold; white-space: pre;\">°º¤ø,¸¸,ø¤º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø¤º°`°º¤ø,¸    LOADING FULL OBSERVED GLOBAL DATASETS    °º¤ø,¸¸,ø¤º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø¤º°`°º¤ø,¸\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed14cec8-fb4d-45b7-91f8-a722a6cfe5dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### This is my personal setup to load all my downloaded datasets in single incomplete OR complete folder/region-based datasets.\n",
    "## You can choose to use a different setup to load your data.\n",
    "\n",
    "## Function to concatenate the datasets for each folder into our raw \"observed\" datasets (as opposed to climatology or percentile datasets)\n",
    "## ------------------------------------------------------------------------------------------\n",
    "\n",
    "# This setup allows you to either load one or more subsets for the same region. Ex. the \"Mid\" region below only has one region active.\n",
    "region_dict_list = [\n",
    "   # {\"Atlantic\": [\"Central\", \"Top\", \"Right\"]}, # Here, the North Atlantic region data was subsetted into three different parts with unique id's.\n",
    "   # {\"Pacific\": [\"Center\", \n",
    "    #             \"Left\"]},\n",
    "    {\"Mid\": [\n",
    "        #\"All\", # This two subset id names are bad to use (they are very vague); make sure to keep notes of what your chosen id's mean!\n",
    "        \"Mid\"   \n",
    "    ]}\n",
    "]\n",
    "\n",
    "# This function helps show a global-scale plot of the data that you have downloaded and loaded into single datasets!\n",
    "def show_map(ds_input, chosen_depth=0.494):\n",
    "    date = \"2003-08-22\"\n",
    "    ds = ds_input.sel(time=date, depth=chosen_depth, method='nearest')\n",
    "    projection_choice = ccrs.Mercator()\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), \n",
    "                           subplot_kw={'projection': projection_choice})\n",
    "    im = ax.pcolormesh(ds.longitude, ds.latitude, ds,\n",
    "                       transform=ccrs.PlateCarree(),\n",
    "                       cmap='RdYlBu_r')\n",
    "    ax.set_extent([0, 360, -30, 90], crs=ccrs.PlateCarree())\n",
    "    ax.coastlines()\n",
    "    ax.gridlines(draw_labels=True)\n",
    "    cbar = plt.colorbar(im, ax=ax, shrink=0.7)\n",
    "    cbar.set_label('Sea Surface Temperature (°C)', rotation=270, labelpad=15)\n",
    "    ax.set_title(f'Sea Surface Temperature - {date}', fontsize=14)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "## We need to iterate over all the folders in storage\n",
    "for folder in region_dict_list:\n",
    "    for folder_name, sub_folders in folder.items():\n",
    "        \n",
    "        # Initializing...\n",
    "        observed_data_directory = \"\"\n",
    "        folder_name_datasets = []\n",
    "        \n",
    "        # Actual code to concatenate:\n",
    "        for sub_folder_name in sub_folders:\n",
    "            # Your filepath here; this is my setup\n",
    "            observed_data_directory = f'{temperature_data_directory}/{folder_name}/{folder_name}_{sub_folder_name}'\n",
    "            \n",
    "            # Get the ordered paths...\n",
    "            paths = glob.glob(f'{observed_data_directory}/daily_data_*.zarr')\n",
    "            paths.sort()\n",
    "            \n",
    "            # Merge all the datasets in each subfolder...\n",
    "            datasets = [xr.open_zarr(path) for path in paths]\n",
    "            full_ds = xr.concat(datasets, dim=\"time\")\n",
    "            \n",
    "            ## This section below lists specific folder_name (dataset) adjustments for my setup; adjustments for your setup may vary\n",
    "            # Specifically, I made sure that there were no overlapping values along any of my coordinates except for latitude\n",
    "            \n",
    "            if folder_name == \"Atlantic\":\n",
    "                if (min(full_ds.longitude.values)==-101):\n",
    "                    full_ds = full_ds.sel(longitude=slice(-101, -14.001))\n",
    "                if (min(full_ds.latitude.values)<0):\n",
    "                    max_val = max(full_ds.latitude.values)\n",
    "                    full_ds = full_ds.sel(latitude=slice(-0.75, max_val))\n",
    "                best_chunks = {'depth': -1, 'time': 28, 'latitude': 218, 'longitude': 242}\n",
    "                \n",
    "            elif folder_name == \"Pacific\":\n",
    "                if (max(full_ds.longitude.values)>259):\n",
    "                    full_ds = full_ds.sel(longitude=slice(110.001, 258.999))\n",
    "                if (min(full_ds.latitude.values)<-1.5):\n",
    "                    full_ds = full_ds.sel(latitude=slice(-0.75, 90.001))\n",
    "                best_chunks = {'depth': -1, 'time': 28, 'latitude': 218, 'longitude': 229}\n",
    "                \n",
    "            elif folder_name == \"Mid\":\n",
    "                if sub_folder_name == \"Mid\":\n",
    "                    best_chunks = {'depth': -1, 'time': 28, 'latitude': 221, 'longitude': 361}\n",
    "                else:\n",
    "                    full_ds = full_ds.sel(latitude=slice(-20.9, -0.75))\n",
    "                    full_ds = full_ds.where(~((full_ds.longitude > 19.999) & (full_ds.longitude < 49.001)), drop=True)\n",
    "                    best_chunks = {'depth': -1, 'time': 28, 'latitude': 121, 'longitude': 361}\n",
    "            \n",
    "            # Adding the dataset to the list...\n",
    "            folder_name_datasets.append(full_ds)\n",
    "                \n",
    "             \n",
    "        ## Combining stored datasets for each folder...\n",
    "        print(\"Folder:\", folder_name, '\\n')\n",
    "        \n",
    "        # If we have many datasets appended to the list, we combine them\n",
    "        if len(folder_name_datasets) > 1: \n",
    "            folder_combined_ds = xr.combine_by_coords(folder_name_datasets, compat='no_conflicts')\n",
    "        # If there is just one dataset appended to the list, we ignore the list\n",
    "        else:\n",
    "            folder_combined_ds = full_ds\n",
    "        \n",
    "        folder_combined_ds = folder_combined_ds.chunk(best_chunks)\n",
    "        \n",
    "        # Saving this dataset globally...\n",
    "        globals()[f'{folder_name}_obs'] = folder_combined_ds\n",
    "        print(globals()[f'{folder_name}_obs'])\n",
    "        \n",
    "        # Showing a quick map of the dataset (to make sure everything came out right!)\n",
    "        show_map(globals()[f'{folder_name}_obs'].thetao)\n",
    "        print('\\n-----------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4362b0e-6112-40e7-976d-e0de9c624b0d",
   "metadata": {},
   "source": [
    "<div style=\"color:#104E8B; padding: 10px; text-align: center; font-family: Georgia, serif; font-weight: bold; white-space: pre;\">°º¤ø,¸¸,ø¤º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø¤º°`°º¤ø,¸    CALCULATING MEANS & PERCENTILES    °º¤ø,¸¸,ø¤º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø¤º°`°º¤ø,¸</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724ec30-60a2-4364-ab67-51cf08331786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' ----------------------------------------------- '''\n",
    "''' Function to keep track of and show memory usage '''\n",
    "''' ----------------------------------------------- '''\n",
    "\n",
    "stop_monitoring = True # We begin by NOT showing any memory usage\n",
    "\n",
    "def monitor_memory(interval_minutes=5, log_file=None):\n",
    "    interval = interval_minutes * 60  \n",
    "    \n",
    "    while not stop_monitoring:\n",
    "        mem = psutil.Process(os.getpid()).memory_info().rss / (1024**3)  # in GB\n",
    "        print(f\" | Memory usage: {mem:.2f} GB | Memory: {psutil.virtual_memory().percent}% used | \")\n",
    "        \n",
    "        if log_file:\n",
    "            with open(log_file, 'a') as f:\n",
    "                f.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')}: {mem:.2f} GB\\n\")\n",
    "        time.sleep(interval)\n",
    "\n",
    "## -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' ---------------------------------------------------------------------------------------------------------- '''\n",
    "''' Function to create a chunk list of missing day of the year datasets based on your own file directory setup '''\n",
    "''' ---------------------------------------------------------------------------------------------------------- '''\n",
    "\n",
    "def gather_missing_from_storage(folder_name_arg, sub_folder_name_arg, baseline_name_arg, \n",
    "                                chunk_start_arg, chunk_end_arg, \n",
    "                                current_percentile, return_check_all=True):\n",
    "    error_counter = 0\n",
    "    error_list = []\n",
    "    \n",
    "    for i in range(1, 366+1):\n",
    "        # We check only within the desired chunk interval\n",
    "        if i >= chunk_start_arg and i <= chunk_end_arg:\n",
    "            \n",
    "            id_path = f\"{folder_name_arg}_{sub_folder_name_arg}\"\n",
    "            file_name = f\"{id_path}_thetao_thresh_300m_subset_{i}_{baseline_name_arg}.zarr\"\n",
    "            filepath = f'{temperature_directory}/Thresh{current_percentile}th/{folder_name_arg}/{id_path}/{file_name}'\n",
    "\n",
    "            # We check if the filepath exists in the target location; if it does not, we add it to the error_list to process them in the code!\n",
    "            if not os.path.exists(filepath):\n",
    "                error_counter += 1\n",
    "                error_list.append(i)\n",
    "        \n",
    "    # If there are no missing doy datasets in our storage, we return None; otherwise, we return the list of missing doys!\n",
    "    if error_counter == 0:\n",
    "        print(\"All doys checked and present! (You should also check file sizes to verify everything downloaded correctly!\")\n",
    "        return None\n",
    "    else:\n",
    "        if return_check_all:\n",
    "            print(f\"All doys checked; you are missing {error_counter} doy datasets in total!\\n\")\n",
    "\n",
    "        return error_list\n",
    "\n",
    "## -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' ------------------------------------------------------------------------------------------------ '''\n",
    "''' Function to normalize the unique day of the year value of each observed day in the format: 1-366 '''\n",
    "''' ------------------------------------------------------------------------------------------------ '''\n",
    "\n",
    "def normalize_dayofyear(time_coord):\n",
    "    doy = time_coord.dt.dayofyear\n",
    "    is_leap = time_coord.dt.is_leap_year\n",
    "\n",
    "    # This code ensures March 1 is always day 61, regardless of leap year\n",
    "    normalized_doy = xr.where(\n",
    "        (~is_leap) & (doy >= 60),  # If it is a non-leap year, doy 60 is March 1. If we have March 1 or later,\n",
    "        doy + 1,                          # then we push forward March 1 and/or the later days by 1 day.\n",
    "        doy                               # Otherwise, we keep original for leap years and Jan-Feb 28.\n",
    "    )\n",
    "\n",
    "    return normalized_doy\n",
    "\n",
    "## -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' ------------------------------------------------------------------------------- '''\n",
    "''' Function to check whether the inputted time period is at least roughly 30 years '''\n",
    "''' ------------------------------------------------------------------------------- '''\n",
    "\n",
    "def rough_30_year_period_check(time_slice, tolerance=0.01):\n",
    "    start_str = time_slice.start\n",
    "    stop_str = time_slice.stop\n",
    "\n",
    "    start_date = datetime.strptime(start_str, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(stop_str, '%Y-%m-%d')\n",
    "\n",
    "    delta = relativedelta(end_date, start_date)\n",
    "    total_years = delta.years + delta.months/12 + delta.days/365.25\n",
    "\n",
    "    return total_years, abs(total_years - 30) <= tolerance\n",
    "\n",
    "## -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' ------------------------------------------------------------------------------------- '''\n",
    "''' Function to just save the percentile threshold/climatological mean dataset to storage '''\n",
    "''' ------------------------------------------------------------------------------------- '''\n",
    "\n",
    "def save_dataset_to_storage(folder_name_arg, sub_folder_name_arg, baseline_name_arg,\n",
    "                            clim_arg, show_debug_arg, single_download_arg, \n",
    "                            current_percentile=None, current_doy=None, \n",
    "                            start_val_arg = None, end_val_arg = None, \n",
    "                            ds_to_save=None):\n",
    "    \n",
    "    id_path = f\"{folder_name_arg}_{sub_folder_name_arg}\"\n",
    "    \n",
    "    # We check if we are saving a climatology dataset or percentile one\n",
    "    if clim_arg:\n",
    "        print(\"Starting doy for current subset: \", start_val_arg)\n",
    "        print(\"Ending doy for current subset: \", end_val_arg, '\\n')\n",
    "\n",
    "        if start_val_arg == 1 and end_val_arg == 366:\n",
    "            final_ds_to_save = ds_to_save\n",
    "        else:\n",
    "            final_ds_to_save = ds_to_save.sel({'normalized_doy': slice(start_val_arg, end_val_arg)})\n",
    "        print(\"Current climatology subset to save: \", '\\n', final_ds_to_save, '\\n')\n",
    "\n",
    "        file_name = f\"{id_path}_thetao_clim_300m_subset_{start_val_arg}_to_{end_val_arg}_{baseline_name_arg}.zarr\"\n",
    "        filepath = f'{temperature_means_directory}/{folder_name_arg}/{id_path}/{file_name}'\n",
    "        \n",
    "    else:\n",
    "        final_ds_to_save = ds_to_save\n",
    "        print(f\"{current_percentile}th Percentile for doy {current_doy} is being saved:\\n\", final_ds_to_save, '\\n')\n",
    "        file_name = f\"{id_path}_thetao_thresh_300m_subset_{current_doy}_{baseline_name_arg}.zarr\"\n",
    "        filepath = f'{temperature_directory}/Thresh{current_percentile}th/{folder_name_arg}/{id_path}/{file_name}'\n",
    "    \n",
    "    print(\"Filepath of subset: \", filepath)\n",
    "\n",
    "    # Continue to saving or stop (if we are at the end of the debug)\n",
    "    if show_debug_arg:\n",
    "        raise ValueError('End of debug. Proceed with the setting \"show_debug = False\" to start saving the percentile thresholds.')\n",
    "        \n",
    "    else:\n",
    "        with ProgressBar():\n",
    "            final_ds_to_save.to_zarr(filepath, mode='w', consolidated=True)\n",
    "        \n",
    "        if clim_arg:\n",
    "            print(f\"Saved subset: doys {start_val_arg} to {end_val_arg}\")\n",
    "        else:\n",
    "            print(f\"Saved subset: doys {current_doy}\")\n",
    "        \n",
    "        print(f\"Moving on!\", \"\\n\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        # Optional: for single downloads\n",
    "        if single_download_arg:\n",
    "            stop_monitoring = True\n",
    "            raise ValueError(\"Single dataset file-saving finished. Please enter a new desired chunk starting value to begin from.\")\n",
    "            \n",
    "## -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' ----------------------------------------------------------------- '''\n",
    "''' Function to actually calculate the percentile threshold dataset to storage '''\n",
    "''' ----------------------------------------------------------------- '''\n",
    "\n",
    "# Function to calculate the percentile threshold values for specific depths\n",
    "def calculate_thetao_thresh_or_clim_given_a_percentile(thetao_data, baseline_slice, \n",
    "                                                       folder_name, sub_folder_name, baseline_name, \n",
    "                                                       optimal_chunks, window_half_width=5, \n",
    "                                                       minutes_per_memory_update=5, percentile=90, \n",
    "                                                       start_chunking_doy=1, end_chunking_doy=366,\n",
    "                                                       show_debug=True, single_download=False, \n",
    "                                                       chunk_list=False, finish_at_chunk_end=False):     \n",
    "    \n",
    "    if show_debug:\n",
    "        debug_message_1 = \"You have set show_debug to true; this will show how the percentiles/means are processed based on your inputted arguments \\nand provide a preview of the output.\\n\"\n",
    "        debug_message_2 = \"\\nIf you are satisfied with the output (and your arguments), compute and save the calculated percentiles/means by setting\\nshow_debug to false.\\n\"\n",
    "        print(debug_message_1, debug_message_2)\n",
    "    \n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Part 0: Running a few quick error checks for the provided arguments!\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "              \n",
    "    # Checking start and end bounds\n",
    "    if start_chunking_doy < 1 or end_chunking_doy > 366:\n",
    "        raise ValueError(\"Please provide a start_chunking_doy that is ≥ 1 and an end_chunking_doy that is ≤ 366.\")\n",
    "        \n",
    "    if chunk_end < chunk_start:\n",
    "        raise ValueError(\"Please make sure your chunk_end is greater than your chunk_start; these are your dataset processing bounds.\")\n",
    "        \n",
    "    # Running a rough time check for the baseline provided\n",
    "    total_time, is_time_slice_30_years = rough_30_year_period_check(baseline_slice)\n",
    "    \n",
    "    if not is_time_slice_30_years:\n",
    "        error_message = \"Please check that your chosen baseline time slice covers a 30 year period.\"\n",
    "        raise ValueError(f\"{error_message}.\\n            The chosen slice covers roughly {total_time} years.\")\n",
    "    \n",
    "    # Establishing if we calculating percentiles or means \n",
    "    calculate_clim = True if (percentile == None) else False\n",
    "    chunk_size = end_chunking_doy\n",
    "    \n",
    "    # Checking the chunk size for our climatological calculations\n",
    "    if calculate_clim and chunk_size == 0:\n",
    "        raise ValueError(\"Please set an integer value for end_chunking_doy, which sets the size of the doy batch you use to save the means in.\")\n",
    "    \n",
    "    \n",
    "    if show_debug: \n",
    "        print(\"All clear!\\n\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Part 1: Assign normalized unique day of the year (doy) values to the sliced observation dataset\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    print(f\"Chosen baseline slice: {baseline_slice}\")\n",
    "    print(\"Note: the chosen baseline period has been identified as roughly covering a 30 year period. Do ensure this is the case separately.\\n\")\n",
    "    print(f\"Chosen window half-width: {window_half_width}\")\n",
    "    print(f\"(This means we use {window_half_width} days before and after each day of the year for each doy in our climatology/threshold.)\", '\\n')\n",
    "    \n",
    "    if calculate_clim:\n",
    "        print(f\"The selected doy chunk size for climatological mean dataset batches is {chunk_size} doys.\\n\")\n",
    "        end_chunk_val = 366 if not single_download else (start_chunking_doy + chunk_size - 1)\n",
    "        \n",
    "        print(f\"Calculating means in batches of (at most) {chunk_size} doys between {start_chunking_doy} and {end_chunk_val}.\\n\")\n",
    "    else:\n",
    "        percentile_used = percentile/100\n",
    "        print(f\"Final percentile used (in calculations): {percentile_used} ({percentile}th percentile)\", '\\n')\n",
    "        print(f\"Calculating thresholds individually for doys between {start_chunking_doy} and {end_chunking_doy}.\\n\")\n",
    "    \n",
    "    # Doy values for specific dates (for later)\n",
    "    feb28_doy = 59\n",
    "    feb29_doy = 60\n",
    "    mar1_doy = 61\n",
    "    \n",
    "    # We create a missing doys chunk list for percentiles only, if desired\n",
    "    if not calculate_clim:\n",
    "        if chunk_list:\n",
    "            missing_list = gather_missing_from_storage(folder_name, sub_folder_name, baseline_name, \n",
    "                                                       start_chunking_doy, end_chunking_doy,\n",
    "                                                       percentile, return_check_all=show_debug)\n",
    "\n",
    "            ## Quick check to make sure the returned missing doy list provided is valid.\n",
    "            if missing_list is not None:\n",
    "                if type(missing_list) is list:\n",
    "                    filtered_list = [i for i in missing_list if i >= start_chunking_doy and i <= end_chunking_doy]\n",
    "\n",
    "                    print(\"Missing doys within the set chunk interval were found!\\n\") \n",
    "                    print(f\"We are processing all missing doys within the filtered chunk_list:\\n{filtered_list}\\n\\n\")\n",
    "            else:\n",
    "                raise ValueError(\"No missing doys were found in the doy interval! Turn off chunk_list to proceed anyway!\")\n",
    "    \n",
    "    ## Extracting baseline period data\n",
    "    thetao_baseline = thetao_data.sel(time=baseline_slice)\n",
    "    if show_debug: print(\"Original Thetao Baseline Period Data: \", '\\n', thetao_baseline, '\\n')\n",
    "   \n",
    "    # Assigning normalized doy values to the baseline period dataset\n",
    "    thetao_norm = thetao_baseline.assign_coords(\n",
    "        normalized_doy=('time', normalize_dayofyear(thetao_baseline.time).data))\n",
    "    if show_debug: print(\"Thetao with Normalized Doy: \", '\\n', thetao_norm, '\\n')\n",
    "        \n",
    "    '''\n",
    "    # Totally optional debug option here: show ALL normalized day of the year (doy) values;\n",
    "    # all years are in the 366-day format, with some missing day 60 (feb 29)\n",
    "    with np.printoptions(threshold=np.inf):\n",
    "        print(thetao_norm.normalized_doy.values) \n",
    "    '''\n",
    "    \n",
    "\n",
    "    if show_debug: \n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Part 2: Get the actual doy values of the baseline period data (should be 1 - 366)\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "    unique_doys = np.unique(thetao_norm.normalized_doy.data)\n",
    "    unique_doys = unique_doys[~np.isnan(unique_doys)]  # Remove any NaN values\n",
    "    unique_doys = unique_doys.astype(int)  # Ensure integer day-of-year values\n",
    "    if show_debug: print(f\"Found {len(unique_doys)} unique day-of-year values!\")\n",
    "    if show_debug: print(\"Unique doys:\", '\\n', unique_doys, '\\n')\n",
    "\n",
    "         \n",
    "    global stop_monitoring\n",
    "    if show_debug: \n",
    "        choice_message = \"climatological mean\" if calculate_clim else \"percentile threshold\"\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "        print(f\"Part 3: Calculate the desired {choice_message} data for the desired doy(s).\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "        stop_monitoring = True # We don't want to start showing memory use.\n",
    "    else:\n",
    "        # We start monitoring here so that it only runs once\n",
    "        stop_monitoring = False # We do want to start showing memory use.\n",
    "        monitor_thread = threading.Thread(target=monitor_memory, kwargs={'interval_minutes': minutes_per_memory_update})\n",
    "        monitor_thread.daemon = True\n",
    "        monitor_thread.start()\n",
    "        \n",
    "    # Error messages for later\n",
    "    chunk_list_message_finished = \"All missing doys within the chunk list (except Feb 29) processed!\\n\"\n",
    "    chunk_message_finished = \"Set chunk end reached!\\n\"\n",
    "    no_feb29_possible_warning = \"WARNING: Cannot interpolate Feb 29; missing Feb 28 or Mar 1 data!\\n\"\n",
    "    \n",
    "    # Initialize a dictionary for the climatological means if wanted\n",
    "    if calculate_clim:\n",
    "        seas_clim_dict = {}\n",
    "        \n",
    "    # Bool for debug purposes\n",
    "    shown_once = False\n",
    " \n",
    "    # Loop for doys 1 - 366 (excluding Feb 29, doy 60)\n",
    "    for doy in unique_doys:\n",
    "        # We skip February 29th (to interpolate later)\n",
    "        if doy == feb29_doy:  \n",
    "            continue # Note: doy 60 data is still used within the appropriate window_data when available\n",
    "        \n",
    "        # These next checks only run for percentile calculations; all doys are loaded in the climatological dictionary (except Feb 29)\n",
    "        if not calculate_clim:\n",
    "            # We skip doys to begin on the desired chunk_start value\n",
    "            if doy < start_chunking_doy: \n",
    "                continue\n",
    "\n",
    "            # Now we look at doys greater than the end_chunking_doy\n",
    "            if doy > end_chunking_doy:\n",
    "                # For percentiles, we check if we have a chunk list and whether we wish to continue past the set end_chunking_doy bound\n",
    "                if chunk_list:\n",
    "                    if finish_at_chunk_end: # If we want to stop at the end of the set chunk interval:\n",
    "                        stop_monitoring = True\n",
    "                        raise ValueError(chunk_list_message_finished)\n",
    "\n",
    "                # And run this code otherwise for doys beyond the set chunk interval\n",
    "                else: \n",
    "                    if finish_at_chunk_end:\n",
    "                        stop_monitoring = True\n",
    "                        raise ValueError(chunk_message_finished)\n",
    "\n",
    "            # for percentiles, we skip any doys that are not found in the list of missing doys created above (if the chunk_list was set to true)\n",
    "            if chunk_list:\n",
    "                if doy not in missing_list:\n",
    "                    continue \n",
    "        \n",
    "        # Create window around this DOY\n",
    "        window_doys = []\n",
    "        \n",
    "        for w in range(-window_half_width, window_half_width + 1):\n",
    "            target_doy = doy + w\n",
    "            \n",
    "            if show_debug and not shown_once: \n",
    "                print(\"Day of the year: \", doy, \"| Target Window Index: \", w, \"| Target Window Value: \", target_doy)\n",
    "\n",
    "            # Handle year wraparound properly\n",
    "            if target_doy < 1:\n",
    "                target_doy += 366\n",
    "            elif target_doy > 366:\n",
    "                target_doy -= 366\n",
    "            \n",
    "            # Handle year boundaries by keeping only valid doys\n",
    "            if target_doy in unique_doys:\n",
    "                window_doys.append(target_doy)\n",
    "            \n",
    "            if show_debug and not shown_once: print(\"Window Doys: \", window_doys, '\\n')\n",
    "\n",
    "        # Now, we select the data for this window\n",
    "        window_data = thetao_norm.where(thetao_norm.normalized_doy.isin(window_doys), drop=True)\n",
    "        \n",
    "        '''\n",
    "        ### Feature to be added: the ability to tweak the data prior to any percentile calculations in a manner like so:\n",
    "        window_data = window_data.sel(latitude=slice(-3, 0))\n",
    "        '''\n",
    "        \n",
    "        if show_debug and not shown_once: print(\"Final window data from the baseline period dataset: \", '\\n', window_data, '\\n')\n",
    "        \n",
    "        # Now, we calculate the percentile threshold/climatological mean across the time dimension\n",
    "        if window_data.time.size > 0:\n",
    "            # We calculate the climatological mean if that is what is desired\n",
    "            if calculate_clim:\n",
    "                seas_clim_dict[doy] = window_data.mean(dim = 'time', skipna = True).expand_dims(normalized_doy=[doy])\n",
    "                \n",
    "                if show_debug and not shown_once:\n",
    "                    print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "                    print(f\"Part 4: Store the climatological means across all doys in an empty dictionary!\")\n",
    "                    print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "                    print(f\"Dictionary updated for doy {doy} with the time-averaged final window dataset in Part 3.\\n\\nDictionary entry:\\n\", seas_clim_dict[doy], '\\n')\n",
    "                    shown_once = True\n",
    "            \n",
    "            # Otherwise, we calculate the percentile for a doy and rechunk the result\n",
    "            else:\n",
    "                doy_to_save = window_data.chunk({'time':-1}).quantile(percentile_used, dim='time', skipna=True).expand_dims(normalized_doy=[doy])\n",
    "                doy_to_save = doy_to_save.chunk(optimal_chunks)\n",
    "\n",
    "                if show_debug: \n",
    "                    print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "                    print(f\"Part 4: Save the percentile threshold dataset one single doy at a time!\")\n",
    "                    print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "                # Now, we save the percentile threshold dataset to storage\n",
    "                save_dataset_to_storage(folder_name_arg=folder_name, sub_folder_name_arg=sub_folder_name, baseline_name_arg=baseline_name,\n",
    "                                        clim_arg=calculate_clim, show_debug_arg=show_debug, single_download_arg=single_download, \n",
    "                                        current_percentile=percentile, current_doy=doy, ds_to_save=doy_to_save)\n",
    "          \n",
    "        \n",
    "    # After the for loop over the 1-366 day of the year range, we handle February 29th using linear interpolation\n",
    "    # doy 60 is not being printed? added to dict...\n",
    "    \n",
    "    if (feb29_doy in unique_doys):\n",
    "        # If we have a dictionary with our climatological means...\n",
    "        if calculate_clim:\n",
    "            if feb28_doy in seas_clim_dict and mar1_doy in seas_clim_dict:\n",
    "                feb_28_ds = seas_clim_dict[feb28_doy].squeeze().drop_vars('normalized_doy')\n",
    "                mar_1_ds = seas_clim_dict[mar1_doy].squeeze().drop_vars('normalized_doy')\n",
    "                seas_clim_dict[feb29_doy] = 0.5 * (feb_28_ds + mar_1_ds)\n",
    "                seas_clim_dict[feb29_doy] = seas_clim_dict[feb29_doy].expand_dims(normalized_doy=[feb29_doy])\n",
    "                if show_debug: print(\"Interpolated February 29 dataset (doy 60) in the dictionary:\\n\", seas_clim_dict[feb29_doy], '\\n')\n",
    "            else:\n",
    "                print(no_feb29_possible_warning)\n",
    "        \n",
    "        # Otherwise, we interpolate percentile thresholds using saved percentile datasets\n",
    "        else:\n",
    "            # Quick check to ensure the 60th doy is within our chunk interval\n",
    "            if (start_chunking_doy < feb29_doy) and (feb29_doy < end_chunking_doy):\n",
    "                # We check if we have a chunk list with doy 60 among the missing doys\n",
    "                if chunk_list:\n",
    "                    if feb29_doy not in missing_list:\n",
    "                        stop_monitoring = True\n",
    "                        raise ValueError(chunk_list_message_finished)\n",
    "\n",
    "                # Function for finding the files necessary to interpolate for Feb 29\n",
    "                def find_file_and_return_it(folder_name_arg, sub_folder_name_arg, baseline_name_arg, current_doy, current_percentile):\n",
    "                    id_path = f\"{folder_name_arg}_{sub_folder_name_arg}\"\n",
    "                    file_name = f\"{id_path}_thetao_thresh_300m_subset_{current_doy}_{baseline_name_arg}.zarr\"\n",
    "                    filepath = f'{temperature_directory}/Thresh{current_percentile}th/{folder_name_arg}/{id_path}/{file_name}'\n",
    "\n",
    "                    # We check if the path/file exists in the target location\n",
    "                    if os.path.exists(filepath):\n",
    "                        ds = xr.open_zarr(filepath).squeeze('normalized_doy', drop=True)\n",
    "                        return True, ds\n",
    "                    else:\n",
    "                        return False, None\n",
    "\n",
    "                # We check for the files required for interpolation\n",
    "                file_found_feb28, thresh_feb28_ds = find_file_and_return_it(folder_name, sub_folder_name, baseline_name, feb28_doy, percentile)\n",
    "                file_found_mar1, thresh_mar1_ds = find_file_and_return_it(folder_name, sub_folder_name, baseline_name, mar1_doy, percentile)\n",
    "\n",
    "                # We proceed with interpolation if both files are found\n",
    "                if file_found_feb28 and file_found_mar1:\n",
    "                    doy_to_save = 0.5 * (thresh_feb28_ds + thresh_mar1_ds)\n",
    "                    doy_to_save = doy_to_save.expand_dims(normalized_doy=[feb29_doy])\n",
    "                    doy_to_save = doy_to_save.chunk(optimal_chunks)\n",
    "\n",
    "                    # Now, we save the percentile threshold dataset to storage\n",
    "                    save_dataset_to_storage(folder_name_arg=folder_name, sub_folder_name_arg=sub_folder_name, baseline_name_arg=baseline_name,\n",
    "                                           clim_arg=calculate_clim, show_debug_arg=show_debug, single_download_arg=single_download, \n",
    "                                           current_percentile=percentile, current_doy=feb29_doy, ds_to_save=doy_to_save)\n",
    "                \n",
    "            # If the two required datasets for interpolating Feb 29 are missing, we run this:\n",
    "            else: \n",
    "                print(no_feb29_possible_warning)\n",
    "            \n",
    "    # We proceed with the full climatology dictionary if we are calculating climatological means\n",
    "    if calculate_clim:\n",
    "        if show_debug: \n",
    "            print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "            print(\"Part 5: Creating the complete climatology dataset from the dictionary\")\n",
    "            print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "        # We create the correct coordinates from our dictionary for our final dataset\n",
    "        doy_coords = np.array(sorted(seas_clim_dict.keys())) # array for full year (1 to 366, if leap)\n",
    "        if show_debug: print(\"Dictionary Keys of Registered Unique Day of the Year (doy) Climatological Mean Datasets\\n\", \n",
    "                             \"(Should include all values from 1 to 366):\\n\", doy_coords, '\\n')\n",
    "\n",
    "        # We stack the resulting dictionary datasets while maintaining the correct order\n",
    "        seas_clim_list = [seas_clim_dict[doy] for doy in doy_coords]\n",
    "        seas_clim_year = xr.concat(seas_clim_list, dim='normalized_doy')\n",
    "        seas_clim_year = seas_clim_year.assign_coords(normalized_doy=('normalized_doy', doy_coords))\n",
    "        seas_clim_year = seas_clim_year.chunk(optimal_chunks)\n",
    "        \n",
    "        # Additional chunking that prevents crashing (can be lowered for more stability)\n",
    "        if chunk_size <= 61:\n",
    "            seas_clim_year = seas_clim_year.chunk({'normalized_doy': chunk_size})\n",
    "        else:\n",
    "            seas_clim_year = seas_clim_year.chunk({'normalized_doy': 10})\n",
    "            \n",
    "        print(\"Final Climatology Dataset:\\n\", seas_clim_year, '\\n')\n",
    "\n",
    "        # We check if we are saving this datasets fully or in batches; a chunk_size of 366 implies the full dataset is being saved (no batch saving)\n",
    "        if chunk_size == 366:\n",
    "            batch_saving = False\n",
    "        else:\n",
    "            batch_saving = True\n",
    "                \n",
    "        if show_debug: \n",
    "            saving_choice_message = \"in one go\" if not batch_saving else f\"via batches of {chunk_size} doys\"\n",
    "\n",
    "            print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "            print(f\"Part 5: Saving the climatology dataset {saving_choice_message}!\")\n",
    "            print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "            \n",
    "        # Coordinate values for batch saving (not full climatology dataset saving)\n",
    "        coord_values = seas_clim_year['normalized_doy'].values\n",
    "\n",
    "        # We are saving the dataset in batches\n",
    "        if batch_saving:\n",
    "            for i in range(0, len(coord_values), chunk_size):\n",
    "                # First, we gather the starting and ending values of the processed chunk\n",
    "                start_val = coord_values[i]\n",
    "\n",
    "                # Quick check to see where to begin downloading a batch from...\n",
    "                if start_val < start_chunking_doy:\n",
    "                    continue\n",
    "\n",
    "                # Grab the end index and value\n",
    "                end_idx = min(i + chunk_size, len(coord_values))\n",
    "                end_val = coord_values[end_idx - 1]\n",
    "                \n",
    "                # Save the dataset (subsetting occurs in the function)              \n",
    "                save_dataset_to_storage(folder_name_arg=folder_name, sub_folder_name_arg=sub_folder_name, baseline_name_arg=baseline_name,\n",
    "                                        clim_arg=calculate_clim, show_debug_arg=show_debug, single_download_arg=single_download,\n",
    "                                        start_val_arg=start_val, end_val_arg=end_val, ds_to_save=seas_clim_year)  \n",
    "        \n",
    "        # We are saving the full dataset\n",
    "        else:\n",
    "            save_dataset_to_storage(folder_name_arg=folder_name, sub_folder_name_arg=sub_folder_name, baseline_name_arg=baseline_name,\n",
    "                                    clim_arg=calculate_clim, show_debug_arg=show_debug, single_download_arg=single_download,\n",
    "                                    start_val_arg=start_chunking_doy, end_val_arg=end_chunking_doy, ds_to_save=seas_clim_year)  \n",
    "            \n",
    "        stop_monitoring = True\n",
    "\n",
    "    stop_monitoring = True # reset the monitoring before the next loop\n",
    "    \n",
    "## -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "## Running the main function for the 1993–2022 Baseline # 30 year baseline\n",
    "threshold_period_Baseline9322 = slice('1993-01-01', '2022-12-31')\n",
    "\n",
    "## Set up for the for loop to iterate over for different regions\n",
    "\n",
    "# Dictionary of chunk configs (set these to divide their coordinates' values by a small number less than 400 that leaves no remainder)\n",
    "chunk_configs = {\n",
    "    #\"Atlantic\": {\n",
    "        #\"Central\": {'depth': -1, 'latitude': 101, 'longitude': 209},\n",
    "        #\"Right\": {'depth': -1, 'latitude': 276, 'longitude': 204},\n",
    "     #   \"Top\": {'depth': -1, 'latitude': 196, 'longitude': 209}\n",
    "    #},\n",
    "   # \"Pacific\": {\n",
    "    #    \"Center\": {'depth': -1, 'latitude': 218, 'longitude': 257},\n",
    "     #   \"Left\": {'depth': -1, 'latitude': 221, 'longitude': 244}\n",
    "   # },\n",
    "   \"Mid\": {\n",
    "   #     \"All\": {'depth': -1, 'latitude': 253, 'longitude': 270},\n",
    "        \"Mid\": {'depth': -1, 'latitude': 221, 'longitude': 361}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Regions to process thresholds for (adjust as need be)\n",
    "region_dict_list = [\n",
    "    #{\"Atlantic\": [#\"Central\", \n",
    "    #              \"Top\", \n",
    "                 # \"Right\"\n",
    "   # ]},\n",
    "  #  {\"Pacific\": [\"Center\", \n",
    "                 #\"Left\"\n",
    "              #  ]},\n",
    "    {\"Mid\": [\"Mid\",\n",
    "        #\"All\",\n",
    "    ]}\n",
    "]\n",
    "\n",
    "# What should the starting chunk value be (if you have already saved previous chunks for a threshold dataset)?\n",
    "chunk_start = 1\n",
    "\n",
    "# What should be the final chunk to be calculated (anything greater than this is excluded)?\n",
    "chunk_end = 366\n",
    "\n",
    "# For convenience, a for loop structure is used to process climatological means/percentile thresholds for folders in our directory, whose\n",
    "# naming structure are implied by the dictionaries above. \n",
    "\n",
    "# Dictionary entries can be commented with a # at will to process only select subfolders at a time (within the larger regional folders).\n",
    "\n",
    "for folder in region_dict_list:\n",
    "    for folder_filename, sub_folders in folder.items():\n",
    "        \n",
    "        # Initializing...\n",
    "        observed_data_directory = \"\"\n",
    "        optimal_chunking = None\n",
    "        full_ds = None\n",
    "        \n",
    "        # Code to concatenate:\n",
    "        for sub_folder_filename in sub_folders:\n",
    "            # Your custom subsetted-region filepath here; this is my setup.\n",
    "            observed_data_directory = f'{temperature_data_directory}/{folder_filename}/{folder_filename}_{sub_folder_filename}'\n",
    "            \n",
    "            # Get the ordered paths...\n",
    "            paths = glob.glob(f'{observed_data_directory}/daily_data_*.zarr')\n",
    "            paths.sort()\n",
    "            \n",
    "            # Merge all the datasets in the subfolder into one to pass to the function; also designate the chunks to be used\n",
    "            datasets = [xr.open_zarr(path) for path in paths]\n",
    "            full_ds = xr.concat(datasets, dim=\"time\")\n",
    "            \n",
    "            optimal_chunking = chunk_configs[folder_filename][sub_folder_filename]\n",
    "            full_ds = full_ds.chunk(optimal_chunking)\n",
    "            \n",
    "            # Calling the function to calculate the mean thresholds (percentile=None)\n",
    "            calculate_thetao_thresh_or_clim_given_a_percentile(thetao_data = full_ds.thetao, \n",
    "                                                               baseline_slice = threshold_period_Baseline9322, \n",
    "                                                               folder_name = folder_filename, sub_folder_name = sub_folder_filename, \n",
    "                                                               baseline_name = baseline_choice, \n",
    "                                                               optimal_chunks = optimal_chunking, window_half_width = 5, \n",
    "                                                               minutes_per_memory_update = 45, percentile = None,\n",
    "                                                               start_chunking_doy = chunk_start, end_chunking_doy = chunk_end,\n",
    "                                                               show_debug = True, single_download = False, \n",
    "                                                               chunk_list = False, finish_at_chunk_end = True)\n",
    "            \n",
    "            # Calling the function to calculate the percentile thresholds (percentile=NUMBER)\n",
    "         #   calculate_thetao_thresh_or_clim_given_a_percentile(thetao_data = full_ds.thetao, \n",
    "          #                                                     baseline_slice = threshold_period_Baseline9322, \n",
    "           #                                                    folder_name = folder_filename, sub_folder_name = sub_folder_filename, \n",
    "            #                                                   baseline_name = baseline_choice, \n",
    "             #                                                  optimal_chunks = optimal_chunking, window_half_width = 5, \n",
    "              #                                                 minutes_per_memory_update = 45, percentile = chosen_percentile,\n",
    "               #                                                start_chunking_doy = chunk_start, end_chunking_doy = chunk_end,\n",
    "                #                                               show_debug = True, single_download = False, \n",
    "                 #                                              chunk_list = False, finish_at_chunk_end = True)\n",
    "\n",
    "print(\"We have finished saving all desired doy percentile threshold datasets completely!\")\n",
    "stop_monitoring = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c102917e-226f-4081-b4bc-b454f4fde893",
   "metadata": {},
   "source": [
    "<div style=\"color:#008B00; padding: 10px; text-align: center; font-family: Georgia, serif; font-weight: bold; white-space: pre;\">ø¤º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø¤º°`°º¤ø,¸  FILE VALIDATION, VERIFICATION, AND ANIMATIONS  °º¤ø,¸¸,ø¤º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø¤\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae01ab3-4f96-465b-b258-62fbf198c578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Check stored percentile threshold or climatological mean datasets (if you want to do so independently from the code above)\n",
    "\n",
    "def gather_missing_from_storage(baseline_name_arg, folder_name_arg, sub_folder_name_arg=None,\n",
    "                                check_climatological_means=False, current_percentile=90, return_check_all=True):\n",
    "    \n",
    "    check_size_notif = \"\\n(You should also check file sizes to verify everything downloaded correctly!)\"\n",
    "    error_counter = 0\n",
    "    error_list = []\n",
    "    \n",
    "    if check_climatological_means:\n",
    "        # Gather the existing filepaths\n",
    "        clim_data_directory = filepath = f'{temperature_means_directory}/{folder_name_arg}'\n",
    "        paths = glob.glob(f'{clim_data_directory}/{folder_name_arg}_thetao_clim_300m_subset*{baseline_name_arg}.zarr')\n",
    "        paths.sort()\n",
    "        \n",
    "        # Extract doy number ranges from existing files\n",
    "        covered = set()\n",
    "        for path in paths:\n",
    "            file_nums = os.path.basename(path).replace('.zarr', '').split('_')\n",
    "            # For the filename format: folderName_thetao_clim_300m_subset_START_to_END_baseline9322.zarr\n",
    "            # Index:                      [0]      [1]   [2]  [3]   [4]    [5] [6] [7]     [8]\n",
    "             \n",
    "            # Extract the doy number pairs\n",
    "            start, end = int(file_nums[5]), int(file_nums[7])\n",
    "            covered.update(range(start, end + 1))\n",
    "    \n",
    "        # Full year (1-366) check\n",
    "        missing_doys = sorted(set(range(1, 367)) - covered)\n",
    "        \n",
    "        \n",
    "        if not missing_doys:\n",
    "            print(f'Files for all {folder_name_arg} climatology doy ranges (a full 1-366 year) exist! {check_size_notif}')\n",
    "            return None\n",
    "        else:\n",
    "            print(f'Some {folder_name_arg} climatology doy ranges missing!\\n\\nMissing: {sorted(missing_doys)}\\n')\n",
    "            return missing_doys\n",
    "    \n",
    "    # Checking percentile thresholds\n",
    "    else: \n",
    "        for i in range(1, 366+1):\n",
    "            # Filepath setup\n",
    "            id_path = f\"{folder_name_arg}_{sub_folder_name_arg}\"\n",
    "            file_name = f\"{id_path}_thetao_thresh_300m_subset_{i}_{baseline_name_arg}.zarr\"\n",
    "            filepath = f'{temperature_directory}/Thresh{current_percentile}th/{folder_name_arg}/{id_path}/{file_name}'\n",
    "\n",
    "            # We check if the path/file exists in the target location\n",
    "            if os.path.exists(filepath):\n",
    "                if return_check_all:\n",
    "                    print(f'File for doy {i} exists at the specified path!\\n')\n",
    "            else:\n",
    "                if return_check_all:\n",
    "                    error_bar = \"------------------------------------------------------------------------------\"\n",
    "                    print(f'{error_bar}\\nWARNING: File for doy {i} not found at the specified path!\\n{error_bar}\\n')\n",
    "\n",
    "                error_counter += 1\n",
    "                error_list.append(i)\n",
    "\n",
    "        # If there are no missing doy datasets in our storage, we return None; otherwise, we return the list of missing doys!\n",
    "        if error_counter == 0:\n",
    "            print(f\"All {folder_name_arg} {sub_folder_name_arg} doys checked and present! {check_size_notif}\")\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"All {folder_name_arg} {sub_folder_name_arg} doys checked; you are missing {error_counter} doy datasets in total!\\n\")\n",
    "            print(\"The missing doys are: \")\n",
    "            print(error_list)\n",
    "            return error_list\n",
    "\n",
    "# Ex. climatology:\n",
    "missing_list_clim = gather_missing_from_storage(baseline_name_arg = \"Baseline9322\", folder_name_arg = \"Mid\", check_climatological_means = True)\n",
    "\n",
    "# Ex. thresholds:\n",
    "missing_list_thresh = gather_missing_from_storage(baseline_name_arg = \"Baseline9322\", folder_name_arg = \"Mid\",\n",
    "                                                 sub_folder_name_arg = \"Mid\", return_check_all = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a911489a-27f6-450b-a56d-fd5ff74d9e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to create an animation that shows the mean and percentile latitude and longitude maps for the full 1 - 366 period. \n",
    "def check_processed_datasets_with_an_animation(baseline_name_arg, folder_name_arg, \n",
    "                                               sub_folder_name_arg=None,\n",
    "                                               custom_output_filename=None,\n",
    "                                               show_climatological_means=False, \n",
    "                                               percentile=0, chosen_depth=0.494):\n",
    "    \n",
    "    ## Check proper arguments are provided for threshold datasets\n",
    "    if not show_climatological_means and percentile == 0:\n",
    "        raise ValueError(\"Please set a non-zero numeric percentile (based on the percentile you used above in your percentile datasets)!\")\n",
    "    \n",
    "    if not show_climatological_means and sub_folder_name_arg == None:\n",
    "        raise ValueError(\"Please provide a valid sub_folder_name!\")\n",
    "        \n",
    "    if show_climatological_means:\n",
    "        sub_folder_name_arg = None\n",
    "        percentile = 0\n",
    "        \n",
    "    \n",
    "    ## Gather the stored dataset filepaths\n",
    "    if show_climatological_means:\n",
    "        data_type = \"Clim\"\n",
    "        data_id   = f\"{folder_name_arg}\"\n",
    "        data_path = f\"{data_type}/{data_id}\"\n",
    "    else:\n",
    "        data_type = f\"Thresh{percentile}th\"\n",
    "        data_id   = f\"{folder_name_arg}_{sub_folder_name_arg}\"\n",
    "        data_path = f\"{data_type}/{folder_name_arg}/{data_id}\"\n",
    "        \n",
    "    data_directory = f'{temperature_directory}/{data_path}'   \n",
    "    paths = glob.glob(f'{data_directory}/{data_id}_thetao*{baseline_name_arg}.zarr')\n",
    "    \n",
    "    # A quick check to ensure we have located files given our arguments\n",
    "    if not paths:\n",
    "        start_error = \"No files found matching the pattern\"\n",
    "        cont_error = \"\\nPlease verify you inputted the proper baseline_name, folder_name, sub_folder_name, percentile, and show_climatological_means arguments!\"\n",
    "        raise FileNotFoundError(f\"{start_error}:\\n{data_directory}/{data_id}_thetao...300m...{baseline_name_arg}.zarr\\n{cont_error}\")\n",
    "    \n",
    "    \n",
    "    ## Fill a dictionary where all (1 to 366) doys are matched with their corresponding filepaths\n",
    "    doys_dict = {}\n",
    "    \n",
    "    for filepath in paths:\n",
    "        # Open and check what doys are in this file\n",
    "        ds = xr.open_zarr(filepath)\n",
    "        \n",
    "        # In my earlier percentile datasets (calculated in the same manner), my normalized_doys were saved as variable doys instead.\n",
    "        # this following if statement will likely be unnecessary for you.\n",
    "        if 'doy' in ds.coords:\n",
    "            ds = ds.rename({'doy': 'normalized_doy'}).expand_dims('normalized_doy')\n",
    "        \n",
    "        ds = ds.thetao\n",
    "        file_doys = ds['normalized_doy'].values\n",
    "\n",
    "        # Handle both single value and arrays\n",
    "        if np.isscalar(file_doys):\n",
    "            file_doys = [file_doys]\n",
    "         \n",
    "        # Map each doy to its file\n",
    "        for doy in file_doys:\n",
    "            doys_dict[int(doy)] = filepath\n",
    "         \n",
    "        ds.close()\n",
    "    \n",
    "    \n",
    "    ## Use a file and its features to set up the plot\n",
    "    available_doys = sorted(doys_dict.keys())\n",
    "    setup_file = doys_dict[available_doys[0]]\n",
    "    \n",
    "    if not show_climatological_means:\n",
    "        setup_ds = xr.open_zarr(filepath).thetao.drop_vars(\"quantile\")\n",
    "    else:\n",
    "        setup_ds = xr.open_zarr(filepath).thetao\n",
    "    \n",
    "    # Quick fix for my personal, early datasets\n",
    "    if 'doy' in setup_ds.coords:\n",
    "        setup_ds = setup_ds.rename({'doy': 'normalized_doy'}).expand_dims('normalized_doy')\n",
    "    \n",
    "    # Depth selection\n",
    "    setup_ds = setup_ds.sel(depth=chosen_depth, method='nearest')\n",
    "    first_depth = round(setup_ds.depth.item(), 1)\n",
    "    setup_ds = setup_ds.drop_vars(\"depth\")\n",
    "\n",
    "    lon = setup_ds.longitude.values\n",
    "    lat = setup_ds.latitude.values\n",
    "    \n",
    "    # Check for single or multiple-doys in the setup dataset, and return the thetao data for just one (the first) doy\n",
    "    if len(setup_ds['normalized_doy'].values.shape) == 0 or setup_ds['normalized_doy'].values.size == 1:\n",
    "        # Single day file\n",
    "        setup_ds   = setup_ds.drop_vars(\"normalized_doy\").squeeze()\n",
    "        setup_data = setup_ds.values\n",
    "    else:\n",
    "        # Multi-day file\n",
    "        setup_data = setup_ds.isel(normalized_doy=0).values\n",
    "    \n",
    "    setup_ds.close()\n",
    "    \n",
    "    \n",
    "    ## Initialize the plot    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6), \n",
    "                           subplot_kw={'projection': ccrs.Mercator()})\n",
    "    \n",
    "    pcm = ax.pcolormesh(\n",
    "        lon, lat, setup_data,\n",
    "        cmap='RdYlBu_r',\n",
    "        vmin=-5, vmax=35,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "    )\n",
    "    \n",
    "    ax.set_extent([0, 360, -30, 90], crs=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.LAND, color='lightgray')\n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth=0.8)\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    \n",
    "    title = ax.set_title('')\n",
    "    title_base = '(Relative to 1993-2022)' if baseline_name_arg == 'Baseline9322' else f'({baseline_name_arg})'\n",
    "    \n",
    "    \n",
    "    ## Animation function\n",
    "    def animate(i):\n",
    "        doy = available_doys[i]\n",
    "        filepath = doys_dict[doy]\n",
    "        \n",
    "        # Load the dataset\n",
    "        ds = xr.open_zarr(filepath).thetao.sel(depth=chosen_depth, method='nearest').drop_vars(\"depth\")\n",
    "        \n",
    "        # Quick fix for my personal, early datasets\n",
    "        if 'doy' in ds.coords:\n",
    "            ds = ds.rename({'doy': 'normalized_doy'}).expand_dims('normalized_doy')\n",
    "        \n",
    "        if not show_climatological_means:\n",
    "            ds = ds.drop_vars(\"quantile\")\n",
    "            \n",
    "        # Check if this is a single-day or multi-day file\n",
    "        doy_values = ds['normalized_doy'].values\n",
    "        \n",
    "        # Load the data if available for a single doy or select the correct doy in a dataset\n",
    "        if np.isscalar(doy_values) or doy_values.size == 1:\n",
    "            frame_data = ds.values\n",
    "        else:\n",
    "            doy_idx = np.where(doy_values == doy)[0][0]\n",
    "            frame_data = ds.isel(normalized_doy=doy_idx).values\n",
    "        \n",
    "        # Update the plot\n",
    "        pcm.set_array(frame_data.ravel())\n",
    "        title.set_text(f'{data_type} Day {doy} of the Year\\n{title_base}')\n",
    "        \n",
    "        ds.close()\n",
    "        return pcm, title\n",
    "    \n",
    "    \n",
    "    ## Create the resulting animation\n",
    "    type_message = \"climatological means\" if show_climatological_means else \"percentile thresholds\"\n",
    "    print(f\"Began animation for the {type_message} of the {data_id} datasets!\")\n",
    "    \n",
    "    chosen_doys = len(available_doys)\n",
    "    \n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, animate,\n",
    "        frames=chosen_doys,\n",
    "        interval=200,\n",
    "        blit=False,\n",
    "        repeat=True\n",
    "    )\n",
    "    \n",
    "    writer = animation.PillowWriter(fps=2)\n",
    "    \n",
    "    if custom_output_filename == None:\n",
    "        output_filename = f\"{data_id}_{data_type}_{baseline_name_arg}_{chosen_doys}_doys_total_with_depth_of_{first_depth}.gif\"\n",
    "    else:\n",
    "        output_filename = custom_output_filename\n",
    "    print(f\"Saving animation at: {output_filename}\") \n",
    "    \n",
    "    \n",
    "    ## Save the resulting animation\n",
    "    def print_frame_progress(current_frame, total_frames):\n",
    "        print(f\"\\r → Doy (Frame) Processed: {current_frame + 1}/{total_frames}\", end='', flush=True)\n",
    "\n",
    "    #with ProgressBar():\n",
    "    anim.save(output_filename, writer=writer, dpi=100,\n",
    "              progress_callback=print_frame_progress)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.close(fig)\n",
    "    print(f\"\\nAnimation finished and saved!\\n\")\n",
    "    \n",
    "    return anim\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Examples for creating the animations\n",
    "perc_anims_to_make = [\"All\"] \n",
    "#check_processed_datasets_with_an_animation(\"Baseline9322\", \"Atlantic\", \n",
    " #                                              sub_folder,\n",
    "  #                                             show_climatological_means=True, \n",
    "   #                                            percentile=90, chosen_depth=300)\n",
    "\n",
    "for sub_folder in perc_anims_to_make:    \n",
    "    check_processed_datasets_with_an_animation(\"Baseline9322\", \"Atlantic\", \n",
    "                                               sub_folder,\n",
    "                                               show_climatological_means=False, \n",
    "                                               percentile=90, chosen_depth=300)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangeo23",
   "language": "python",
   "name": "pangeo23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
