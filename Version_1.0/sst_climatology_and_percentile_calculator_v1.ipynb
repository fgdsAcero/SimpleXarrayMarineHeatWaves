{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da928dd-13eb-496a-83a8-c4bdf5a5b7c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' Main Required Dependencies ''';\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "''' Optional Dependencies ''';\n",
    "from datetime import datetime                    # For a ~30 year period check function (Optional)\n",
    "from dateutil.relativedelta import relativedelta # For a ~30 year period check function (Optional)\n",
    "from dask.diagnostics import ProgressBar # To download processed datasets with a progress bar (Optional but HIGHLY recommended!)\n",
    "\n",
    "''' Additional Optional Memory Monitoring Dependencies (memory monitoring is optional but HIGHLY recommended) ''';\n",
    "import psutil \n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "''' Additional Optional Dependencies for the File Validation, Verification, and Animation Section ''';\n",
    "import glob # For loading many datasets at once (required to produce an animation)\n",
    "import matplotlib.pyplot as plt # Required to produce an animation\n",
    "import matplotlib.animation as animation # Required to produce an animation\n",
    "import cartopy.crs as ccrs # For the chosen map projection (in animations)\n",
    "import cartopy.feature as cfeature # To add land features and coastlines (Optional if you use something else)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee69a162-a864-4c58-85ec-ccdf15cd5e16",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFE099; padding: 10px; border: 3px solid #FFC233; text-align: center; font-family: Georgia, serif; font-weight: bold; white-space: pre;\">||| - - - - - - - - - - - - - - - - - - - - - - - <|       SCRIPT NOTES       |> - - - - - - - - - - - - - - - - - - - - - - - |||</div>\n",
    "\n",
    "<div style=\"background-color: #EFFAFA; border: 2px solid #A2E2E2; font-family: Georgia, serif; padding: 10px\">\n",
    "    <div style=\"text-align: center;\">\n",
    "        This is the script to <strong>process percentile thresholds</strong> and <strong>climatological means</strong> from your temperature data (downloads not performed here).\n",
    "    </div>\n",
    "    <br>&#x27A1;&#xFE0E; This code calculates climatological means and percentile thresholds for 366 unique days of the year (including February 29) by default, using all available data. \n",
    "    <br>&#x27A1;&#xFE0E; Default constants and settings are provided to show an example of the usage of this code to (ultimately) detect marine heatwaves based closely on Hobday et al. (2016).\n",
    "    <br>&#x27A1;&#xFE0E; A fixed 30-year historical baseline period is used to calculate climatological means and percentiles by default. Adjust as need be (in the script-wide constants section).  \n",
    "    <br>&#x27A1;&#xFE0E; You can duplicate this script and run its copies simultaneously to download more percentiles/means at once (if need be).\n",
    "    <br>&#x27A1;&#xFE0E; If unusual or unexpected errors appear after attempting to run a cell again, restart the kernel!\n",
    "    <br>&#x27A1;&#xFE0E; If you save many files, you may compare their file sizes to see if any should be removed and redownloaded. Partially saved files will often have drastically different file sizes.\n",
    "    <br>&#x27A1;&#xFE0E; Lastly, in this script, constants define your root directory and output directories; you may tweak these and any constants \n",
    "    (and code) as you see fit to suit your needs!\n",
    "    <br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6847875-7869-4f2d-b04e-e4662650abec",
   "metadata": {},
   "source": [
    "<div style=\"color:#642CA9; padding: 10px; text-align: center; font-family: Georgia, serif; font-weight: bold; white-space: pre;\">°º¤ø,¸¸,ø¤º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø¤º°`°º¤ø,¸    IMPORTANT SCRIPT-WIDE CONSTANTS AND FUNCTIONS    °º¤ø,¸¸,ø¤º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø¤º°`°º¤ø,¸\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd09a47-297e-404a-9cae-7f946cf43eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' - - - Constants set up - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ''';\n",
    "percentile = 90             # Desired (temperature) percentile threshold. 90 here indicates the 90th percentile. \n",
    "minutes_per_mem_update = 10 # For memory monitoring (optional). Minutes (roughly) per memory update (to keep track of its use and avoid crashing/issues).\n",
    "memory_monitoring = True    # Required to be either False or True. Decide whether you want to use the optional memory monitoring feature (True) or not.\n",
    "\n",
    "''' - - - Historical baseline period setup - - - - - - - - - - - - - - - - - - - - - - - - - - - - ''';\n",
    "## Pick a FIXED time period whose data you will use to calculate the climatological means/percentiles\n",
    "baseline_period_slice_choice = slice('1994-01-01', '2023-12-31') # Included time period for mean/percentile calculations\n",
    "baseline_folder_name = \"Baseline9423\" # Custom identifier (for you to set based on the chosen baseline period) for your saved means/percentiles\n",
    "\n",
    "''' - - - Percentile/mean dataset filename/file path setup - - - - - - - - - - - - - - - - - - - - ''';\n",
    "# This is the default file path convention used here (all the * variables are determined automatically later in the code):\n",
    "# {region_id_folder_name}_{custom_id}_sst_{*Data Type}_subset_{*min Day of the Year)_to_{*max Day of the Year}_{baseline_folder_name}.zarr\n",
    "# Output example: Full_fgd_sst_thresh_subset_1_to_366_Baseline9423.zarr\n",
    "\n",
    "# Note: Please avoid using \".\" for any of the following custom identifier constants as it messes with file readability (as well as \n",
    "# any other special characters that could mess with file readability, such as \"*\" and \"/\").\n",
    "\n",
    "# *Data Type is set to be either \"clim\" or \"thresh\" for each processed dataset, short for either climatological means or percentile thresholds. \n",
    "# Instead of saving datasets with \"clim\" or \"thresh\", you may change these data type identifiers here.\n",
    "climatological_means_id = \"clim\"\n",
    "percentile_threshold_id = \"thresh\"\n",
    "\n",
    "# These are the customizable filename variables; adjust as needed. \n",
    "region_id_folder_name = \"Full\" # A \"regional\" identifier to save the severity dataset with. Example: \"North_Atlantic\", \"NA_top\", \"N_Atl_1\", etc.\n",
    "custom_id = \"fgd\"              # A custom identifier to further distinguish the saved data with. You can leave this empty as \"\".\n",
    "final_custom_id = f\"{custom_id}_\" if custom_id != \"\" else \"\" # If you set a custom_id (not left as \"\"), then it adds a \"_\" to it for the filename.\n",
    "\n",
    "# These are the components of the filename convention used to produce datasets named similarly to the output example above. Tweak as desired. \n",
    "processed_filename_start = f\"{region_id_folder_name}_{final_custom_id}sst\" # example component output: \"Full_fgd_sst\"\n",
    "processed_filename_end = f\"{baseline_folder_name}.zarr\"                    # example component output: \"Baseline9423.zarr\"\n",
    "processed_filename_perc_id = f\"{percentile_threshold_id}_subset\"            # example component output: \"clim_subset_\"\n",
    "processed_filename_clim_id = f\"{climatological_means_id}_subset\"            # example component output: \"thresh_subset_\"\n",
    "\n",
    "''' - - - Root and output directory setup - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ''';\n",
    "# These are the default directory conventions used here:\n",
    "# Root directory: {my_root_directory}/\n",
    "# Raw data directory: {my_root_directory}/{raw_data_folder_name}\n",
    "# Climatological mean data directory: {my_root_directory}/{clim_data_folder_name}\n",
    "# Percentile threshold data directory: {my_root_directory}/{perc_data_folder_name}\n",
    "# In other words, within your root directory, you will have your raw data, climatological mean, and chosen percentile threshold dataset folders.\n",
    "\n",
    "# Additionally, in the climatological mean and percentile threshold directories, a folder will be created named after your baseline_folder_name variable. \n",
    "# All the dataset outputs from this script will be stored in these baseline_folder_name folders. You can tweak this setup and the output directories.\n",
    "# Final output path example:  d2/Thresh90th/Baseline9423/Full/Full_fgd_sst_thresh_subset_1_to_366_Baseline9423.zarr\n",
    "\n",
    "my_root_directory = \"/d0\" # Should be your root directory, from which you access your data from and will save data to.\n",
    "raw_data_folder_name = \"Data\"\n",
    "clim_data_folder_name = \"Clim\"\n",
    "perc_data_folder_name = f\"Thresh{percentile}th\"\n",
    "raw_data_directory = f\"{my_root_directory}/{raw_data_folder_name}\"\n",
    "clim_data_directory = f\"{my_root_directory}/{clim_data_folder_name}/{baseline_folder_name}/{region_id_folder_name}\"\n",
    "perc_data_directory = f\"{my_root_directory}/{perc_data_folder_name}/{baseline_folder_name}/{region_id_folder_name}\"\n",
    "## OPTIONAL: You can remove \"/{region_id_folder_name}\" from the clim and perc data directories if you do not plan \n",
    "#            on subsetting the desired region of interest into multiple subregions.\n",
    "\n",
    "''' - - - Print (verification) statements - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ''';\n",
    "print(\"CHOSEN CONSTANTS:\\n\")\n",
    "print(f\"Percentile threshold:                {percentile}th percentile\")\n",
    "print(f\"Baseline slice object:               {baseline_period_slice_choice}\")\n",
    "print(f\"Corresponding baseline identifier:   {baseline_folder_name}\")\n",
    "print(f\"Memory monitoring:                   {memory_monitoring}\\n\")\n",
    "\n",
    "print(f\"Chosen root directory:                  {my_root_directory}\")\n",
    "print(f\"Chosen raw data directory:              {raw_data_directory}\")\n",
    "print(f\"Chosen output directory (means):        {clim_data_directory}\")\n",
    "print(f\"Chosen output directory (percentiles):  {perc_data_directory}\\n\")\n",
    "\n",
    "example_full_file_name = f\"{processed_filename_start}_{processed_filename_clim_id}_1_to_366_{processed_filename_end}\" # just an example\n",
    "print(f\"Example output filename:           {example_full_file_name}\")\n",
    "print(f\"Example output file path (full):   {clim_data_directory}/{example_full_file_name}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d540ab9-bf91-418f-b2b2-3a714ed85b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REQUIRED FUNCTIONS\n",
    "''' ------------------------------------------------------------------------------------------------ '''\n",
    "''' Function to normalize the unique day of the year value of each observed day in the format: 1-366 '''\n",
    "''' ------------------------------------------------------------------------------------------------ '''\n",
    "\n",
    "def normalize_dayofyear(time_coord):\n",
    "    doy = time_coord.dt.dayofyear\n",
    "    is_leap = time_coord.dt.is_leap_year\n",
    "\n",
    "    # This code ensures March 1 is always the unique day of the year (doy) 61, regardless of leap year status.\n",
    "    normalized_doy = xr.where(\n",
    "        (~is_leap) & (doy >= 60),  # For a non-leap year, doy 60 is March 1 initially. As such, we shift forward\n",
    "        doy + 1,                   # March 1 and any later days by 1 day. This makes March 1 have a doy of 61.\n",
    "        doy                        # Otherwise, we keep the day of the year values as is for leap years and the Jan. to Feb. 28 period.\n",
    "    )                              # Non-leap years end up having a missing doy 60 day by design.\n",
    "    return normalized_doy\n",
    "\n",
    "\n",
    "''' ------------------------------------------------------------------------------------- '''\n",
    "''' Function to just save the percentile threshold/climatological mean dataset to storage '''\n",
    "''' ------------------------------------------------------------------------------------- '''\n",
    "def save_dataset_to_storage(show_debug_arg, single_download_arg, \n",
    "                            current_percentile=None, start_val_arg=None, end_val_arg=None, \n",
    "                            ds_to_save=None):\n",
    "    '''\n",
    "    FUNCTION ARGUMENTS AND THEIR EXPLANATIONS\n",
    "    show_debug_arg:      If set to true, only shows the output file path and dataset, but doesn't save anything. \n",
    "                         show_debug (from the main function) must be set to False for the dataset to be saved.\n",
    "\n",
    "    single_download_arg: If set to True (from the inputted single_download variable in the main function), this\n",
    "                         function will only save the current dataset and stop immediately afterward.\n",
    "\n",
    "    current_percentile:  Is set to to either your set chosen percentile in the main function or remains None.\n",
    "                         If it remains None, the saved dataset is determined to be a climatological means dataset,\n",
    "                         rather than a percentile threshold dataset, adjusting the filename and file path accordingly.\n",
    "\n",
    "    start_val_arg:       Sets the lower (minimum) bound for the desired unique day of the year (doy) data to save \n",
    "                         (within the 1 to 366 range).\n",
    "\n",
    "    end_val_arg:         Sets the upper (maximum) bound for the desired unique day of the year (doy) data to save\n",
    "                         (within the 1 to 366 range).\n",
    "\n",
    "    ds_to_save:          The inputted mean/percentile dataset from the main function to be saved. Only the doys both\n",
    "                         between AND including the start_val_arg and end_val_arg will be saved in the current dataset.\n",
    "                         Example: if start_val_arg = 1 and end_val_arg = 30, doys 1 to 30 will be saved in the dataset.\n",
    "    ''';\n",
    "    \n",
    "    ## We check if we are saving a climatology dataset or percentile one after subsetting the datasets appropriately\n",
    "    print(\"Starting doy for current subset: \", start_val_arg)\n",
    "    print(\"Ending doy for current subset: \", end_val_arg, '\\n')\n",
    "\n",
    "    if start_val_arg == 1 and end_val_arg == 366:\n",
    "        final_ds_to_save = ds_to_save\n",
    "    else:\n",
    "        final_ds_to_save = ds_to_save.sel({'normalized_doy': slice(start_val_arg, end_val_arg)})\n",
    "    \n",
    "    desired_doy_period = f\"{start_val_arg}_to_{end_val_arg}\" # For use in the filename\n",
    "    \n",
    "    # Here, we load the file path destinations for the percentile threshold or climatological mean datasets\n",
    "    if current_percentile is None: # climatological means\n",
    "        print(f\"Current climatological means subset to save:\\n{final_ds_to_save}\\n\")\n",
    "        file_name = f\"{processed_filename_start}_{processed_filename_clim_id}_{desired_doy_period}_{processed_filename_end}\"\n",
    "        filepath = f'{clim_data_directory}/{file_name}'\n",
    "    else: # percentile thresholds\n",
    "        print(f\"Current {current_percentile}th percentile subset to save:\\n{final_ds_to_save}\\n\")\n",
    "        file_name = f\"{processed_filename_start}_{processed_filename_perc_id}_{desired_doy_period}_{processed_filename_end}\"\n",
    "        filepath = f'{perc_data_directory}/{file_name}'\n",
    "        \n",
    "    print(\"Filepath of subset: \", filepath)\n",
    "\n",
    "    # Lastly, we either finish by saving or stop if we are at the end of the debug\n",
    "    if show_debug_arg:\n",
    "        raise ValueError('End of debug. Proceed with the setting \"show_debug = False\" to start saving the dataset.')\n",
    "    else:\n",
    "        try: # This should run if the optional ProgressBar dependency was fulfilled.\n",
    "            with ProgressBar():\n",
    "                final_ds_to_save.to_zarr(filepath, mode='w', consolidated=True)\n",
    "        except Exception as e:\n",
    "            final_ds_to_save.to_zarr(filepath, mode='w', consolidated=True)\n",
    "        \n",
    "        print(f\"Saved subset: doys {start_val_arg} to {end_val_arg}\\nMoving on!\\n\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"Required functions loaded!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bee969-2bfc-4ad0-ad5d-0bca88fc8aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIONAL\n",
    "''' ------------------------------------------------------------------------------- '''\n",
    "''' Function to check whether the inputted time period is at least roughly 30 years '''\n",
    "''' ------------------------------------------------------------------------------- '''\n",
    "\n",
    "def rough_30_year_period_check(time_slice, tolerance=0.01):\n",
    "    start_str = time_slice.start\n",
    "    stop_str = time_slice.stop\n",
    "\n",
    "    start_date = datetime.strptime(start_str, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(stop_str, '%Y-%m-%d')\n",
    "\n",
    "    delta = relativedelta(end_date, start_date)\n",
    "    total_years = delta.years + delta.months/12 + delta.days/365.25\n",
    "    \n",
    "    return total_years, abs(total_years - 30) <= tolerance\n",
    "\n",
    "# Running a rough time check for the baseline provided\n",
    "total_time, is_time_slice_30_years = rough_30_year_period_check(baseline_period_slice_choice)\n",
    "\n",
    "if not is_time_slice_30_years:\n",
    "    error_message = \"Please check that your chosen baseline time slice covers a 30 year period.\"\n",
    "    raise ValueError(f\"{error_message}.\\n            The chosen slice covers roughly {total_time} years.\")\n",
    "else:\n",
    "    period_checked = f\"{baseline_period_slice_choice.start} to {baseline_period_slice_choice.stop}\"\n",
    "    check_pass_msg_1 = f\"The chosen baseline period of {period_checked} has been identified as roughly covering a 30 year period.\" \n",
    "    print(f\"{check_pass_msg_1} Do ensure this is the case separately.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf2c5d5-260c-4acb-a189-bb5e506b7bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "''' ----------------------------------------------- '''\n",
    "''' Function to keep track of and show memory usage '''\n",
    "''' ----------------------------------------------- ''' \n",
    "\n",
    "if memory_monitoring:\n",
    "    def monitor_memory(interval_minutes=5, log_file=None):\n",
    "        interval = interval_minutes * 60  \n",
    "        \n",
    "        while not stop_monitoring:\n",
    "            mem = psutil.Process(os.getpid()).memory_info().rss / (1024**3)  # in GB\n",
    "            print(f\" | Memory usage: {mem:.2f} GB | Memory: {psutil.virtual_memory().percent}% used | \")\n",
    "            \n",
    "            if log_file:\n",
    "                with open(log_file, 'a') as f:\n",
    "                    f.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')}: {mem:.2f} GB\\n\")\n",
    "            time.sleep(interval)\n",
    "    print(\"Memory function loaded!\")\n",
    "    \n",
    "else:\n",
    "    if \"monitor_memory\" in globals():\n",
    "        del monitor_memory\n",
    "    \n",
    "    print('You have chosen to not use memory monitoring. If this was a mistake, update the constant \"memory_monitoring\" to True!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49942d61-7a45-4c5e-988b-35881b5d6d11",
   "metadata": {},
   "source": [
    "<div style=\"color:#CD6600; padding: 10px; text-align: center; font-family: Georgia, serif; font-weight: bold; white-space: pre;\">°º¤ø,¸¸,ø¤º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø¤º°`°º¤ø,¸    LOADING FULL OBSERVED GLOBAL DATASETS    °º¤ø,¸¸,ø¤º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø¤º°`°º¤ø,¸\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a87417-0765-4d0a-8e5f-21f340d1e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' - - - Full dataset loading (from the set raw data directory) - - - - - - - - - - - - - - - - - - - - - - - ''';\n",
    "# Collect all applicable raw data datasets\n",
    "raw_data_files = f'{raw_data_directory}/sst*.nc' # Adjust to identify your saved files as need be. \n",
    "#                                                  The * allows us to later grab all the datasets that end with \".nc\" and start with \"sst\" here.\n",
    "\n",
    "# Set any latitude and longitude bounds for your processed means/percentiles and raw data datasets to use, if desired\n",
    "lat_bounds = slice(-15, 90) # latitude bounds\n",
    "lon_bounds = slice(0, 360)  # longitude bounds\n",
    "\n",
    "# Load the raw temperature datasets as one with xarray\n",
    "ds = xr.open_mfdataset(\n",
    "    raw_data_files,             # Glob pattern (the * grabs all datasets)\n",
    "    parallel=True,              # Enable parallel file opening \n",
    "    chunks='auto',              # Let dask choose optimal chunking \n",
    "    combine='by_coords',        # Merge based on coordinate values\n",
    "    engine='h5netcdf')          # Specify the engine (may not work with the wrong engine based on your dataset's format)\n",
    "\n",
    "full_ds = ds.sel(lat=lat_bounds, lon=lon_bounds).sst # Make sure to access the appropriate desired variable if it is not called \"sst\"\n",
    "print(\"Full raw data dataset:\\n\", full_ds, '\\n')\n",
    "\n",
    "# Make sure to perform any further desired filtering/cleaning here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4862c-305d-4a26-b0c4-a3c44ea338de",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' - - - Rechunk the raw data dataset - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ''';\n",
    "optimal_chunking = {'lat': 210, 'lon': 160}\n",
    "# These values were chosen as they were below 300 and divided the observations of my full dataset neatly.\n",
    "# For instance, there were 420 latitude observations and 1440 longitude observations in my loaded dataset. 420/210 = 2 and 1440/160 = 9.\n",
    "# It is recommended you check what kind of chunk sizes are best for your system and dataset for efficient saving/storage.\n",
    "\n",
    "temps_full = full_ds.chunk(optimal_chunking)\n",
    "print(\"Final (properly chunked) raw data dataset:\\n\", temps_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d12c80-665c-4589-9af3-9220ca854794",
   "metadata": {},
   "source": [
    "<div style=\"color:#104E8B; padding: 10px; text-align: center; font-family: Georgia, serif; font-weight: bold; white-space: pre;\">º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø¤º°`°º¤ø,¸    CALCULATING CLIMATOLOGICAL MEANS AND PERCENTILE THRESHOLDS    °º¤ø,¸¸,ø¤º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724ec30-60a2-4364-ab67-51cf08331786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' ----------------------------------------------------------------- '''\n",
    "''' Function to actually calculate the percentile threshold dataset to storage '''\n",
    "''' ----------------------------------------------------------------- '''\n",
    "\n",
    "# Function to calculate the percentile threshold values for specific depths\n",
    "def calculate_temp_thresh_or_clim_given_a_percentile(temp_data, baseline_slice, \n",
    "                                                     optimal_chunks, window_half_width=5, \n",
    "                                                     minutes_per_memory_update=5, chosen_percentile=None, \n",
    "                                                     start_chunking_doy=1, end_chunking_doy=366, doy_batch_size = None, \n",
    "                                                     show_debug=True, perform_memory_monitoring=False, \n",
    "                                                     single_download=False):     \n",
    "    '''\n",
    "    FUNCTION ARGUMENTS AND THEIR EXPLANATIONS\n",
    "    show_debug:                Setting this to True will go through all the steps up to generating the final percentile \n",
    "                               threshold or climatological means dataset output. Setting this to False will proceed with \n",
    "                               calculating and saving the desired dataset on your system based on your specified output \n",
    "                               directories (from the constants section near the top of the script).\n",
    "                               \n",
    "    temp_data:                 Your FULL inputted temperature dataset (loaded in the temperature dataset \n",
    "                               loading section before).\n",
    "    \n",
    "    baseline_slice:            Your set baseline period slice object (the baseline_period_slice_choice).\n",
    "    \n",
    "    optimal_chunks:            The chunks you want to use on your output dataset. They may be the same as the \n",
    "                               chunks used for the inputted raw data dataset, following a similar setup: \n",
    "                               {'latitude variable name': ###, 'longitude variable name': ###}. The chunks \n",
    "                               could be different if desired, and should be if you decide to tweak the code to \n",
    "                               modify your output dataset before saving it. Such a feature is not yet \n",
    "                               implemented here, but could be done through a modification like: \n",
    "                               window_data = window_data.sel(latitude=slice(-3, 0)), for instance.\n",
    "\n",
    "    window_half_width:         By default, the window half-width is set to 5. This means that the data of the \n",
    "                               5 days before and after each unique day of the year are used to calculate the\n",
    "                               means and percentiles of each \"central\" unique day of the year. \n",
    "                               For example, when show_debug is set to True, for the unique day of the year 1:\n",
    "                               Window Doys:  [362, 363, 364, 365, 366, 1, 2, 3, 4, 5, 6] \n",
    "                               Accordingly, the data from the 5 days before and after day 1 (and day 1's data) \n",
    "                               are used to calculate the means or percentiles of day 1. This data window shifts \n",
    "                               to 363 to 366 and 1 to 7 for the 2nd unique day of the year, and so on.\n",
    "                               \n",
    "    minutes_per_memory_update: The desired number of minutes per each memory usage update from the optional\n",
    "                               monitor_memory() function. Make sure the appropriate dependencies listed at the top\n",
    "                               of this script are installed and active, and that the monitor_memory() function is loaded.\n",
    "                               Reaching 100% memory use will cause your Jupyterhub server to crash and require a restart. \n",
    "                               You may avoid this by saving smaller datasets (such as by saving less days of the year in \n",
    "                               a single dataset and performing more spatial subsetting of larger regions of interest).\n",
    "    \n",
    "    chosen_percentile:         Must be set to a desired percentile (like the defined percentile constant) or None. Setting\n",
    "                               a numeric percentile (greater than 0 and less than 100) will calculate percentile thresholds,\n",
    "                               while setting chosen_percentile to None will calculate climatological means instead.\n",
    "    \n",
    "    start_chunking_doy:        When batch saving is disabled (doy_batch_size is set to None), start_chunking_doy sets the \n",
    "                               earliest possible unique day of the year (doy) from the full (1-366) climatological\n",
    "                               mean/percentile threshold dataset to be saved (up to the final doy set by end_chunking_doy). \n",
    "                               Essentially, with batch saving disabled, you save only the doy interval you set from \n",
    "                               start_chunking_doy to end_chunking_doy. \n",
    "                               \n",
    "                               When batch saving is enabled (doy_batch_size is set to a number), start_chunking_doy is used\n",
    "                               to identify the earliest possible doy batch to be saved out of a pre-determined set of\n",
    "                               possible doy batches determined by your doy_batch_size (see doy_batch_size for details). \n",
    "                               If you had previously saved a doy batch like 21 to 40 (with a doy_batch_size of 20 doys), for \n",
    "                               the same doy_batch_size, using any of the numbers present within the 21 to 40 interval will\n",
    "                               redownload the same doy batch (of doys 21 to 40); accordingly, setting start_chunking_doy to \n",
    "                               41 will proceed to save the next batch (of doys 41 to 60) and setting start_chunking_doy to 1 \n",
    "                               will proceed to save the earlier, initial batch (of doys 1 to 20).\n",
    "    \n",
    "    end_chunking_doy:          When batch saving is disabled (doy_batch_size is set to None), end_chunking_doy sets the \n",
    "                               latest possible unique day of the year (doy) from the full (1-366) climatological\n",
    "                               mean/percentile threshold dataset to be saved up to (from the initial doy set by \n",
    "                               start_chunking_doy). Essentially, with batch saving disabled, you save only the doy \n",
    "                               interval you set from start_chunking_doy to end_chunking_doy.\n",
    "\n",
    "                               When batch saving is enabled (doy_batch_size is set to a number), end_chunking_doy is used\n",
    "                               to identify the latest possible doy batch to be saved out of a pre-determined set of\n",
    "                               possible doy batches determined by your doy_batch_size (see doy_batch_size for details).\n",
    "                               \n",
    "                               When saving more than one dataset (single_download is set to False), batches of doys will\n",
    "                               continue to be saved (in independent files named by default based on the doys they contain,\n",
    "                               like 21_to_40, 41_to_60, and so on, for a batch size of 20 and similarly for other batch sizes)\n",
    "                               until the batch with the end_chunking_doy is reached. For instance, for a batch size of 20, \n",
    "                               if the start_chunking_doy is set to 1 and the end_chunking_doy is set to 100, 20-doy batches \n",
    "                               will be saved until the batch that contains 100, the 81_to_100 batch, is reached; no other\n",
    "                               batches would be downloaded. \n",
    "                               \n",
    "                               If only one dataset is being saved (single_download is set to True), end_chunking_doy can \n",
    "                               be any number after the start_chunking_doy, as only the earliest batch detected to have the \n",
    "                               start_chunking_doy will be saved. \n",
    "\n",
    "    doy_batch_size:            When this is set to False, only the unique day of the year (doy) interval you set from \n",
    "                               start_chunking_doy to end_chunking_doy will be saved. When doy_batch_size is set to \n",
    "                               any number between 1 and 366, you enable batch saving. This means that the percentile/mean\n",
    "                               data of the 366 total doys (by default) are automatically split into consecutive batches that\n",
    "                               contain the data of (at most) doy_batch_size doys. For instance, for a doy_batch_size of 20, \n",
    "                               one may save the full 1-366 mean/percentile dataset in separate, smaller files that contain \n",
    "                               the data of 20 doys, such as doys 1 to 20, 21 to 40, 41 to 60, 341 to 360, and the smallest \n",
    "                               final batch of 361 to 366. You can set show_debug to True to see how saving differs between \n",
    "                               using and not using batch saving, as well as batch saving with different doy_batch_size batches.\n",
    "                               \n",
    "    perform_memory_monitoring: If set to True, this will allow the use of the optional memory monitoring feature, where memory \n",
    "                               use every minutes_per_memory_update minutes (roughly) is printed out. Of course, the required memory \n",
    "                               monitoring function (in the script-wide constants and functions section) must be run and its \n",
    "                               dependencies installed and loaded for it to work; if they are not, the code safely proceeds without \n",
    "                               memory monitoring (when it is set to True). \n",
    "                               \n",
    "                               Setting perform_memory_monitoring to False will disable the optional memory monitoring feature. \n",
    "                               This feature may be helpful in allowing you to identify if your system is close to using up all \n",
    "                               (100%) of the available memory based on your chosen settings/constants/inputs, (which ought \n",
    "                               to be avoided, as this would crash your Jupyterhub environment, requiring a restart).\n",
    "    \n",
    "    single_download:           If set to True, this will ensure only the (earliest) specified dataset is downloaded \n",
    "                               (see doy_batch_size and start_chunking_doy). If set to False, the forceful termination of \n",
    "                               possibly saving any further datasets at the end of the script is ignored (only for batch \n",
    "                               saving; see doy_batch_size).\n",
    "    ''';\n",
    "    \n",
    "    if show_debug:\n",
    "        debug_message_1 = \"You have set show_debug to true; this will show how the percentiles/means are processed based on your inputted arguments \\nand provide a preview of the output.\\n\"\n",
    "        debug_message_2 = \"\\nIf you are satisfied with the output (and your arguments), compute and save the calculated percentiles/means by setting\\nshow_debug to false.\\n\"\n",
    "        print(debug_message_1, debug_message_2)\n",
    "    \n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Part 0: Running a few quick error checks for the provided arguments!\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "              \n",
    "    # Checking start and end bounds\n",
    "    if start_chunking_doy < 1 or end_chunking_doy > 366:\n",
    "        raise ValueError(\"Please provide a start_chunking_doy that is ≥ 1 and an end_chunking_doy that is ≤ 366.\")\n",
    "        \n",
    "    if end_chunking_doy < start_chunking_doy: ## bug in other code, this is named chunk_end, not explicitly called.\n",
    "        raise ValueError(\"Please make sure your end_chunking_doy is greater than your start_chunking_doy; these are your dataset processing bounds.\")\n",
    "        \n",
    "    # Establishing if we calculating percentiles or means \n",
    "    calculate_means = True if (chosen_percentile == None) else False    \n",
    "    \n",
    "    # Checking if we are performing batch-saving (or saving everything all at once)\n",
    "    if doy_batch_size == None: # We are saving everything at once, no need to batch save\n",
    "        batch_saving = False\n",
    "        doy_batch_size = end_chunking_doy - start_chunking_doy + 1 # Setting this to the output day of the year coordinate (size) value \n",
    "        \n",
    "    else:\n",
    "        batch_saving = True # We are not saving everything at once but want to do so in batches!\n",
    "        \n",
    "    \n",
    "    if show_debug: \n",
    "        print(\"All clear!\\n\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Part 1: Assign normalized unique day of the year (doy) values to the sliced observation dataset\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    print(f\"Chosen baseline slice: {baseline_slice}\")\n",
    "    print(f\"Chosen window half-width: {window_half_width}\")\n",
    "    print(f\"(This means we use the data in the {window_half_width} days before and after each day of the year (doy) to calculate the means/percentiles of each doy.)\", '\\n')\n",
    "    \n",
    "    if calculate_means:\n",
    "        print(f\"Calculating means in batches of (at most) {doy_batch_size} doys between {start_chunking_doy} and {end_chunking_doy}.\\n\")\n",
    "    else:\n",
    "        percentile_used = chosen_percentile/100\n",
    "        print(f\"Final percentile used (in calculations): {percentile_used} ({chosen_percentile}th percentile)\", '\\n')\n",
    "        print(f\"Calculating percentiles in batches of (at most) {doy_batch_size} doys between {start_chunking_doy} and {end_chunking_doy}.\\n\")\n",
    "    \n",
    "    \n",
    "    # Doy values for specific dates (for later)\n",
    "    feb28_doy = 59\n",
    "    feb29_doy = 60\n",
    "    mar1_doy = 61\n",
    "    \n",
    "    ## Extracting baseline period data\n",
    "    temp_baseline_data = temp_data.sel(time=baseline_slice)\n",
    "    if show_debug: print(\"Original Temperature Baseline Period Data: \", '\\n', temp_baseline_data, '\\n')\n",
    "   \n",
    "    # Assigning normalized doy values to the baseline period dataset\n",
    "    temp_norm = temp_baseline_data.assign_coords(\n",
    "        normalized_doy=('time', normalize_dayofyear(temp_baseline_data.time).data))\n",
    "    if show_debug: print(\"Temperatures with Normalized Unique Days of the Year (1-366): \", '\\n', temp_norm, '\\n')\n",
    "        \n",
    "    '''\n",
    "    # Totally optional debug here: show ALL normalized day of the year (doy) values;\n",
    "    # all years are in the 366-day format, with some missing day 60 (feb 29)\n",
    "    with np.printoptions(threshold=np.inf):\n",
    "        print(temp_norm.normalized_doy.values) \n",
    "    '''\n",
    "    \n",
    "    if show_debug: \n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Part 2: Get the actual doy values of the baseline period data (should be 1 - 366)\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "    unique_doys = np.unique(temp_norm.normalized_doy.data)\n",
    "    unique_doys = unique_doys[~np.isnan(unique_doys)]  # Remove any NaN values\n",
    "    unique_doys = unique_doys.astype(int)  # Ensure integer day-of-year values\n",
    "    if show_debug: print(f\"Found {len(unique_doys)} unique day-of-year values!\")\n",
    "    if show_debug: print(\"Unique doys:\", '\\n', unique_doys, '\\n')\n",
    "\n",
    "         \n",
    "    global stop_monitoring\n",
    "    if show_debug: \n",
    "        choice_message = \"climatological mean\" if calculate_means else \"percentile threshold\"\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "        print(f\"Part 3: Calculate the desired {choice_message} data for the desired doy(s).\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "        stop_monitoring = True # We don't want to start showing memory use.\n",
    "\n",
    "        if perform_memory_monitoring:\n",
    "            msgmem = \"Please ensure the memory monitoring functions are run and their dependencies are installed.\"\n",
    "            print(f\"Memory monitoring is enabled. When saving the dataset, the memory usage will be displayed!\\n{msgmem}\\n\")\n",
    "        else:\n",
    "            mem1msg = \"The optional memory monitoring feature is disabled\"\n",
    "            mem2msg = \"To display memory usage:\"\n",
    "            mem3msg = \"set perform_memory_monitoring to True and the desired minutes_per_memory_update after running the memory functions.\"\n",
    "            print(f\"{mem1msg}. {mem2msg}\\n{mem3msg}\\n\")\n",
    "    else:\n",
    "        # We start monitoring here so that it only runs once\n",
    "        stop_monitoring = False # We do want to start showing memory use.\n",
    "        \n",
    "        if perform_memory_monitoring:\n",
    "            try:\n",
    "                monitor_thread = threading.Thread(target=monitor_memory, kwargs={'interval_minutes': minutes_per_memory_update})\n",
    "                monitor_thread.daemon = True\n",
    "                monitor_thread.start()\n",
    "            except Exception as e:\n",
    "                msgafter = \"Proceeding without memory monitoring until the issues are resolved!\"\n",
    "                print(f\"The required memory monitoring functions were not run and/or its required dependencies are missing.\\n{msgafter}\\n\") \n",
    "        else:\n",
    "            print(\"Proceeding without monitoring memory!\\n\")\n",
    "        \n",
    "    # Error messages for later\n",
    "    chunk_message_finished = \"Set chunk end reached!\\n\"\n",
    "    no_feb29_possible_warning = \"WARNING: Cannot interpolate Feb 29; missing Feb 28 or Mar 1 data!\\n\"\n",
    "    \n",
    "    # Initialize a dictionary for the climatological means or percentile thresholds\n",
    "    seas_dict = {}\n",
    "        \n",
    "    # Bool for debug purposes\n",
    "    shown_once = False\n",
    " \n",
    "    # Loop for doys 1 - 366 (excluding Feb 29, doy 60)\n",
    "    for doy in unique_doys:\n",
    "        # We skip February 29th (to interpolate later)\n",
    "        if doy == feb29_doy:  \n",
    "            continue # Note: doy 60 data is still used within the appropriate window_data when available\n",
    "        \n",
    "        # Create window around this DOY\n",
    "        window_doys = []\n",
    "        \n",
    "        for w in range(-window_half_width, window_half_width + 1):\n",
    "            target_doy = doy + w\n",
    "            \n",
    "            if show_debug and not shown_once: \n",
    "                print(\"Day of the year: \", doy, \"| Target Window Index: \", w, \"| Target Window Value: \", target_doy)\n",
    "\n",
    "            # Handle year wraparound properly\n",
    "            if target_doy < 1:\n",
    "                target_doy += 366\n",
    "            elif target_doy > 366:\n",
    "                target_doy -= 366\n",
    "            \n",
    "            # Handle year boundaries by keeping only valid doys\n",
    "            if target_doy in unique_doys:\n",
    "                window_doys.append(target_doy)\n",
    "            \n",
    "            if show_debug and not shown_once: print(\"Window Doys: \", window_doys, '\\n')\n",
    "\n",
    "        # Now, we select the data for this window\n",
    "        window_data = temp_norm.where(temp_norm.normalized_doy.isin(window_doys), drop=True)\n",
    "        \n",
    "        if show_debug and not shown_once: print(f\"Final window data for doy {doy} from the baseline period dataset: \", '\\n', window_data, '\\n')\n",
    "        \n",
    "        # Now, we calculate the percentile threshold/climatological mean across the time dimension\n",
    "        if window_data.time.size > 0:\n",
    "            # We calculate the climatological mean if that is what is desired\n",
    "            if calculate_means:\n",
    "                doy_to_save = window_data.mean(dim = 'time', skipna = True).expand_dims(normalized_doy=[doy])\n",
    "            # Otherwise, we calculate the percentile for a doy \n",
    "            else:\n",
    "                doy_to_save = window_data.chunk({'time':-1}).quantile(percentile_used, dim='time', skipna=True).expand_dims(normalized_doy=[doy])\n",
    "            seas_dict[doy] = doy_to_save\n",
    "\n",
    "            if show_debug and not shown_once:\n",
    "                message_type = \"climatological means\" if calculate_means else \"percentile thresholds\"\n",
    "                print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "                print(f\"Part 4: Store the {message_type} across all doys in an empty dictionary!\")\n",
    "                print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "                first_part = f\"Dictionary presently updated for doy {doy} with the time-averaged final window dataset in Part 3.\"\n",
    "                print(f\"{first_part}\\n\\nDictionary entry:\\n\", seas_dict[doy], '\\n')\n",
    "                shown_once = True  \n",
    "                \n",
    "    # After the for loop over the 1-366 day of the year range, we handle February 29th using linear interpolation\n",
    "    if (feb29_doy in unique_doys):\n",
    "        # If we have a dictionary with our percentiles/climatologies...\n",
    "        if feb28_doy in seas_dict and mar1_doy in seas_dict:\n",
    "            feb_28_ds = seas_dict[feb28_doy].squeeze().drop_vars('normalized_doy')\n",
    "            mar_1_ds = seas_dict[mar1_doy].squeeze().drop_vars('normalized_doy')\n",
    "            seas_dict[feb29_doy] = 0.5 * (feb_28_ds + mar_1_ds)\n",
    "            seas_dict[feb29_doy] = seas_dict[feb29_doy].expand_dims(normalized_doy=[feb29_doy])\n",
    "            if show_debug: print(\"Interpolated February 29 dataset (doy 60) in the dictionary:\\n\", seas_dict[feb29_doy], '\\n')\n",
    "        else:\n",
    "            print(no_feb29_possible_warning)\n",
    "            \n",
    "    # Now we proceed with the full climatology dictionary\n",
    "    if show_debug: \n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Part 5: Creating the complete dataset from the dictionary\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "    # We create the correct coordinates from our dictionary for our final dataset\n",
    "    doy_coords = np.array(sorted(seas_dict.keys())) # array for full year (1 to 366, if leap)\n",
    "    if show_debug: print(\"Dictionary Keys of All Calculated Unique Day of the Year Datasets\\n\", \n",
    "                         \"(Should include all values from 1 to 366):\\n\", doy_coords, '\\n')\n",
    "\n",
    "    # We stack the resulting dictionary datasets while maintaining the correct order\n",
    "    seas_list = [seas_dict[doy] for doy in doy_coords]\n",
    "    seas_year = xr.concat(seas_list, dim='normalized_doy')\n",
    "    seas_year = seas_year.assign_coords(normalized_doy=('normalized_doy', doy_coords))\n",
    "    seas_year = seas_year.chunk(optimal_chunks)\n",
    "    \n",
    "    # Additional chunking of the resulting doy mean/percentile dataset to prevent crashes during saving.\n",
    "    max_doy_chunking_val = 61 # Can be reduced for more stability (or increased to allow larger chunk size values)\n",
    "\n",
    "    if doy_batch_size <= max_doy_chunking_val: # For small datasets, it's fine to chunk by the total data available\n",
    "        seas_year = seas_year.chunk({'normalized_doy': doy_batch_size})\n",
    "        \n",
    "    else: # For larger datasets with means/percentiles for many doys, we find the best, largest value to chunk by (that produces no remainders)\n",
    "        best_chunk_val = max_doy_chunking_val # Default; may lead to non-perfect chunking if no best (largest) divisor (under the max allowed) is found\n",
    "        \n",
    "        for divisor in range(max_doy_chunking_val, 0, -1): # Iterate from the maximum allowed divisor down to 1 to determine the best chunk size value\n",
    "            if doy_batch_size % divisor == 0: # The current largest divisor is found to leave no remainder\n",
    "                if divisor != 1: # If we find a divisor that is not 1 (the smallest possible value), we set that as the best chunk size to use\n",
    "                    best_chunk_val = divisor\n",
    "                break # End the best chunk-size (divisor) calculator for loop\n",
    "\n",
    "        # Lastly, apply the best largest chunk size determined\n",
    "        seas_year = seas_year.chunk({'normalized_doy': best_chunk_val})\n",
    "\n",
    "    print(\"\\nFinal Climatology Dataset:\\n\", seas_year, '\\n')\n",
    "\n",
    "    \n",
    "    if show_debug: \n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "        print(f\"Part 5: Saving the output dataset!\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "        \n",
    "    # Coordinate values for batch saving (not full climatology dataset saving)\n",
    "    coord_values = seas_year['normalized_doy'].values\n",
    "\n",
    "    # We are saving the dataset in batches\n",
    "    if batch_saving:\n",
    "        if show_debug: \n",
    "            print(f\"Proceeding with batch saving in batches of (at most) {doy_batch_size} doys between {start_chunking_doy} and {end_chunking_doy}!\\n\")\n",
    "            \n",
    "        for i in range(0, len(coord_values), doy_batch_size):\n",
    "            # First, we gather the starting and ending values of the processed chunk\n",
    "            start_val = coord_values[i]\n",
    "\n",
    "            # Figure out when to stop downloading data (based on the set end_chunking_doy value)\n",
    "            if start_val > end_chunking_doy:\n",
    "                continue\n",
    "            \n",
    "            # Grab the end index and value\n",
    "            end_idx = min(i + doy_batch_size, len(coord_values))\n",
    "            end_val = coord_values[end_idx - 1]\n",
    "\n",
    "             # Quick check to see where to begin downloading a batch from...\n",
    "            if start_val < start_chunking_doy: # We skip any batches where the starting doy is earlier than the start_chunking_doy.\n",
    "                # Howver, if the start_chunking_doy value is within the current batch, we allow it to be the first to be saved.\n",
    "                if start_val <= start_chunking_doy <= end_val:\n",
    "                    pass \n",
    "                else: # If the batch does not contain the start_chunking_doy and only contains doys before it, then we skip saving it!\n",
    "                    continue \n",
    "\n",
    "            # Save the dataset (subsetting occurs in the function)              \n",
    "            save_dataset_to_storage(show_debug_arg=show_debug, single_download_arg=single_download,\n",
    "                                    current_percentile=chosen_percentile, \n",
    "                                    start_val_arg=start_val, end_val_arg=end_val, \n",
    "                                    ds_to_save=seas_year) \n",
    "            \n",
    "            # If we only want to download a single dataset:\n",
    "            if single_download:     \n",
    "                stop_monitoring = True # stop memory monitoring\n",
    "                raise ValueError(\"Single dataset file-saving finished. Please enter a new desired chunk starting value to begin from.\")\n",
    "            \n",
    "    # We are saving the full dataset\n",
    "    else:   \n",
    "        if show_debug: \n",
    "            print(\"Proceeding with saving the desired doy interval data all at once!\\n\")\n",
    "            \n",
    "        save_dataset_to_storage(show_debug_arg=show_debug, single_download_arg=single_download,\n",
    "                                current_percentile=chosen_percentile, start_val_arg=start_chunking_doy, \n",
    "                                end_val_arg=end_chunking_doy, ds_to_save=seas_year) \n",
    "\n",
    "        \n",
    "    stop_monitoring = True # Reset the monitoring before the next loop\n",
    "    \n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Calling the main function to calculate and store the 90th percentiles and climatological means!\n",
    "\n",
    "calculate_temp_thresh_or_clim_given_a_percentile(temp_data=temps_full, baseline_slice=baseline_period_slice_choice,\n",
    "                                                 optimal_chunks=optimal_chunking, window_half_width=5, \n",
    "                                                 minutes_per_memory_update=minutes_per_mem_update, chosen_percentile=None, \n",
    "                                                 start_chunking_doy=1, end_chunking_doy=366, doy_batch_size=None,\n",
    "                                                 show_debug=True, perform_memory_monitoring=True, single_download=False)\n",
    "\n",
    "calculate_temp_thresh_or_clim_given_a_percentile(temp_data=temps_full, baseline_slice=baseline_period_slice_choice,\n",
    "                                                 optimal_chunks=optimal_chunking, window_half_width=5,\n",
    "                                                 minutes_per_memory_update=minutes_per_mem_update, chosen_percentile=percentile,\n",
    "                                                 start_chunking_doy=1, end_chunking_doy=366, doy_batch_size=None,\n",
    "                                                 show_debug=True, perform_memory_monitoring=True, single_download=False)\n",
    "\n",
    "print(\"We have finished saving all desired doy datasets completely!\")\n",
    "stop_monitoring = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c102917e-226f-4081-b4bc-b454f4fde893",
   "metadata": {},
   "source": [
    "<div style=\"color:#008B00; padding: 10px; text-align: center; font-family: Georgia, serif; font-weight: bold; white-space: pre;\">ø¤º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø¤º°`°º¤ø,¸  FILE VALIDATION, VERIFICATION, AND ANIMATIONS  °º¤ø,¸¸,ø¤º°`°º¤ø,¸,ø¤°º¤ø,¸¸,ø¤\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a911489a-27f6-450b-a56d-fd5ff74d9e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' ---------------------------------------------------------------------------------------------------------- '''\n",
    "''' Function to make an animation of the calculated climatological mean/percentile latitude and longitude maps '''\n",
    "''' ---------------------------------------------------------------------------------------------------------- '''\n",
    "\n",
    "'''\n",
    "NOTES\n",
    "- Make sure that your climatological mean/percentile threshold dataset folders contain unique (no repeating) datasets that encapsulate\n",
    " the full 1-366 days of the year (doy). Making animations with some doys missing has not been tested yet, but this code may be tweaked to suit\n",
    " your needs. If need be, you can edit the code after the \"## ***\" line to access specific files, as that code section determines the files to be read.\n",
    " \n",
    "- Animations are produced in the same location this script is present. You may tweak this if desired.\n",
    "''';\n",
    "\n",
    "def check_processed_datasets_with_an_animation(baseline_name_arg, region_id_arg, custom_id_arg,\n",
    "                                               custom_output_filename=None,\n",
    "                                               chosen_percentile=None):\n",
    "    \n",
    "    ## Check proper arguments are provided for threshold datasets\n",
    "    if chosen_percentile == None:\n",
    "        data_type = \"Means\"\n",
    "        show_climatological_means = True\n",
    "    else:\n",
    "        data_type = f\"{chosen_percentile}th_Percentiles\" \n",
    "        show_climatological_means = False\n",
    "        \n",
    "        if chosen_percentile <= 0:\n",
    "            raise ValueError(\"Please set a positive, non-zero numeric percentile (based on the percentile you used above in your percentile datasets)!\")\n",
    "\n",
    "\n",
    "    ## *** \n",
    "    ## Gather the stored dataset filepaths \n",
    "    if show_climatological_means:\n",
    "        data_directory = f\"{my_root_directory}/{clim_data_folder_name}/{baseline_name_arg}/{region_id_arg}\"\n",
    "    else:\n",
    "        data_directory = f\"{my_root_directory}/{perc_data_folder_name}/{baseline_name_arg}/{region_id_arg}\"\n",
    "    \n",
    "    glob_path = f\"{data_directory}/{region_id_arg}_{custom_id_arg}sst_*_{baseline_name_arg}.zarr\"\n",
    "\n",
    "    # Gather file paths from the set glob directory path constructed from the script-wide constants and function's inputs\n",
    "    paths = glob.glob(glob_path)\n",
    "    \n",
    "    # A quick check to ensure we have located files given our arguments\n",
    "    if not paths:\n",
    "        start_error = \"No files found matching the pattern\"\n",
    "        cont_error = \"\\nPlease verify you inputted the proper baseline_name, folder_name, sub_folder_name, percentile, and show_climatological_means arguments!\"\n",
    "        raise FileNotFoundError(f\"{start_error}:\\n{data_directory}_sst...{baseline_name_arg}.zarr\\n{cont_error}\")\n",
    "    \n",
    "    \n",
    "    ## Fill a dictionary where all (1 to 366) doys are matched with their corresponding filepaths\n",
    "    doys_dict = {}\n",
    "    \n",
    "    for filepath in paths:\n",
    "        # Open and check what doys are in this file\n",
    "        ds = xr.open_zarr(filepath).sst\n",
    "        file_doys = ds['normalized_doy'].values\n",
    "\n",
    "        # Handle both single value and arrays\n",
    "        if np.isscalar(file_doys):\n",
    "            file_doys = [file_doys]\n",
    "         \n",
    "        # Map each doy to its file\n",
    "        for doy in file_doys:\n",
    "            doys_dict[int(doy)] = filepath\n",
    "         \n",
    "        ds.close()\n",
    "    \n",
    "    \n",
    "    ## Use a file and its features to set up the plot\n",
    "    available_doys = sorted(doys_dict.keys())\n",
    "    setup_file = doys_dict[available_doys[0]]\n",
    "    \n",
    "    if not show_climatological_means:\n",
    "        setup_ds = xr.open_zarr(filepath).sst.drop_vars(\"quantile\")\n",
    "    else:\n",
    "        setup_ds = xr.open_zarr(filepath).sst\n",
    "    \n",
    "    # Quick fix for my personal, early datasets (should be unnecessary for you; you could name your variables differently too, as I did here)\n",
    "    if 'doy' in setup_ds.coords:\n",
    "        setup_ds = setup_ds.rename({'doy': 'normalized_doy'}).expand_dims('normalized_doy')\n",
    "    \n",
    "    lon = setup_ds.lon.values\n",
    "    lat = setup_ds.lat.values\n",
    "    \n",
    "    # Check for single or multiple-doys in the setup dataset, and return the thetao data for just one (the first) doy\n",
    "    if len(setup_ds['normalized_doy'].values.shape) == 0 or setup_ds['normalized_doy'].values.size == 1:\n",
    "        # Single day file\n",
    "        setup_ds   = setup_ds.drop_vars(\"normalized_doy\").squeeze()\n",
    "        setup_data = setup_ds.values\n",
    "    else:\n",
    "        # Multi-day file\n",
    "        setup_data = setup_ds.isel(normalized_doy=0).values\n",
    "    \n",
    "    setup_ds.close()\n",
    "    \n",
    "    \n",
    "    ## Initialize the plot    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6), \n",
    "                           subplot_kw={'projection': ccrs.Mercator()})\n",
    "    \n",
    "    pcm = ax.pcolormesh(\n",
    "        lon, lat, setup_data,\n",
    "        cmap='RdYlBu_r',\n",
    "        vmin=-5, vmax=35,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "    )\n",
    "    \n",
    "    ax.set_extent([0, 360, -5, 90], crs=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.LAND, color='lightgray')\n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth=0.8)\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    \n",
    "    title = ax.set_title('')\n",
    "    title_base = '(Relative to 1993-2022)' if baseline_name_arg == 'Baseline9322' else f'({baseline_name_arg})'\n",
    "    \n",
    "    \n",
    "    ## Animation function\n",
    "    def animate(i):\n",
    "        doy = available_doys[i]\n",
    "        filepath = doys_dict[doy]\n",
    "        \n",
    "        # Load the dataset\n",
    "        ds = xr.open_zarr(filepath).sst\n",
    "      \n",
    "        if not show_climatological_means:\n",
    "            ds = ds.drop_vars(\"quantile\")\n",
    "            \n",
    "        # Check if this is a single-day or multi-day file\n",
    "        doy_values = ds['normalized_doy'].values\n",
    "        \n",
    "        # Load the data if available for a single doy or select the correct doy in a dataset\n",
    "        if np.isscalar(doy_values) or doy_values.size == 1:\n",
    "            frame_data = ds.values\n",
    "        else:\n",
    "            doy_idx = np.where(doy_values == doy)[0][0]\n",
    "            frame_data = ds.isel(normalized_doy=doy_idx).values\n",
    "        \n",
    "        # Update the plot\n",
    "        pcm.set_array(frame_data.ravel())\n",
    "        title.set_text(f'{data_type}: Day {doy} of the Year\\n{title_base}')\n",
    "        \n",
    "        ds.close()\n",
    "        return pcm, title\n",
    "    \n",
    "    \n",
    "    ## Create the resulting animation\n",
    "    type_message = \"climatological means\" if show_climatological_means else \"percentile thresholds\"\n",
    "    print(f\"Began animation for the {type_message} of the {region_id_arg} datasets!\")\n",
    "    \n",
    "    chosen_doys = len(available_doys)\n",
    "    \n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, animate,\n",
    "        frames=chosen_doys,\n",
    "        interval=200,\n",
    "        blit=False,\n",
    "        repeat=True\n",
    "    )\n",
    "    \n",
    "    writer = animation.PillowWriter(fps=2)\n",
    "    \n",
    "    if custom_output_filename == None:\n",
    "        output_filename = f\"{region_id_arg}_{data_type}_{baseline_name_arg}_{chosen_doys}_doys_total.gif\"\n",
    "    else:\n",
    "        output_filename = custom_output_filename\n",
    "    print(f\"Saving animation at: {output_filename}\") \n",
    "    \n",
    "    \n",
    "    ## Save the resulting animation\n",
    "    def print_frame_progress(current_frame, total_frames): # This shows the saving progress (in frames)!\n",
    "        print(f\"\\r → Doy (Frame) Processed: {current_frame + 1}/{total_frames}\", end='', flush=True)\n",
    "\n",
    "    anim.save(output_filename, writer=writer, dpi=100,\n",
    "              progress_callback=print_frame_progress)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.close(fig)\n",
    "    print(f\"\\nAnimation finished and saved!\\n\")\n",
    "    \n",
    "    return anim\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Note: loading a percentile produces an animation for percentile thresholds, and setting it to None an animation for means\n",
    "check_processed_datasets_with_an_animation(baseline_name_arg=baseline_folder_name, region_id_arg=region_id_folder_name, custom_id_arg=final_custom_id,\n",
    "                                           custom_output_filename=None, chosen_percentile=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ddb4fb-de04-4b3b-9e41-564073a63aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to verify against the outputs of marineHeatWaves to come in the future..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangeo23",
   "language": "python",
   "name": "pangeo23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
