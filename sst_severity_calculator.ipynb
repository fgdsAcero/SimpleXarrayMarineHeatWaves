{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd63adab-d7ea-4d07-9288-1e572bd42d79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/sw/anaconda3/2023.09/envs/pangeo23/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ERROR 1: PROJ: proj_create_from_database: Open of /opt/sw/anaconda3/2023.09/envs/pangeo23/share/proj failed\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import psutil\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "import copernicusmarine\n",
    "from copernicusmarine import get\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "\n",
    "import marineHeatWaves as mhwpy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.animation as animation\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ea5ef-0dd5-4416-9720-27f6da36d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Important script-wide constants\n",
    "start_val_arg = 1 # your percentile and mean datasets' starting day of the year (doy) value (both must have this)\n",
    "end_val_arg = 366 # your percentile and mean datasets' ending day of the year (doy) value (both must have this)\n",
    "baseline_name_arg = \"Baseline9322\" # your chosen baseline identifier\n",
    "current_percentile = 90 # your chosen percentile (as a percent, not under 1.0)\n",
    "custom_id_choice = \"fgd\" # your custom identifier used in both mean and percentile datasets \n",
    "my_root_directory = \"\" # Should be your root directory, from which you access data from and save data to\n",
    "# See sst_climatology_and_percentile_calculator.ipynb for your used values (or consult your stored datasets)\n",
    "\n",
    "# Data directories that can be further modified by you may be identified by searching for \n",
    "# \"NOTE: POTENTIAL DIRECTORY TWEAKING HERE\" in this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd8de336-9ead-409f-8cd6-61ba6da73438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed data:\n",
      " <xarray.DataArray 'sst' (time: 16153, lat: 420, lon: 1440)> Size: 39GB\n",
      "dask.array<rechunk-merge, shape=(16153, 420, 1440), dtype=float32, chunksize=(32, 210, 160), chunktype=numpy.ndarray>\n",
      "Coordinates:\n",
      "  * time            (time) datetime64[ns] 129kB 1981-09-01 ... 2025-11-21\n",
      "  * lat             (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon             (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "    normalized_doy  (time) int64 129kB 245 246 247 248 249 ... 323 324 325 326\n",
      "Attributes:\n",
      "    long_name:     Daily Sea Surface Temperature\n",
      "    units:         degC\n",
      "    valid_range:   [-3. 45.]\n",
      "    precision:     2.0\n",
      "    dataset:       NOAA High-resolution Blended Analysis\n",
      "    var_desc:      Sea Surface Temperature\n",
      "    level_desc:    Surface\n",
      "    statistic:     Mean\n",
      "    parent_stat:   Individual Observations\n",
      "    actual_range:  [-1.8  34.82] \n",
      "\n",
      "Clim. Means:\n",
      " <xarray.DataArray 'sst' (normalized_doy: 366, lat: 420, lon: 1440)> Size: 885MB\n",
      "dask.array<open_dataset-sst, shape=(366, 420, 1440), dtype=float32, chunksize=(61, 210, 160), chunktype=numpy.ndarray>\n",
      "Coordinates:\n",
      "  * lat             (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon             (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * normalized_doy  (normalized_doy) int64 3kB 1 2 3 4 5 ... 362 363 364 365 366 \n",
      "\n",
      "Percentile Thresholds:\n",
      " <xarray.DataArray 'sst' (normalized_doy: 366, lat: 420, lon: 1440)> Size: 2GB\n",
      "dask.array<open_dataset-sst, shape=(366, 420, 1440), dtype=float64, chunksize=(61, 210, 160), chunktype=numpy.ndarray>\n",
      "Coordinates:\n",
      "  * lat             (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon             (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * normalized_doy  (normalized_doy) int64 3kB 1 2 3 4 5 ... 362 363 364 365 366\n",
      "    quantile        float64 8B ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Loading SST data ---------------------------------------------------------------------------------\n",
    "def normalize_dayofyear(time_coord):\n",
    "    doy = time_coord.dt.dayofyear\n",
    "    is_leap = time_coord.dt.is_leap_year\n",
    "\n",
    "    # This code ensures March 1 is always day 61, regardless of leap year\n",
    "    normalized_doy = xr.where(\n",
    "        (~is_leap) & (doy >= 60),  # If it is a non-leap year, doy 60 is March 1. If we have March 1 or later,\n",
    "        doy + 1,                          # then we push forward March 1 and/or the later days by 1 day.\n",
    "        doy                               # Otherwise, we keep original for leap years and Jan-Feb 28.\n",
    "    )\n",
    "    return normalized_doy\n",
    "\n",
    "# Your filepath here; this is my setup.\n",
    "raw_data_directory = f'{my_root_directory}/OISST/Data/sst.day.mean.*.nc'\n",
    "\n",
    "# Sometimes this will result in a crash the first time. Wait a bit and run the cell again...\n",
    "ds = xr.open_mfdataset(\n",
    "    raw_data_directory,         # Glob pattern\n",
    "    parallel=True,              # Enable parallel file opening \n",
    "    chunks='auto',              # Let Dask choose optimal chunking\n",
    "    combine='by_coords',        # Merge based on coordinate values\n",
    "    engine='netcdf4'            # Specify the engine (it may crash otherwise; if it still does, restart the kernel)\n",
    ")\n",
    "\n",
    "optimal_chunking = {'lat': 210, 'lon': 160} # for the raw data\n",
    "optimal_chunks = {'normalized_doy':-1, 'lat': 210, 'lon': 160} # for the to-be-created severity data\n",
    "full_ds = ds.sel(lat=slice(-15, 90), lon=slice(0, 360)).sst\n",
    "full_ds = full_ds.chunk(optimal_chunking)\n",
    "Full_obs = full_ds.assign_coords(\n",
    "        normalized_doy=('time', normalize_dayofyear(full_ds.time).data))\n",
    "print(\"Observed data:\\n\", Full_obs, '\\n')\n",
    "\n",
    "## Loading constants\n",
    "folder_name_arg = \"Full\"\n",
    "\n",
    "custom_name_arg = \"fgd\"\n",
    "if custom_name_arg != \"\":\n",
    "    id = f\"{custom_name_arg}_\"\n",
    "else:\n",
    "    id =\"\"\n",
    "    \n",
    "## Loading climatological means data \n",
    "file_name = f\"{folder_name_arg}_{id}sst_clim_subset_{start_val_arg}_to_{end_val_arg}_{baseline_name_arg}.zarr\"\n",
    "filepath = f'{my_root_directory}/OISST/Clim/{baseline_name_arg}/{file_name}' # NOTE: POTENTIAL DIRECTORY TWEAKING HERE\n",
    "Full_means = xr.open_zarr(filepath).sst\n",
    "print(\"Clim. Means:\\n\", Full_means, '\\n')\n",
    "\n",
    "## Loading percentile threshold data\n",
    "file_name = f\"{folder_name_arg}_{id}sst_thresh_subset_{start_val_arg}_to_{end_val_arg}_{baseline_name_arg}.zarr\"\n",
    "filepath = f'{my_root_directory}/OISST/Thresh{current_percentile}th/{baseline_name_arg}/{file_name}' # NOTE: POTENTIAL DIRECTORY TWEAKING HERE\n",
    "Full_thresh = xr.open_zarr(filepath).sst\n",
    "print(\"Percentile Thresholds:\\n\", Full_thresh, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cef78e4-7658-4c63-88f3-8e6cd7f9b7b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------o-------------------------------o------------------------------o----------------------------\n",
      "\n",
      "Baseline used: Baseline9322\n",
      "\n",
      "The smoothWidth argument was set to: 31.\n",
      "This means a rolling window of 31 doys centered on each individual doy is used.\n",
      "For each doy, the rolling window contains the previous 15.0 doys, the center doy, and the next 15.0 doys.\n",
      "\n",
      "Starting Save of Baseline9322 1982 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_1982_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "1982 Severity:\n",
      "  | Memory usage: 9.36 GB | Memory: 33.8% used | \n",
      "<xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 1982-01-01 1982-01-02 ... 1982-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 84.61 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 1983 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_1983_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "1983 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 1983-01-01 1983-01-02 ... 1983-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 73.56 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 1984 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_1984_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "1984 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 366)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 1984-01-01 1984-01-02 ... 1984-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 81.21 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 1985 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_1985_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "1985 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 1985-01-01 1985-01-02 ... 1985-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[####################################### ] | 98% Completed | 54.17 sms | Memory usage: 9.15 GB | Memory: 33.7% used | \n",
      "[########################################] | 100% Completed | 77.33 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 1986 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_1986_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "1986 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 1986-01-01 1986-01-02 ... 1986-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 77.41 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 1987 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_1987_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "1987 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 1987-01-01 1987-01-02 ... 1987-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 79.36 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 1988 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_1988_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "1988 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 366)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 1988-01-01 1988-01-02 ... 1988-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 75.75 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 1989 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_1989_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "1989 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 1989-01-01 1989-01-02 ... 1989-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[####################################### ] | 98% Completed | 38.28 sms | Memory usage: 8.84 GB | Memory: 33.5% used | \n",
      "[########################################] | 100% Completed | 83.19 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 1990 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_1990_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "1990 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 1990-01-01 1990-01-02 ... 1990-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 87.73 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 1991 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_1991_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "1991 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 1991-01-01 1991-01-02 ... 1991-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 83.32 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 1992 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_1992_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "1992 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 366)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 1992-01-01 1992-01-02 ... 1992-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[####################################### ] | 99% Completed | 79.58 sms | Memory usage: 3.54 GB | Memory: 31.4% used | \n",
      "[########################################] | 100% Completed | 80.31 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 1993 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_1993_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "1993 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 1993-01-01 1993-01-02 ... 1993-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 78.23 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 1994 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_1994_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "1994 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 1994-01-01 1994-01-02 ... 1994-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 84.88 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 1995 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_1995_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "1995 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 1995-01-01 1995-01-02 ... 1995-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 82.93 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 1996 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_1996_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "1996 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 366)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 1996-01-01 1996-01-02 ... 1996-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[####################################### ] | 98% Completed | 47.22 ss | Memory usage: 8.78 GB | Memory: 33.5% used | \n",
      "[########################################] | 100% Completed | 76.67 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 1997 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_1997_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "1997 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 1997-01-01 1997-01-02 ... 1997-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 82.78 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 1998 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_1998_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "1998 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 1998-01-01 1998-01-02 ... 1998-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 89.71 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 1999 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_1999_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "1999 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 1999-01-01 1999-01-02 ... 1999-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 88.44 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2000 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2000_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2000 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 366)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2000-01-01 2000-01-02 ... 2000-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[#################                       ] | 43% Completed | 3.63 sms | Memory usage: 5.61 GB | Memory: 32.3% used | \n",
      "[########################################] | 100% Completed | 83.39 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2001 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2001_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2001 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2001-01-01 2001-01-02 ... 2001-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 87.30 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2002 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2002_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2002 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2002-01-01 2002-01-02 ... 2002-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 88.36 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2003 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2003_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2003 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2003-01-01 2003-01-02 ... 2003-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[####################################### ] | 98% Completed | 40.05 ss | Memory usage: 9.18 GB | Memory: 33.7% used | \n",
      "[########################################] | 100% Completed | 83.04 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2004 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2004_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2004 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 366)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2004-01-01 2004-01-02 ... 2004-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 85.02 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2005 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2005_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2005 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2005-01-01 2005-01-02 ... 2005-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 86.04 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2006 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2006_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2006 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2006-01-01 2006-01-02 ... 2006-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[####################################### ] | 99% Completed | 81.63 sms | Memory usage: 6.36 GB | Memory: 32.5% used | \n",
      "[########################################] | 100% Completed | 92.73 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2007 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2007_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2007 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2007-01-01 2007-01-02 ... 2007-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 78.63 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2008 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2008_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2008 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 366)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2008-01-01 2008-01-02 ... 2008-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 86.95 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2009 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2009_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2009 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2009-01-01 2009-01-02 ... 2009-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 85.72 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2010 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2010_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2010 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2010-01-01 2010-01-02 ... 2010-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[####################################### ] | 98% Completed | 31.77 sms | Memory usage: 9.26 GB | Memory: 33.7% used | \n",
      "[########################################] | 100% Completed | 88.36 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2011 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2011_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2011 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2011-01-01 2011-01-02 ... 2011-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 83.39 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2012 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2012_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2012 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 366)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2012-01-01 2012-01-02 ... 2012-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 88.23 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2013 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2013_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2013 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2013-01-01 2013-01-02 ... 2013-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[####################################### ] | 98% Completed | 67.40 sms | Memory usage: 9.62 GB | Memory: 33.8% used | \n",
      "[########################################] | 100% Completed | 90.01 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2014 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2014_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2014 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2014-01-01 2014-01-02 ... 2014-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 92.03 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2015 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2015_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2015 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2015-01-01 2015-01-02 ... 2015-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 87.22 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2016 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2016_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2016 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 366)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2016-01-01 2016-01-02 ... 2016-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 92.93 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2017 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2017_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2017 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2017-01-01 2017-01-02 ... 2017-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      " | Memory usage: 3.35 GB | Memory: 31.3% used | \n",
      "[########################################] | 100% Completed | 85.89 ss\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2018 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2018_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2018 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2018-01-01 2018-01-02 ... 2018-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 81.88 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2019 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2019_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2019 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2019-01-01 2019-01-02 ... 2019-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 87.19 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2020 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2020_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2020 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 366)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2020-01-01 2020-01-02 ... 2020-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[####################################### ] | 98% Completed | 40.27 ss | Memory usage: 9.07 GB | Memory: 33.6% used | \n",
      "[########################################] | 100% Completed | 87.85 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2021 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2021_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2021 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2021-01-01 2021-01-02 ... 2021-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 89.61 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2022 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2022_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2022 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2022-01-01 2022-01-02 ... 2022-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 87.02 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2023 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2023_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2023 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 365)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2023-01-01 2023-01-02 ... 2023-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(73, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[####################################### ] | 98% Completed | 71.57 ss | Memory usage: 8.21 GB | Memory: 33.2% used | \n",
      "[########################################] | 100% Completed | 91.61 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting Save of Baseline9322 2024 Severity Dataset...\n",
      "\n",
      "File Path:  /d7/fgd2105/OISST/Severity_90th/Baseline9322/Full_severity_fgd_SST_PC90th_2024_smoothWidth31_Baseline9322.zarr \n",
      "\n",
      "2024 Severity:\n",
      " <xarray.Dataset> Size: 4GB\n",
      "Dimensions:       (lat: 420, lon: 1440, time: 366)\n",
      "Coordinates:\n",
      "  * lat           (lat) float32 2kB -14.88 -14.62 -14.38 ... 89.38 89.62 89.88\n",
      "  * lon           (lon) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time          (time) datetime64[ns] 3kB 2024-01-01 2024-01-02 ... 2024-12-31\n",
      "Data variables:\n",
      "    severity      (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray>\n",
      "    severity_mhw  (time, lat, lon) float64 2GB dask.array<chunksize=(61, 210, 160), meta=np.ndarray> \n",
      "\n",
      "[########################################] | 100% Completed | 86.64 s\n",
      "Done saving! Moving along! \n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Finished saving data for all years!\n",
      "--------------------o-------------------------------o------------------------------o---------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Function to calculate column-averaged severity\n",
    "def get_max_and_min_years(ds):\n",
    "    min_yr = ds.time[0].dt.year.item()\n",
    "    max_yr = ds.time[-1].dt.year.item()\n",
    "    return min_yr, max_yr\n",
    "\n",
    "\n",
    "def check_valid_type(data, data_type, data_name, correction_message):\n",
    "    if not isinstance(data, data_type):\n",
    "        raise ValueError(f\"Invalid {data_name} provided. Please provide a proper {data_type.__name__} {data_name}, or {correction_message}.\")\n",
    "\n",
    "\n",
    "def event_gap_filling(events_series: np.ndarray, minDuration: int = 5, maxGap: int = 2) -> np.ndarray:\n",
    "    '''\n",
    "    Gap-filling function for marine heatwave (MHW) events following the Hobday et al. (2016) definition.\n",
    "    \n",
    "    Key rules:\n",
    "    1. Only events of 5+ (or minDuration+) days duration are considered valid MHWs\n",
    "    2. Gaps of 2 (or maxGap) days or less between any valid events should merge them\n",
    "    3. When merging occurs, the entire merged period becomes one event\n",
    "    4. The resulting event must still meet the 5-day minimum duration\n",
    "    \n",
    "    Examples from Hobday et al. (2016):\n",
    "    - [5hot, 2cool, 6hot]  13-day event (5 + 2 + 6 = 13)\n",
    "    - [5hot, 1cool, 2hot]  5-day event (only first 5 days qualify)\n",
    "    - [2hot, 1cool, 5hot]  5-day event (only last 5 days qualify)\n",
    "    - [5hot, 4cool, 6hot]  two separate events (5-day + 6-day)\n",
    "    '''\n",
    "    \n",
    "    # Input validation and conversion\n",
    "    assert events_series.ndim == 1, f\"Expected 1D series, got {events_series.ndim}D\"\n",
    "\n",
    "    # We convert the dataset to boolean if it is not boolean\n",
    "    if events_series.dtype != bool:\n",
    "        print(\"Dataset conversion occurred!\", '\\n')\n",
    "        events_series = events_series.astype(bool)\n",
    "\n",
    "    # If the input series is full of False values, we return an all 0's array\n",
    "    if not np.any(events_series):\n",
    "        return np.zeros_like(events_series, dtype=int)\n",
    "    \n",
    "    ## First, we find all consecutive True sequences that are  minDuration\n",
    "    # We initialize our empty list that stores all the valid sequences  the minDuration value\n",
    "    valid_sequences = []\n",
    "    i = 0\n",
    "\n",
    "    # Main loop that examines every position in the series\n",
    "    while i < len(events_series):\n",
    "        # If we identify a True value, we check for a potential marine heatwave event\n",
    "        if events_series[i]:\n",
    "            # We save the starting point\n",
    "            start_idx = i\n",
    "            \n",
    "            # We advance over a series of consecutive True values (until reaching the end of the array or a False value)\n",
    "            while i < len(events_series) and events_series[i]:\n",
    "                i += 1\n",
    "\n",
    "            # We save the end value and duration of our series accordingly\n",
    "            end_idx = i - 1 \n",
    "            duration = end_idx - start_idx + 1 # we add 1 since indeces are 0-based\n",
    "\n",
    "            # If the minimumDuration is met, we store the start and end indeces as a tuple in our list\n",
    "            if duration >= minDuration:\n",
    "                valid_sequences.append((start_idx, end_idx))\n",
    "                \n",
    "        else: # If the current position if a False day, we move along\n",
    "            i += 1\n",
    "\n",
    "    # If no valid sequences exist, we return a 0-value array\n",
    "    if not valid_sequences:\n",
    "        return np.zeros_like(events_series, dtype=int)\n",
    "    \n",
    "    ## Next, we group valid event sequences by checking gaps between them\n",
    "    # We initialize an empty list to store events that should be merged\n",
    "    event_groups = []\n",
    "    current_group = [valid_sequences[0]] # sets the current group to check\n",
    "\n",
    "    # We loop through all valid sequences (starting from the second one), comparing each with the previous\n",
    "    for i in range(1, len(valid_sequences)):\n",
    "        \n",
    "        prev_end = current_group[-1][1] # we extract the end index ([1]) of the last sequence in the current_group ([-1])\n",
    "        curr_start = valid_sequences[i][0] # we extract the start index ([0]) of the current valid_sequence checked ([i])\n",
    "\n",
    "        # We calculate the number of days between the current event's start index and the previous event's end index\n",
    "        gap_length = curr_start - prev_end - 1\n",
    "\n",
    "        # If the gap is <= the maxGap value, we save/merge (via append) the current valid sequence to/with the current group\n",
    "        if gap_length <= maxGap:\n",
    "            current_group.append(valid_sequences[i])\n",
    "        # If the gap is not <= the maxGap value, we add the untouched current group back to the merged event list and reset the current group\n",
    "        else:\n",
    "            event_groups.append(current_group)\n",
    "            current_group = [valid_sequences[i]]\n",
    "\n",
    "    # We add the last group to the merged event group list (manually)\n",
    "    event_groups.append(current_group)\n",
    "    \n",
    "    ## Finally, we create our output with appropriate event IDs\n",
    "    # We begin with an all 0's array\n",
    "    out = np.zeros_like(events_series, dtype=int)\n",
    "\n",
    "    # We iterate through all the groups in the merged group list\n",
    "    for event_id, group in enumerate(event_groups, 1):\n",
    "        # We find the overall start and end index for each group \n",
    "        group_start = group[0][0]\n",
    "        group_end = group[-1][1]\n",
    "\n",
    "        # We assign a unique event id for all values from group start to the group end indeces (inclusive)\n",
    "        out[group_start:group_end + 1] = event_id\n",
    "\n",
    "    # We return our output\n",
    "    return out\n",
    "        \n",
    "    \n",
    "# Create a continous time coordinate for a passed dataset\n",
    "def create_continuous_time_coordinate(ds):\n",
    "    n_timesteps = len(ds.normalized_doy)\n",
    "    continuous_time = np.arange(1, n_timesteps + 1)\n",
    "    \n",
    "    # Add the continuous coordinate and make it the dominant coordinate\n",
    "    ds = ds.assign_coords(time_continuous=('normalized_doy', continuous_time))\n",
    "    ds = ds.swap_dims({'normalized_doy':'time_continuous'}).chunk({'time_continuous':-1})\n",
    "    return ds\n",
    "\n",
    "\n",
    "def run_mhw_test(initial_mask_ds, final_mask_ds, lat_val=None, lon_val=None):\n",
    "    # Quick check to ensure values were inputted\n",
    "    if any(val is None for val in (lat_val, lon_val)):\n",
    "        raise ValueError(\"Missing a key argument in the run_mhw_test function. Please ensure a valid integer value is provided for each value argument!\")\n",
    "    \n",
    "    print(\"--------------------------------------------------   TEST START   ---------------------------------------------------\\n\")\n",
    "    print(f\"RUNNING TEST FOR NEAREST LATITUDE: {lat_val}, LONGITUDE: {lon_val}.\\n\")\n",
    "    \n",
    "    # Grab a subset from the initial_mask_ds\n",
    "    subset_original = initial_mask_ds.sel(lat=lat_val, lon=lon_val, method='nearest')\n",
    "    real_lat = subset_original.lat.values.item()\n",
    "    real_lon = subset_original.lon.values.item()\n",
    "    print(f\"ACTUAL VALUES DETECTED:\\nLATITUDE: {real_lat}, LONGITUDE: {real_lon}\\n\")\n",
    "    \n",
    "    original_labeled = subset_original.values\n",
    "    print(\"Exceed values:\\n\", original_labeled, '\\n\\n')\n",
    "\n",
    "    events_labeled = final_mask_ds.sel(lat=lat_val, lon=lon_val, method='nearest').values\n",
    "    print(\"Event gap-filled values:\\n\", events_labeled, '\\n\\n')\n",
    "\n",
    "    print(\"----------o-----------0----------o-------------\\n\")\n",
    "\n",
    "    length = min(len(original_labeled), len(events_labeled))\n",
    "    step = 10\n",
    "\n",
    "    for i in range(0, length, step):\n",
    "        end_idx = min(i + step, length)\n",
    "        print(f\"Values {i} to {end_idx - 1}:\")\n",
    "        print(\"Exceed values:\", original_labeled[i:end_idx])\n",
    "        print(\"Event gap-filled values:\", events_labeled[i:end_idx])\n",
    "        print(\"-\" * 60)\n",
    "        print(\" \")\n",
    "    print(\"--------------------------------------------------   END OF TEST   ---------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "stop_monitoring = True # We begin by NOT showing any memory usage\n",
    "def monitor_memory(interval_minutes=5, log_file=None):\n",
    "    interval = interval_minutes * 60  \n",
    "    \n",
    "    while not stop_monitoring:\n",
    "        mem = psutil.Process(os.getpid()).memory_info().rss / (1024**3)  # in GB\n",
    "        print(f\" | Memory usage: {mem:.2f} GB | Memory: {psutil.virtual_memory().percent}% used | \")\n",
    "        \n",
    "        if log_file:\n",
    "            with open(log_file, 'a') as f:\n",
    "                f.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')}: {mem:.2f} GB\\n\")\n",
    "        time.sleep(interval)\n",
    "    \n",
    "    \n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "def calculate_severity(obs_data, thresh_data, clim_data, \n",
    "                       baseline_name, folder_name, custom_id,\n",
    "                       best_chunks,\n",
    "                       smoothWidth=31, percentile=90,\n",
    "                       minDaysMhwDuration=5, maxGapDaysInMhw=2, \n",
    "                       minutes_per_mem_update=5,\n",
    "                       show_debug=True,\n",
    "                       custom_years=False, start_yr=None, end_yr=None,\n",
    "                       mhw_test=False, mhw_test_lat=None, mhw_test_lon=None):\n",
    "\n",
    "    # Quick check for inputs regarding a marine heatwave test a specific coordinate\n",
    "    if mhw_test:\n",
    "        check_valid_type(mhw_test_lat, int, \"latitude value\", \"set mhw_test to False\")\n",
    "        check_valid_type(mhw_test_lon, int, \"longitude value\", \"set mhw_test to False\")\n",
    "    \n",
    "    # Gather the min and max years in the observed data\n",
    "    min_obs_yr, max_obs_yr = get_max_and_min_years(obs_data)\n",
    "\n",
    "    # Setting up custom years, or going with the default (min to max years in the observed data)\n",
    "    if custom_years:\n",
    "        check_valid_type(start_yr, int, \"starting year\", \"set custom_years to False\")\n",
    "        check_valid_type(end_yr, int, \"ending year\", \"set custom_years to False\")\n",
    "        years = range(start_yr, end_yr + 1)\n",
    "    else:\n",
    "        years = range(min_obs_yr, max_obs_yr + 1)\n",
    "\n",
    "    # Set up a variable to control whether a second smoothing (of the climatological means/percentiles) is applied\n",
    "    apply_smoothing = False if smoothWidth == 0 else True\n",
    "    shown_once = False # for debugging purposes\n",
    "\n",
    "    # Set up the id to save the dataset with, if desired\n",
    "    if custom_id != \"\":\n",
    "        id = f\"{custom_id}_\"\n",
    "    else:\n",
    "        id =\"\"\n",
    "    \n",
    "    # Beginning of code for severity calculation:\n",
    "    print(\"--------------------o-------------------------------o------------------------------o----------------------------\")\n",
    "    print(f'\\nBaseline used: {baseline_name}\\n') # We only have one baseline\n",
    "    print(f\"The smoothWidth argument was set to: {smoothWidth}.\")\n",
    "    \n",
    "    if apply_smoothing:\n",
    "        print(f\"This means a rolling window of {smoothWidth} doys centered on each individual doy is used.\")\n",
    "        print(f\"For each doy, the rolling window contains the previous {(smoothWidth-1)/2} doys, the center doy, and the next {(smoothWidth-1)/2} doys.\\n\")\n",
    "    else:\n",
    "        print(\"No smoothing was applied; using the threshold and mean datasets as is!\\n\")\n",
    "\n",
    "    # Showing the original datasets if show_debug is enabled\n",
    "    if show_debug: \n",
    "        print('Original raw, \"observed\" data:\\n', obs_data, '\\n')\n",
    "        print(\"Original percentile threshold data:\\n\", thresh_data, '\\n')\n",
    "        print(\"Original climatological means data:\\n\", clim_data, '\\n')\n",
    "        print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "    if apply_smoothing:\n",
    "        ## PADDING AND SMOOTHING\n",
    "        # Padding with smoothWidth (which is more than enough for the desired smoothing with smoothWidth)\n",
    "        padded_clim = clim_data.pad(normalized_doy=smoothWidth, mode='wrap')\n",
    "        if show_debug: print(\"Padded Means:\\n\", padded_clim, '\\n')\n",
    "    \n",
    "        padded_thresh = thresh_data.pad(normalized_doy=smoothWidth, mode='wrap')\n",
    "        if show_debug: print(\"Padded Threshold:\\n\", padded_thresh, '\\n\\n')\n",
    "    \n",
    "        # Smoothing\n",
    "        clim_smoothed = padded_clim.rolling(normalized_doy=smoothWidth, center=True, min_periods=smoothWidth).mean() \n",
    "        if show_debug: print(f\"Padded Means Smoothed by {smoothWidth} doys:\\n\", clim_smoothed, '\\n')\n",
    "    \n",
    "        clim_processed = clim_smoothed.isel(normalized_doy=slice(smoothWidth, -smoothWidth))\n",
    "        if show_debug: print(\"Smoothed Means (No Pad):\\n\", clim_processed, '\\n\\n')\n",
    "    \n",
    "        thresh_smoothed = padded_thresh.rolling(normalized_doy=smoothWidth, center=True, min_periods=smoothWidth).mean()\n",
    "        if show_debug: print(f\"Padded Threshold Smoothed by {smoothWidth} doys: \", thresh_smoothed, '\\n')\n",
    "    \n",
    "        thresh_processed = thresh_smoothed.isel(normalized_doy=slice(smoothWidth, -smoothWidth))\n",
    "        if show_debug: print(\"Threshold Smoothed (No Pad): \", thresh_processed, '\\n')\n",
    "\n",
    "        \n",
    "        # We also save early and late smoothed doy data for late/early-year mhw detection later\n",
    "        # MAY NEED TO SET THESE ELSEWHERE IF USING A LARGER smoothWidth (if the smoothWidth extends past 31 to include the interpolated Feb 29)\n",
    "        doys_to_gather = smoothWidth # can be adjusted to your needs\n",
    "        thresh_prev_yr_days = thresh_smoothed.isel(normalized_doy=slice(0, doys_to_gather)).copy()\n",
    "        thresh_next_yr_days = thresh_smoothed.isel(normalized_doy=slice(-doys_to_gather, None)).copy()\n",
    "        \n",
    "    elif not apply_smoothing:\n",
    "        # We still save early and late raw threshold doy data for late/early-year mhw detection later\n",
    "        doys_to_gather = 15 # additional days to append to the dataset for year-round mhw detection (should be greater than 6)\n",
    "        thresh_prev_yr_days = thresh_data.isel(normalized_doy=slice(0, doys_to_gather)).copy()\n",
    "        thresh_next_yr_days = thresh_data.isel(normalized_doy=slice(-doys_to_gather, None)).copy()\n",
    "        \n",
    "        # We also run the prior raw datasets through these new variables\n",
    "        thresh_processed = thresh_data\n",
    "        clim_processed = clim_data\n",
    "\n",
    "    if show_debug:\n",
    "        print(f'\\n\"Next Year\" Threshold Dataset Doys:\\n{thresh_next_yr_days}\\n')\n",
    "        print(f'\"Previous Year\" Threshold Dataset Doys:\\n{thresh_prev_yr_days}\\n')\n",
    "\n",
    "    \n",
    "    # Quick monitoring initiation\n",
    "    global stop_monitoring\n",
    "    if show_debug:\n",
    "        stop_monitoring = True # We don't want to start showing memory use.\n",
    "        print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "    else:\n",
    "        # We start monitoring here so that it only runs once\n",
    "        stop_monitoring = False # We do want to start showing memory use.\n",
    "        monitor_thread = threading.Thread(target=monitor_memory, kwargs={'interval_minutes': minutes_per_mem_update})\n",
    "        monitor_thread.daemon = True\n",
    "        monitor_thread.start()\n",
    "\n",
    "    ## CALCULATING ANOMALIES BY YEAR (observed - climatological means) AND DETECTING MHWS\n",
    "    for year in years:\n",
    "        # Time subsetting\n",
    "        obs_year_data = obs_data.sel(time=f'{year}')\n",
    "        obs_year_data_norm = obs_year_data.swap_dims({'time':'normalized_doy'})\n",
    "        if show_debug and not shown_once: print(f\"Observed data for {year}:\\n\", obs_year_data_norm, '\\n')\n",
    "\n",
    "        # Aligning the datasets (returns doys common to both)\n",
    "        obs_aligned, clim_aligned = xr.align(obs_year_data_norm, clim_processed, join=\"inner\")\n",
    "        clim_aligned = clim_aligned.chunk(best_chunks)\n",
    "\n",
    "        obs_aligned, thresh_aligned = xr.align(obs_year_data_norm, thresh_processed, join=\"inner\")\n",
    "        thresh_aligned = thresh_aligned.chunk(best_chunks)\n",
    "\n",
    "        if show_debug and not shown_once: \n",
    "            print(f\"Thresh aligned: \", thresh_aligned, '\\n')\n",
    "            print(f\"Clim aligned: \", clim_aligned, '\\n')\n",
    "\n",
    "        # We also save the time variable separately for later, then drop it from the observed\n",
    "        obs_aligned_time = obs_aligned.swap_dims({'normalized_doy':'time'}).drop_vars('normalized_doy').time\n",
    "        if show_debug and not shown_once: print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "        ### DETECTING MHWS ACROSS THE YEARS\n",
    "        ## Grabbing previous and next year data\n",
    "        obs_aligned = obs_aligned.swap_dims({'normalized_doy':'time'})\n",
    "        if show_debug and not shown_once: print(f\"Full Year Data from the Current Year ({year}):\\n\", obs_aligned, '\\n')\n",
    "\n",
    "        # Important objects for padding\n",
    "        prev_year_data = None\n",
    "        next_year_data = None\n",
    "\n",
    "        # We grab the previous year's data if it is not the first or final year in the dataset\n",
    "        if year > min_obs_yr:\n",
    "            prev_year = year - 1\n",
    "            prev_year_full = obs_data.sel(time=f'{prev_year}')\n",
    "\n",
    "            # If it exists, we grab the previous year's final period of length smoothWidth (up to 366)\n",
    "            if len(prev_year_full) > 0:\n",
    "                prev_year_data = prev_year_full.isel(time=slice(-doys_to_gather, None))\n",
    "                if show_debug and not shown_once: print(f\"End of the Year Data from the Previous Year ({prev_year}):\\n\", prev_year_data, '\\n')\n",
    "\n",
    "        # We grab the next year's data if it is not the first or final year in the dataset\n",
    "        if year < max_obs_yr:\n",
    "            next_year = year + 1\n",
    "            next_year_full = obs_data.sel(time=f'{next_year}')\n",
    "\n",
    "            if len(next_year_full) > 0:\n",
    "                next_year_data = next_year_full.isel(time=slice(0, doys_to_gather))\n",
    "                if show_debug and not shown_once: print(f\"Beginning of the Year Data from the Next Year ({next_year}):\\n\", next_year_data, '\\n')\n",
    "\n",
    "        ## Merging the previous and next year datasets, if they are available\n",
    "        data_pieces = [] \n",
    "\n",
    "        # We first append the previous year data to our array, if it exists\n",
    "        if prev_year_data is not None: data_pieces.append(prev_year_data)\n",
    "\n",
    "        # Next, we append the current year data to our array, if it exists\n",
    "        data_pieces.append(obs_aligned)\n",
    "\n",
    "        # Lastly, we append the next year data to our array, if it exists\n",
    "        if next_year_data is not None: data_pieces.append(next_year_data)\n",
    "\n",
    "        if show_debug and not shown_once: print(\"Data pieces (prev + full current + next):\\n\", data_pieces, '\\n\\n')\n",
    "\n",
    "        # Merging the datasets\n",
    "        obs_year_data_extended = xr.concat(data_pieces, dim='time')\n",
    "        obs_year_data_extended = obs_year_data_extended.sortby('time')\n",
    "        if show_debug and not shown_once: print(\"Initial merged observed data:\\n\", obs_year_data_extended, '\\n')\n",
    "\n",
    "        # We switch back to our desired normalized doy time dimension\n",
    "        obs_year_data_extended = obs_year_data_extended.swap_dims({'time':'normalized_doy'}).drop_vars('time')\n",
    "        obs_year_data_extended = obs_year_data_extended.chunk(best_chunks)\n",
    "        if show_debug and not shown_once: \n",
    "            print(\"Final merged observed data: \", obs_year_data_extended, '\\n')\n",
    "            print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "        ## Threshold padding\n",
    "        data_pieces_thresh = []\n",
    "\n",
    "        # We append previous year data if it exists first\n",
    "        if prev_year_data is not None:\n",
    "            data_pieces_thresh.append(thresh_prev_yr_days)\n",
    "            if show_debug and not shown_once: print('\"Previous\" Year Threshold Data:\\n', thresh_prev_yr_days, '\\n')  # doys with doys_to_gather length up to 366\n",
    "\n",
    "        # We then append current year data\n",
    "        data_pieces_thresh.append(thresh_aligned)\n",
    "\n",
    "        # We lastly append next year data if it exists\n",
    "        if next_year_data is not None:\n",
    "            data_pieces_thresh.append(thresh_next_yr_days)\n",
    "            if show_debug and not shown_once: print('\"Next\" Year Threshold Data:\\n', thresh_next_yr_days, '\\n') # doy 1 up to the end of doys_to_gather doys\n",
    "\n",
    "        if show_debug and not shown_once: print(\"Data pieces thresh (prev + full current + next):\\n\", data_pieces_thresh, '\\n\\n')\n",
    "\n",
    "        # Since we cannot sort by time, order matters most here!\n",
    "        thresh_data_extended = xr.concat(data_pieces_thresh, dim='normalized_doy')\n",
    "        thresh_data_extended = thresh_data_extended.chunk(best_chunks)\n",
    "        if show_debug and not shown_once: \n",
    "            print(\"Final Merged Threshold Data:\\n\", thresh_data_extended, '\\n')\n",
    "            print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "        ## Creating continuous coordinates (but only if the obs and thresh datasets have matching days of the year)\n",
    "        if obs_year_data_extended.normalized_doy.equals(thresh_data_extended.normalized_doy):\n",
    "            if show_debug and not shown_once: \n",
    "                print(\"Aligned observed and padded thresh datasets' normalized_doys match!\\n\")\n",
    "                print(\"Creating a new, continous coordinate for each for marine heatwave detection...\\n\")\n",
    "\n",
    "            obs_time_aligned = create_continuous_time_coordinate(obs_year_data_extended)\n",
    "            thresh_time_aligned = create_continuous_time_coordinate(thresh_data_extended)\n",
    "\n",
    "            if show_debug and not shown_once:\n",
    "                print(\"Continuous Observed: \", '\\n', obs_time_aligned, '\\n\\n',\n",
    "                      \"Continuous Thresh: \", '\\n', thresh_time_aligned, '\\n')\n",
    "        else:\n",
    "            print(\"ERROR DETECTED! PRINTING RELEVANT OUTPUT:\\n\")\n",
    "            print(obs_year_data_extended.normalized_doy, '\\n\\n', thresh_data_extended.normalized_doy, '\\n')\n",
    "            print(obs_year_data_extended.normalized_doy.values, '\\n',\n",
    "                 thresh_data_extended.normalized_doy.values)\n",
    "\n",
    "            raise ValueError(\"Unexpected Error Detected: Coordinate arrays of observed and threshold datasets do not match exactly; please debug!\")\n",
    "\n",
    "        if show_debug and not shown_once: print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "        ## Exceedence (marine heatwave) mask labeling for a series of connected events\n",
    "\n",
    "        # To be able to use the scipy.ndimage.label function properly to detect events that last over year-end boundaries, \n",
    "        # it is crucial to create a continuous coordinate first (done in previous section) and run it through the custom function.\n",
    "\n",
    "        # Exceedence bool mask creation (for marine heatwaves)\n",
    "        exceed = (obs_time_aligned > thresh_time_aligned) # initial check if the observed temps are greater than their 90th percentiles\n",
    "        exceed = exceed.fillna(False) # Replace NaNs with False   \n",
    "\n",
    "        # Applying mhw event series labeling over the lat-lon grid\n",
    "        events_gaps_filled = xr.apply_ufunc(\n",
    "            event_gap_filling,\n",
    "            exceed,\n",
    "            input_core_dims=[['time_continuous']],\n",
    "            output_core_dims=[['time_continuous']],\n",
    "            vectorize=True,\n",
    "            dask='parallelized',\n",
    "            output_dtypes=[int],\n",
    "            kwargs={'maxGap': maxGapDaysInMhw, 'minDuration':minDaysMhwDuration}, \n",
    "            dask_gufunc_kwargs={\"output_sizes\": {\"time_continuous\": exceed.sizes[\"time_continuous\"]}},\n",
    "        )\n",
    "\n",
    "        events_gaps_filled = events_gaps_filled.transpose('time_continuous','lat','lon')\n",
    "        events_gaps_filled = events_gaps_filled.swap_dims({'time_continuous':'normalized_doy'})\n",
    "\n",
    "        if show_debug and not shown_once: print(\"Event-labelled, gap-filled series: \", events_gaps_filled, '\\n')\n",
    "\n",
    "        # We run the MHW test with padded data, to ensure we correctly identify MHWs within the full current year period\n",
    "        if mhw_test:\n",
    "            run_mhw_test(exceed, events_gaps_filled, lat_val=mhw_test_lat, lon_val=mhw_test_lon)\n",
    "\n",
    "        # Now, we remove the padding\n",
    "        prev_yr_slice = doys_to_gather if (prev_year_data is not None) else 0\n",
    "        next_yr_slice = -doys_to_gather if (next_year_data is not None) else None\n",
    "\n",
    "        events_gaps_filled_unpadded = events_gaps_filled.isel(normalized_doy=slice(prev_yr_slice, next_yr_slice))\n",
    "        if show_debug and not shown_once: print(\"Marine Heatwave Events Bool Dataset, Unpadded: \", events_gaps_filled_unpadded, '\\n')\n",
    "\n",
    "        mhw_bool_final = events_gaps_filled_unpadded.drop_vars('time_continuous').chunk({'normalized_doy':-1})\n",
    "        if show_debug and not shown_once: \n",
    "            print(\"Final Marine Heatwave Events Bool Dataset: \", mhw_bool_final, '\\n')\n",
    "            print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "        # Rechunk the observed dataset\n",
    "        obs_aligned = obs_aligned.swap_dims({'time':'normalized_doy'}).drop_vars('time').chunk(best_chunks)\n",
    "        if show_debug and not shown_once: \n",
    "            print(\"Aligned observed data:\\n\", obs_aligned, '\\n')\n",
    "            print(\"Aligned thresh data:\\n\", thresh_aligned, '\\n')\n",
    "            print(\"Aligned clim data:\\n\", clim_aligned, '\\n\\n')\n",
    "\n",
    "        ## SEVERITY DENOM (PC90 - CLIM)\n",
    "        sev_denom = thresh_aligned - clim_aligned\n",
    "        mhw_sev_denom = xr.where(mhw_bool_final, thresh_aligned - clim_aligned, np.nan).chunk({'normalized_doy': -1})\n",
    "        \n",
    "        ## SEVERITY NUM (OBS - CLIM)  \n",
    "        sev_num = obs_aligned - clim_aligned\n",
    "        mhw_sev_num = xr.where(mhw_bool_final, obs_aligned - clim_aligned, np.nan).chunk({'normalized_doy': -1})\n",
    "\n",
    "        ## SEVERITY (NUM/DENOM)\n",
    "        severity = sev_num / sev_denom\n",
    "        severity = severity.chunk(best_chunks)\n",
    "        \n",
    "        mhw_severity = mhw_sev_num / mhw_sev_denom\n",
    "        mhw_severity = mhw_severity.chunk(best_chunks)\n",
    "        \n",
    "        if show_debug: \n",
    "            print(\"Severity [(OBS - CLIM) / (PC90 - CLIM)]:\\n\", severity, '\\n')\n",
    "            print(\"Severity (only for marine heatwaves):\\n\", mhw_severity, '\\n')\n",
    "            print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "        # Reassigning time\n",
    "        if len(obs_aligned_time) == severity.sizes['normalized_doy']:\n",
    "            # We rename the mhw labeled events dataset for merging\n",
    "            mhw_sev_final = mhw_severity.rename(f'severity_mhw')\n",
    "            sev_final = severity.rename(f'severity')\n",
    "\n",
    "            # Merge the datasets\n",
    "            full_sev = xr.merge([sev_final, mhw_sev_final])\n",
    "\n",
    "            # If there is perfect alignment with our severity dataset, we assign the original time coordinate back to it!\n",
    "            severity_final = full_sev.assign_coords(time=('normalized_doy', obs_aligned_time.values))\n",
    "            severity_final = severity_final.swap_dims({'normalized_doy': 'time'}).drop_vars('normalized_doy').chunk({'time':-1})\n",
    "        else:\n",
    "            # If there's a dimension mismatch, we raise an error!\n",
    "            raise ValueError(f\"Time/normalized_doy dimension mismatch detected for computation of severity for the {year} year!\")\n",
    "        \n",
    "        if show_debug and not shown_once:\n",
    "            shown_once = True\n",
    "            \n",
    "            \n",
    "        ## SAVING\n",
    "        print(f\"Starting Save of {baseline_name} {year} Severity Dataset...\\n\")\n",
    "\n",
    "        # Setting up the destination filepath\n",
    "        filename = f\"{folder_name}_severity_{id}SST_PC{percentile}th_{year}_smoothWidth{smoothWidth}_{baseline_name}.zarr\"\n",
    "        sev_filepath = f'{my_root_directory}/OISST/Severity_{percentile}th/{baseline_name}/{filename}' # NOTE: POTENTIAL DIRECTORY TWEAKING HERE\n",
    "        print(\"File Path: \", sev_filepath, '\\n')\n",
    "\n",
    "        # Last-minute chunking to not hit the maximum buffer size for chunks\n",
    "        if len(severity_final.time.values) == 365:\n",
    "            severity_final = severity_final.chunk({'time':73})\n",
    "        elif len(severity_final.time.values) == 366:\n",
    "            severity_final = severity_final.chunk({'time':61})\n",
    "        print(f\"{year} Severity:\\n\", severity_final, '\\n')\n",
    "\n",
    "        if show_debug:\n",
    "            raise ValueError(\"You have reached the end of the debug. To begin saving the data, set show_debug to False!\")\n",
    "        else:\n",
    "            with ProgressBar():\n",
    "                severity_final.to_zarr(sev_filepath, mode='w')\n",
    "            \n",
    "            # Freeing up memory\n",
    "            severity_final.close()\n",
    "            del severity_final\n",
    "            gc.collect()\n",
    "            \n",
    "            print(\"Done saving! Moving along!\", '\\n')\n",
    "            print(\"-----------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "    stop_monitoring = True\n",
    "    print(\"Finished saving data for all years!\")\n",
    "    print(\"--------------------o-------------------------------o------------------------------o----------------------------\", '\\n')  \n",
    "\n",
    "\n",
    "calculate_severity(obs_data=Full_obs, thresh_data=Full_thresh.drop_vars('quantile'), clim_data=Full_means, \n",
    "                   baseline_name=baseline_name_arg, folder_name=folder_name_arg, custom_id=custom_id_choice,\n",
    "                   best_chunks=optimal_chunks, \n",
    "                   smoothWidth=31, percentile=current_percentile,\n",
    "                   minDaysMhwDuration=5, maxGapDaysInMhw=2,\n",
    "                   minutes_per_mem_update=5, show_debug=False, \n",
    "                   custom_years=True, start_yr=1982, end_yr=2024,\n",
    "                   mhw_test=False)\n",
    "\n",
    "stop_monitoring = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f316991-d225-4e11-bb36-a6fa3b93d98f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Began animation for the severity in the 2024 Full datasets!\n",
      "Saving animation at: Full_severity_Baseline9322_2024.gif\n",
      "  Doy (Frame) Processed: 366/366\n",
      "Animation finished and saved!\n",
      "\n",
      "Began animation for the marine heatwave severity in the 2024 Full datasets!\n",
      "Saving animation at: Full_severity_mhw_Baseline9322_2024.gif\n",
      "  Doy (Frame) Processed: 366/366\n",
      "Animation finished and saved!\n",
      "\n",
      "Began animation for the severity in the 1993 Full datasets!\n",
      "Saving animation at: Full_severity_Baseline9322_1993.gif\n",
      "  Doy (Frame) Processed: 365/365\n",
      "Animation finished and saved!\n",
      "\n",
      "Began animation for the marine heatwave severity in the 1993 Full datasets!\n",
      "Saving animation at: Full_severity_mhw_Baseline9322_1993.gif\n",
      "  Doy (Frame) Processed: 365/365\n",
      "Animation finished and saved!\n",
      "\n",
      "Began animation for the severity in the 1982 Full datasets!\n",
      "Saving animation at: Full_severity_Baseline9322_1982.gif\n",
      "  Doy (Frame) Processed: 365/365\n",
      "Animation finished and saved!\n",
      "\n",
      "Began animation for the marine heatwave severity in the 1982 Full datasets!\n",
      "Saving animation at: Full_severity_mhw_Baseline9322_1982.gif\n",
      "  Doy (Frame) Processed: 365/365\n",
      "Animation finished and saved!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.animation.FuncAnimation at 0x7f7de41309a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create an animation for MHW severity or severity (regardless of MHWs)\n",
    "\n",
    "## Function to create an animation that shows the mean and percentile latitude and longitude maps for the full 1 - 366 period. \n",
    "def check_severities_with_an_animation(baseline_name_arg, folder_name_arg, \n",
    "                                       year, mhw_only = False,\n",
    "                                        custom_output_filename=None, \n",
    "                                        percentile=None):\n",
    "\n",
    "    ## First, gather the appropriate stored dataset filepath\n",
    "    if mhw_only:\n",
    "        data_var_to_access = \"severity_mhw\"\n",
    "        data_type = \"MHW Severity\"\n",
    "    else:\n",
    "        data_var_to_access = \"severity\"\n",
    "        data_type = \"Severity\"\n",
    "\n",
    "    # NOTE: POTENTIAL DIRECTORY TWEAKING HERE\n",
    "    path = f'{my_root_directory}/OISST/Severity_{percentile}th/{folder_name_arg}_severity_SST_PC{percentile}th_{year}_{baseline_name_arg}.zarr'  \n",
    "    \n",
    "    ## Fill a dictionary where all present days are matched with their corresponding filepath\n",
    "    times_dict = {}\n",
    "    \n",
    "    # Open and check what days are in this file\n",
    "    try:\n",
    "        ds = xr.open_zarr(path)[data_var_to_access]\n",
    "    except:\n",
    "        raise ValueError(f\"No file found at: {path}.\")\n",
    "    \n",
    "    available_times = ds['time'].values\n",
    "    available_times = pd.to_datetime(available_times)\n",
    "   \n",
    "    # Quick fix for my personal, early datasets\n",
    "    lon = ds.lon.values\n",
    "    lat = ds.lat.values\n",
    "\n",
    "    # Set up the plot using the values in the first day\n",
    "    setup_data = ds.isel(time=0).values\n",
    "\n",
    "    ## Initialize the plot    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6), \n",
    "                           subplot_kw={'projection': ccrs.Mercator()})\n",
    "    \n",
    "    pcm = ax.pcolormesh(\n",
    "        lon, lat, setup_data,\n",
    "        cmap='RdYlBu_r',\n",
    "        vmin=-1, vmax=5,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "    )\n",
    "    cbar = plt.colorbar(pcm, ax=ax, label='Temperature Severity')  # Adjust label as needed\n",
    "    \n",
    "    ax.set_extent([0, 360, -5, 90], crs=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.LAND, color='lightgray')\n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth=0.8)\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    \n",
    "    title = ax.set_title('')\n",
    "    title_base = '(Relative to 1993-2022)' if baseline_name_arg == 'Baseline9322' else f'({baseline_name_arg})'\n",
    "    \n",
    "    ## Animation function\n",
    "    def animate(i):\n",
    "        time = available_times[i]\n",
    "        frame_data = ds.isel(time=i).values\n",
    "        \n",
    "        # Update the plot\n",
    "        pcm.set_array(frame_data.ravel())\n",
    "\n",
    "        # Update the title appropriately\n",
    "        time_str = pd.Timestamp(time).strftime('%Y-%m-%d')\n",
    "        title.set_text(f'{data_type} for {time_str}\\n{title_base}')\n",
    "        \n",
    "        return pcm, title\n",
    "    \n",
    "    \n",
    "    ## Create the resulting animation\n",
    "    type_message = \"marine heatwave severity\" if mhw_only else \"severity\"\n",
    "    print(f\"Began animation for the {type_message} in the {year} {folder_name_arg} datasets!\")\n",
    "    \n",
    "    chosen_times = len(available_times)\n",
    "    \n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, animate,\n",
    "        frames=chosen_times,\n",
    "        interval=200,\n",
    "        blit=False,\n",
    "        repeat=True\n",
    "    )\n",
    "    \n",
    "    writer = animation.PillowWriter(fps=2)\n",
    "    \n",
    "    if custom_output_filename == None:\n",
    "        output_filename = f\"{folder_name_arg}_{data_var_to_access}_{baseline_name_arg}_{year}.gif\"\n",
    "    else:\n",
    "        output_filename = custom_output_filename\n",
    "    print(f\"Saving animation at: {output_filename}\") \n",
    "    \n",
    "    \n",
    "    ## Save the resulting animation\n",
    "    def print_frame_progress(current_frame, total_frames):\n",
    "        print(f\"\\r  Date (Frame) Processed: {current_frame + 1}/{total_frames}\", end='', flush=True)\n",
    "\n",
    "    #with ProgressBar():\n",
    "    anim.save(output_filename, writer=writer, dpi=100,\n",
    "              progress_callback=print_frame_progress)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.close(fig)\n",
    "    ds.close()\n",
    "    print(f\"\\nAnimation finished and saved!\\n\")\n",
    "    \n",
    "    return anim\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "check_severities_with_an_animation(\"Baseline9322\", \"Full\", year = 2024, mhw_only = False, custom_output_filename=None, percentile=90)\n",
    "check_severities_with_an_animation(\"Baseline9322\", \"Full\", year = 2024, mhw_only = True, custom_output_filename=None, percentile=90)\n",
    "check_severities_with_an_animation(\"Baseline9322\", \"Full\", year = 1993, mhw_only = False, custom_output_filename=None, percentile=90)\n",
    "check_severities_with_an_animation(\"Baseline9322\", \"Full\", year = 1993, mhw_only = True, custom_output_filename=None, percentile=90)\n",
    "check_severities_with_an_animation(\"Baseline9322\", \"Full\", year = 1982, mhw_only = False, custom_output_filename=None, percentile=90)\n",
    "check_severities_with_an_animation(\"Baseline9322\", \"Full\", year = 1982, mhw_only = True, custom_output_filename=None, percentile=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3b1934-1d40-4eb7-9949-baf3c0197153",
   "metadata": {},
   "source": [
    "MHW Detection Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c660c97f-7d6c-4e27-b5c9-4f26aec30c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------\n",
      "Part 1: Process time arguments and load the raw data!\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Chosen 30-year historical baseline period (slice): slice('1993-01-01', '2022-12-31', None), or [1993, 2022]\n",
      "Note: the chosen baseline period has been identified as roughly covering a 30 year period. Do ensure this is the case separately.\n",
      "\n",
      "Chosen target analysis period: from 1993-1-1 to 2024-12-30\n",
      "\n",
      "Chosen latitude value(s): 11\n",
      "Chosen longitude value(s): 304 (originally -56)\n",
      "\n",
      "Chosen window half-width: 5\n",
      "(This means we use 5 days before and after each day of the year in our climatology calculations.)\n",
      "\n",
      "Chosen smoothing width (or window that is centered on a day): 31\n",
      "(This means we use 15 days before and after each day of the year in our climatology smoothing.)\n",
      "\n",
      "Filtered raw data dataset:\n",
      " <xarray.DataArray 'sst' (time: 16153, lat: 420, lon: 1440)> Size: 39GB\n",
      "dask.array<getitem, shape=(16153, 420, 1440), dtype=float32, chunksize=(32, 420, 1440), chunktype=numpy.ndarray>\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 129kB 1981-09-01 1981-09-02 ... 2025-11-21\n",
      "  * lat      (lat) float32 2kB -14.88 -14.62 -14.38 -14.12 ... 89.38 89.62 89.88\n",
      "  * lon      (lon) float32 6kB 0.125 0.375 0.625 0.875 ... 359.4 359.6 359.9\n",
      "Attributes:\n",
      "    long_name:     Daily Sea Surface Temperature\n",
      "    units:         degC\n",
      "    valid_range:   [-3. 45.]\n",
      "    precision:     2.0\n",
      "    dataset:       NOAA High-resolution Blended Analysis\n",
      "    var_desc:      Sea Surface Temperature\n",
      "    level_desc:    Surface\n",
      "    statistic:     Mean\n",
      "    parent_stat:   Individual Observations\n",
      "    actual_range:  [-1.8  34.82] \n",
      "\n",
      "Final (properly chunked) raw data dataset:\n",
      " <xarray.DataArray 'sst' (time: 16153, lat: 420, lon: 1440)> Size: 39GB\n",
      "dask.array<rechunk-merge, shape=(16153, 420, 1440), dtype=float32, chunksize=(32, 210, 160), chunktype=numpy.ndarray>\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 129kB 1981-09-01 1981-09-02 ... 2025-11-21\n",
      "  * lat      (lat) float32 2kB -14.88 -14.62 -14.38 -14.12 ... 89.38 89.62 89.88\n",
      "  * lon      (lon) float32 6kB 0.125 0.375 0.625 0.875 ... 359.4 359.6 359.9\n",
      "Attributes:\n",
      "    long_name:     Daily Sea Surface Temperature\n",
      "    units:         degC\n",
      "    valid_range:   [-3. 45.]\n",
      "    precision:     2.0\n",
      "    dataset:       NOAA High-resolution Blended Analysis\n",
      "    var_desc:      Sea Surface Temperature\n",
      "    level_desc:    Surface\n",
      "    statistic:     Mean\n",
      "    parent_stat:   Individual Observations\n",
      "    actual_range:  [-1.8  34.82] \n",
      "\n",
      "Filtered raw data (based on our desired lat and lon values):\n",
      " <xarray.DataArray 'sst' (time: 16153)> Size: 65kB\n",
      "dask.array<getitem, shape=(16153,), dtype=float32, chunksize=(32,), chunktype=numpy.ndarray>\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 129kB 1981-09-01 1981-09-02 ... 2025-11-21\n",
      "    lat      float32 4B 11.12\n",
      "    lon      float32 4B 304.1\n",
      "Attributes:\n",
      "    long_name:     Daily Sea Surface Temperature\n",
      "    units:         degC\n",
      "    valid_range:   [-3. 45.]\n",
      "    precision:     2.0\n",
      "    dataset:       NOAA High-resolution Blended Analysis\n",
      "    var_desc:      Sea Surface Temperature\n",
      "    level_desc:    Surface\n",
      "    statistic:     Mean\n",
      "    parent_stat:   Individual Observations\n",
      "    actual_range:  [-1.8  34.82] \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "Part 2: Load the climatalogical and severity datasets!\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Severity Baseline9322 (using 90th percentile threshold) dataset:\n",
      "<xarray.DataArray 'severity_mhw' (time: 15706, lat: 420, lon: 1440)> Size: 76GB\n",
      "dask.array<concatenate, shape=(15706, 420, 1440), dtype=float64, chunksize=(122, 210, 160), chunktype=numpy.ndarray>\n",
      "Coordinates:\n",
      "  * lat      (lat) float32 2kB -14.88 -14.62 -14.38 -14.12 ... 89.38 89.62 89.88\n",
      "  * lon      (lon) float32 6kB 0.125 0.375 0.625 0.875 ... 359.4 359.6 359.9\n",
      "  * time     (time) datetime64[ns] 126kB 1982-01-01 1982-01-02 ... 2024-12-31\n",
      "\n",
      "Filtered severity dataset:\n",
      " <xarray.DataArray 'severity_mhw' (time: 11687)> Size: 93kB\n",
      "dask.array<getitem, shape=(11687,), dtype=float64, chunksize=(122,), chunktype=numpy.ndarray>\n",
      "Coordinates:\n",
      "    lat      float32 4B 11.12\n",
      "    lon      float32 4B 304.1\n",
      "  * time     (time) datetime64[ns] 93kB 1993-01-01 1993-01-02 ... 2024-12-30 \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "Part 3: Gathering the data for validation (finally, right?)!\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Final Processed Raw Data:\n",
      " <xarray.DataArray 'sst' (time: 16153)> Size: 65kB\n",
      "dask.array<getitem, shape=(16153,), dtype=float32, chunksize=(32,), chunktype=numpy.ndarray>\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 129kB 1981-09-01 1981-09-02 ... 2025-11-21\n",
      "Attributes:\n",
      "    long_name:     Daily Sea Surface Temperature\n",
      "    units:         degC\n",
      "    valid_range:   [-3. 45.]\n",
      "    precision:     2.0\n",
      "    dataset:       NOAA High-resolution Blended Analysis\n",
      "    var_desc:      Sea Surface Temperature\n",
      "    level_desc:    Surface\n",
      "    statistic:     Mean\n",
      "    parent_stat:   Individual Observations\n",
      "    actual_range:  [-1.8  34.82] \n",
      "\n",
      "MHW presence (from the saved severity dataset) example (first 6 observations):\n",
      " [False False False False False False] \n",
      "\n",
      "Time Values:\n",
      " [723424 723425 723426 ... 739574 739575 739576] \n",
      "\n",
      "Loading temperature values...\n",
      "[########################################] | 100% Completed | 302.22 s\n",
      "Temperature Values:  [28.58] \n",
      "\n",
      "Detected MHWs (Accordingly to marineHeatWaves.py):\n",
      " [False False False ... False False False] \n",
      "\n",
      "\n",
      "Start: 1993-01-01\n",
      "End: 2024-12-30\n",
      "Length: 11687 days\n",
      "\n",
      "All mhw entries between 1993-01-01 and 2024-12-30 match! \n"
     ]
    }
   ],
   "source": [
    "## This code performs a validation check of the climatological means and \n",
    "# percentile thresholds by comparing them to those of marineHeatWaves\n",
    "\n",
    "## ------------------------------------------------------------------------------------------\n",
    "# Function to validate the produced means and percentiles against the marineHeatWaves package\n",
    "def please_validate_mhws_with_marineHeatWaves(temperature_data_arg=None, \n",
    "                                              baseline_slice=None, baseline_name_arg=None, \n",
    "                                             folder_name=None, custom_id=\"\",\n",
    "                                             analysis_start_date=None, analysis_end_date=None,\n",
    "                                             lat_values=None, lon_values=None,\n",
    "                                             clim_doy_start=1, clim_doy_end=366,\n",
    "                                             show_climatological_means=False,\n",
    "                                             custom_region=False, \n",
    "                                             show_debug=True,\n",
    "                                             percentile=90, \n",
    "                                             window_half_width=5, smooth_width=31, \n",
    "                                             vals_per_entry=6, check_all=True):\n",
    "    \n",
    "    '''\n",
    "    Known Limitation: This function can only validate means and percentiles from January 1st of any particular year \n",
    "    to December 31st of the same year due to how this code handles computations related to each day of the year (doy). \n",
    "    A fix could be done by assigning a new coordinate that runs from 1 to the end of the doy values series in the future.\n",
    "    '''\n",
    "    # Convert negative longitude values to the positive 0-360 deg format\n",
    "    original_lon_values = lon_values\n",
    "    if lon_values < 0:\n",
    "        lon_values = (lon_values + 360) % 360\n",
    "    \n",
    "    if show_debug: \n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "        print(f\"Part 1: Process time arguments and load the raw data!\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    start_dt_analysis = datetime.strptime(analysis_start_date, \"%Y-%m-%d\")\n",
    "    start_analysis_year = start_dt_analysis.year\n",
    "    start_analysis_month = start_dt_analysis.month\n",
    "    start_analysis_day = start_dt_analysis.day\n",
    "    start_string = f\"{start_analysis_year}-{start_analysis_month}-{start_analysis_day}\"\n",
    "    \n",
    "    end_dt_analysis = datetime.strptime(analysis_end_date, \"%Y-%m-%d\")\n",
    "    end_analysis_year = end_dt_analysis.year\n",
    "    end_analysis_month = end_dt_analysis.month\n",
    "    end_analysis_day = end_dt_analysis.day\n",
    "    end_string = f\"{end_analysis_year}-{end_analysis_month}-{end_analysis_day}\"\n",
    "\n",
    "    baseline_dt_start = datetime.strptime(baseline_slice.start, \"%Y-%m-%d\")\n",
    "    baseline_dt_end = datetime.strptime(baseline_slice.stop, \"%Y-%m-%d\")\n",
    "    climatology_period = [baseline_dt_start.year, baseline_dt_end.year]\n",
    "    \n",
    "    print(f\"Chosen 30-year historical baseline period (slice): {baseline_slice}, or {climatology_period}\")\n",
    "    print(\"Note: the chosen baseline period has been identified as roughly covering a 30 year period. Do ensure this is the case separately.\\n\")\n",
    "    print(f\"Chosen target analysis period: from {start_string} to {end_string}\\n\")\n",
    "    print(f\"Chosen latitude value(s): {lat_values}\\nChosen longitude value(s): {lon_values} (originally {original_lon_values})\\n\")\n",
    "    window_msg_1 = \"(This means we use\"\n",
    "    window_msg_2 = \"days before and after each day of the year in our climatology\"\n",
    "    print(f\"Chosen window half-width: {window_half_width}\")\n",
    "    print(f\"{window_msg_1} {window_half_width} {window_msg_2} calculations.)\\n\")\n",
    "    print(f\"Chosen smoothing width (or window that is centered on a day): {smooth_width}\")\n",
    "    smooth_half_width = (smooth_width - 1)//2\n",
    "    print(f\"{window_msg_1} {smooth_half_width} {window_msg_2} smoothing.)\\n\")\n",
    "\n",
    "    ## We get the datasets\n",
    "    # Observed (raw) data\n",
    "    # Your filepath here, this is my setup; NOTE: POTENTIAL DIRECTORY TWEAKING HERE\n",
    "    raw_data_directory = f'{my_root_directory}/OISST/Data/sst.day.mean.*.nc'\n",
    "    \n",
    "    raw_ds = xr.open_mfdataset(\n",
    "        raw_data_directory,         # Glob pattern (the * grabs all datasets)\n",
    "        parallel=True,              # Enable parallel file opening \n",
    "        chunks='auto',              # Let dask choose optimal chunking (for now)\n",
    "        combine='by_coords',        # Merge based on coordinate values (important)\n",
    "        engine='netcdf4')           # Specify the engine (may crash without this)\n",
    "    filt_obs_ds = raw_ds.sel(lat=slice(-15, 90), lon=slice(0, 360)).sst\n",
    "    if show_debug: print(\"Filtered raw data dataset:\\n\", filt_obs_ds, '\\n')\n",
    "    \n",
    "    # Rechunk the final raw data dataset\n",
    "    optimal_chunking = {'lat': 210, 'lon': 160}\n",
    "    sst_ds = filt_obs_ds.chunk(optimal_chunking)\n",
    "    if show_debug: print(\"Final (properly chunked) raw data dataset:\\n\", sst_ds, '\\n')\n",
    "\n",
    "    # Filter for select data \n",
    "    def filter_data(ds, lat_values_arg, lon_values_arg, custom_region_arg):\n",
    "        if custom_region_arg: # if a region of lat and lon values is provided\n",
    "            data_subset = ds.sel(lat=lat_values_arg, lon=lon_values_arg)\n",
    "        else: # if only a point is provided (we use the nearest)\n",
    "            data_subset = ds.sel(lat=lat_values_arg, lon=lon_values_arg, method='nearest')\n",
    "        return data_subset\n",
    "        \n",
    "    obs_ds = filter_data(sst_ds, lat_values, lon_values, custom_region)\n",
    "    \n",
    "    \n",
    "    if show_debug: \n",
    "        print(\"Filtered raw data (based on our desired lat and lon values):\\n\", obs_ds, '\\n')\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "        print(f\"Part 2: Load the climatalogical and severity datasets!\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "    # Set up the id to load the dataset with, if present\n",
    "    if custom_id != \"\":\n",
    "        id = f\"{custom_id}_\"\n",
    "    else:\n",
    "        id =\"\"\n",
    "\n",
    "\n",
    "    # Load the severity dataset\n",
    "    if start_analysis_year == end_analysis_year:\n",
    "        file_name = f\"{folder_name}_severity_{id}SST_PC{percentile}th_{start_analysis_year}_smoothWidth{smooth_width}_{baseline_name_arg}.zarr\"\n",
    "        sev_filepath = f'{my_root_directory}/OISST/Severity_{percentile}th/{baseline_name_arg}/{file_name}' # NOTE: POTENTIAL DIRECTORY TWEAKING HERE\n",
    "        sev_ds = xr.open_zarr(sev_filepath).severity_mhw\n",
    "        print(f\"Severity {baseline_name_arg} (using {percentile}th percentile threshold) dataset:\\n{sev_ds}\\n\")\n",
    "\n",
    "        sev_subset = filter_data(sev_ds, lat_values, lon_values, custom_region)\n",
    "        sev_mhw = sev_subset.sel(time=slice(analysis_start_date, analysis_end_date))\n",
    "        print(\"Filtered severity dataset:\\n\", sev_mhw, '\\n')\n",
    "    else:\n",
    "        file_name = f\"{folder_name}_severity_{id}SST_PC{percentile}th_*_smoothWidth{smooth_width}_{baseline_name_arg}.zarr\"\n",
    "        sev_directory = f'{my_root_directory}/OISST/Severity_{percentile}th/{baseline_name_arg}/{file_name}' # NOTE: POTENTIAL DIRECTORY TWEAKING HERE\n",
    "        sev_ds = xr.open_mfdataset(\n",
    "            sev_directory,         # Glob pattern (the * grabs all datasets)\n",
    "            parallel=True,              # Enable parallel file opening \n",
    "            chunks='auto',              # Let dask choose optimal chunking (for now)\n",
    "            combine='by_coords',        # Merge based on coordinate values (important)\n",
    "            engine='zarr')           # Specify the engine (may crash without this)\n",
    "        print(f\"Severity {baseline_name_arg} (using {percentile}th percentile threshold) dataset:\\n{sev_ds.severity_mhw}\\n\")\n",
    "        \n",
    "        sev_subset = filter_data(sev_ds.severity_mhw, lat_values, lon_values, custom_region)\n",
    "        sev_mhw = sev_subset.sel(time=slice(analysis_start_date, analysis_end_date))\n",
    "        print(\"Filtered severity dataset:\\n\", sev_mhw, '\\n')\n",
    "\n",
    "        \n",
    "    if show_debug: \n",
    "        print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Part 3: Gathering the data for validation (finally, right?)!\")\n",
    "        print(\"---------------------------------------------------------------------------------------------------------\\n\")\n",
    "        \n",
    "    # Take the mean of a custom region\n",
    "    if custom_region:\n",
    "        obs_proc = obs_ds.mean(dim=['lat', 'lon'])\n",
    "        if show_debug: print(\"Processed Mean of the Set Custom Region (for the Raw Data):\\n\", obs_proc, '\\n')\n",
    "    else: \n",
    "        obs_proc = obs_ds\n",
    "\n",
    "    # Drop unwanted coordinates/variables\n",
    "    undesired_coords = ['quantile', 'lat', 'lon']\n",
    "    for coord in undesired_coords:\n",
    "        if coord in obs_proc.coords:\n",
    "            obs_proc = obs_proc.drop_vars([coord])\n",
    "    print(\"Final Processed Raw Data:\\n\", obs_proc, '\\n')\n",
    "\n",
    "    # Severity dataset mhw presence bool set up\n",
    "    mhw_calc = ~np.isnan(sev_mhw)\n",
    "    print(\"MHW presence (from the saved severity dataset) example (first 6 observations):\\n\", mhw_calc.values[:6], '\\n')\n",
    "\n",
    "    observed_times = np.array([pd.Timestamp(time).toordinal() for time in obs_proc.time.values])\n",
    "    print(\"Time Values:\\n\", observed_times, '\\n')\n",
    "\n",
    "    with ProgressBar():\n",
    "        print(\"Loading temperature values...\")\n",
    "        observed_temps = obs_proc.values\n",
    "        print(\"Temperature Values: \", observed_temps[:1], '\\n')\n",
    "\n",
    "    '''\n",
    "    # marineHeatWaves detect function (based on marineheatwaves.py):\n",
    "    def detect(t, temp, climatologyPeriod=[None,None], pctile=90, windowHalfWidth=5, smoothPercentile=True, \n",
    "                 smoothPercentileWidth=31, minDuration=5, joinAcrossGaps=True, maxGap=2, \n",
    "                 maxPadLength=False, coldSpells=False, alternateClimatology=False, Ly=False)\n",
    "    \n",
    "    ## outputs: mhw, clim\n",
    "    # mhw - Detected marine heat waves (MHWs). Each key (following list) is a \n",
    "     list of length N where N is the number of detected MHWs (and related mhw information).\n",
    "     \n",
    "    # clim - Climatology of SST. Each key (following list) is a seasonally-varying time series \n",
    "      [1D numpy array of length T] of a particular measure:\n",
    "          #  'thresh' - Seasonally varying threshold (e.g., 90th percentile)\n",
    "          #   'seas' -  Climatological seasonal cycle\n",
    "\n",
    "    ## inputs:\n",
    "    # t - Time vector, in datetime format\n",
    "    # temp - Temperature vector [1D numpy array of length T]\n",
    "    # climatologyPeriod  -   Period over which climatology is calculated, specified\n",
    "                             as list of start and end years [year1, year2]. Default is to calculate\n",
    "                             over the full range of years in the supplied time series.\n",
    "    # pctile                 Threshold percentile (%) for detection of extreme values\n",
    "                             (DEFAULT = 90)\n",
    "    # windowHalfWidth    -   Width of window (one sided) about day-of-year used for the pooling of \n",
    "                             values and calculation of threshold percentile (DEFAULT = 5 [days]). \n",
    "                             In other words, the number of days before and after each central day-of-year \n",
    "                             would be: \n",
    "                                       Total Window Size = (windowHalfWidth * 2) + 1 = \n",
    "                                                           (5 days x 2 sides) + 1 center day = 11 days\n",
    "    # smoothPercentile       Boolean switch indicating whether to smooth the threshold\n",
    "                             percentile timeseries with a moving average (DEFAULT = True). \n",
    "                             Note: this smooths both climatological means AND percentiles!\n",
    "                             Here is the relevant code from the .py file:\n",
    "                             \n",
    "                                # Smooth if desired\n",
    "                                if smoothPercentile:\n",
    "                                    # If the length of year is < 365/366 (e.g. a 360 day year from a Climate Model)\n",
    "                                    if Ly:\n",
    "                                        valid = ~np.isnan(thresh_climYear)\n",
    "                                        thresh_climYear[valid] = runavg(thresh_climYear[valid], smoothPercentileWidth)\n",
    "                                        valid = ~np.isnan(seas_climYear)\n",
    "                                        seas_climYear[valid] = runavg(seas_climYear[valid], smoothPercentileWidth)\n",
    "                                    # >= 365-day year\n",
    "                                    else: ## (This is the relevant part for this validation code \\/ ) ##\n",
    "                                        thresh_climYear = runavg(thresh_climYear, smoothPercentileWidth)\n",
    "                                        seas_climYear = runavg(seas_climYear, smoothPercentileWidth)\n",
    "                                        \n",
    "    # smoothPercentileWidth  Width of moving average window for smoothing threshold\n",
    "                             (DEFAULT = 31 [days])\n",
    "    # minDuration            Minimum duration for acceptance detected MHWs\n",
    "                             (DEFAULT = 5 [days])\n",
    "    # joinAcrossGaps         Boolean switch indicating whether to join MHWs\n",
    "                             which occur before/after a short gap (DEFAULT = True)\n",
    "    # maxGap                 Maximum length of gap allowed for the joining of MHWs\n",
    "                             (DEFAULT = 2 [days])\n",
    "    # maxPadLength           Specifies the maximum length [days] over which to interpolate\n",
    "                             (pad) missing data (specified as nans) in input temp time series.\n",
    "                             i.e., any consecutive blocks of NaNs with length greater\n",
    "                             than maxPadLength will be left as NaN. Set as an integer.\n",
    "                             (DEFAULT = False, interpolates over all missing values).\n",
    "                             Here, we don't apply any interpolation to align with what was done in the prior code.\n",
    "                             This means maxPadLength is set to 0 here. Missing NA data remains missing.\n",
    "    # coldSpells             Specifies if the code should detect cold events instead of\n",
    "                             heat events. (DEFAULT = False)\n",
    "    # alternateClimatology   Specifies an alternate temperature time series to use for the\n",
    "                             calculation of the climatology. Format is as a list of numpy\n",
    "                             arrays: (1) the first element of the list is a time vector,\n",
    "                             in datetime format (e.g., date(1982,1,1).toordinal())\n",
    "                             [1D numpy array of length TClim] and (2) the second element of\n",
    "                             the list is a temperature vector [1D numpy array of length TClim].\n",
    "                             (DEFAULT = False)\n",
    "    # Ly                     Specifies if the length of the year is < 365/366 days (e.g. a \n",
    "                             360 day year from a climate model). This affects the calculation\n",
    "                             of the climatology. (DEFAULT = False) Since it is False, that means \n",
    "                             the function expects days indexed 1 through 365 (or 366).\n",
    "    '''\n",
    "    # The full observed_temps series IS required as an input (includes the climatology period)\n",
    "    mhw, clim = mhwpy.detect(t=observed_times, temp=observed_temps, \n",
    "                            climatologyPeriod=climatology_period, \n",
    "                            pctile=percentile, smoothPercentile=True, smoothPercentileWidth=31,\n",
    "                             maxPadLength=0)\n",
    "\n",
    "    # Initialize a boolean array with the same length as the time series \n",
    "    mhw_present = np.zeros(len(observed_times), dtype=bool)\n",
    "    \n",
    "    # Fill in True values for all the days within the detected MHW event(s)\n",
    "    for mhw_event in range(mhw['n_events']):\n",
    "        start_idx = mhw['index_start'][mhw_event]\n",
    "        end_idx = mhw['index_end'][mhw_event]\n",
    "        mhw_present[start_idx:end_idx + 1] = True\n",
    "    print(\"Detected MHWs (Accordingly to marineHeatWaves.py):\\n\", mhw_present, '\\n\\n')\n",
    "    \n",
    "    \n",
    "    ## Filtering for the desired time period\n",
    "    # Define your analysis period\n",
    "    start_date =  date(start_analysis_year, start_analysis_month, start_analysis_day).toordinal()\n",
    "    end_date = date(end_analysis_year, end_analysis_month, end_analysis_day).toordinal()\n",
    "\n",
    "    # Create and apply the analysis period mask\n",
    "    date_mask = (observed_times >= start_date) & (observed_times <= end_date)\n",
    "    observed_times_filtered = observed_times[date_mask]\n",
    "    mhw_present_filtered = mhw_present[date_mask]\n",
    "\n",
    "    print(\"Start:\", date.fromordinal(observed_times_filtered[0]))\n",
    "    print(\"End:\", date.fromordinal(observed_times_filtered[-1]))\n",
    "    print(\"Length:\", len(mhw_present_filtered), \"days\\n\")\n",
    "\n",
    "    if np.array_equal(mhw_present_filtered, mhw_calc):\n",
    "        print(f\"All mhw entries between {analysis_start_date} and {analysis_end_date} match! \")\n",
    "    else:\n",
    "        print(\"Mismatch detected mhws from marineHeatWaves.py and the calculated mhws! \")\n",
    "        num_mismatches = np.sum(np.array(mhw_present_filtered) != np.array(mhw_calc))\n",
    "        print(f\"Number of mismatches between {analysis_start_date} and {analysis_end_date}: {num_mismatches}\")\n",
    "        \n",
    "    if check_all:\n",
    "        for i in range(0, len(mhw_present_filtered), vals_per_entry):\n",
    "            next_mhw_vals = mhw_present_filtered[i:i + vals_per_entry]\n",
    "            next_sev_vals = mhw_calc[i:i + vals_per_entry]\n",
    "    \n",
    "            # Check if all entries match\n",
    "            if np.array_equal(next_mhw_vals, next_sev_vals):\n",
    "                print(f\"Chunk {i}: All entries match \")\n",
    "            else:\n",
    "                print(f\"Chunk {i}: Mismatch found \")\n",
    "            \n",
    "            print(\"marineHeatWave: ({})\".format(\", \".join(f\"{val:.5f}\" for val in next_mhw_vals)))\n",
    "            print(\"    Calculated: ({})\".format(\", \".join(f\"{val:.5f}\" for val in next_sev_vals.values)))\n",
    "            print(\"\")\n",
    "    else:\n",
    "        for i in range(0, len(mhw_present_filtered), vals_per_entry):\n",
    "            next_mhw_vals = mhw_present_filtered[i:i + vals_per_entry]\n",
    "            next_sev_vals = mhw_calc[i:i + vals_per_entry]\n",
    "            next_obs_times = [date.fromordinal(day) for day in observed_times_filtered[i:i + vals_per_entry]]\n",
    "            \n",
    "            # Skip matching entries\n",
    "            if np.array_equal(next_mhw_vals, next_sev_vals):\n",
    "                continue\n",
    "                \n",
    "            print(\"Dates: ({})\".format(\", \".join(str(val) for val in next_obs_times)))\n",
    "            print(\"marineHeatWave: ({})\".format(\", \".join(f\"{val:.5f}\" for val in next_mhw_vals)))\n",
    "            print(\"    Calculated: ({})\".format(\", \".join(f\"{val:.5f}\" for val in next_sev_vals.values)))\n",
    "            print(\"\")\n",
    "        \n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "please_validate_mhws_with_marineHeatWaves(baseline_slice=slice(\"1993-01-01\", \"2022-12-31\"), baseline_name_arg=\"Baseline9322\",\n",
    "                                          folder_name=folder_name_arg, custom_id=\"fgd\",\n",
    "                                            analysis_start_date=\"1993-01-01\", analysis_end_date=\"2024-12-30\",\n",
    "                                             lat_values=11, lon_values=-56,\n",
    "                                             clim_doy_start=1, clim_doy_end=366,\n",
    "                                             show_climatological_means=False,\n",
    "                                             custom_region=False, show_debug=True,\n",
    "                                             percentile=90, window_half_width=5, smooth_width=31,\n",
    "                                            vals_per_entry=6, check_all=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangeo23",
   "language": "python",
   "name": "pangeo23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
